<!doctype html>



  


<html class="theme-next muse use-motion" lang="zh-Hans">
<head>
  <meta charset="UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"/>



<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />












  
  
  <link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css" />




  
  
  
  

  

  

  
    

    
  

  

  

  
    
    
    <link href="//fonts.googleapis.com/css?family=Lato:300,300italic,400,400italic,700,700italic&subset=latin,latin-ext" rel="stylesheet" type="text/css">
  






<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css" />

<link href="/css/main.css?v=5.1.0" rel="stylesheet" type="text/css" />


  <meta name="keywords" content="机器学习,算法诊断,偏差方差分析,学习曲线,目标函数," />





  <link rel="alternate" href="/atom.xml" title="蘑菇先生学习记" type="application/atom+xml" />




  <link rel="shortcut icon" type="image/x-icon" href="/favicon.ico?v=5.1.0" />






<meta name="description" content="本文对Advice for applying Machine Learning一文中提到的算法诊断等理论方法进行实践，使用Python工具，具体包括数据的可视化(data visualizing)、模型选择(choosing a machine learning method suitable for the problem at hand)、过拟合和欠拟合识别和处理(identifying">
<meta property="og:type" content="article">
<meta property="og:title" content="Advice for applying Machine Learning(2)">
<meta property="og:url" content="xtf615.com/2017/04/03/practice-ml-advice/index.html">
<meta property="og:site_name" content="蘑菇先生学习记">
<meta property="og:description" content="本文对Advice for applying Machine Learning一文中提到的算法诊断等理论方法进行实践，使用Python工具，具体包括数据的可视化(data visualizing)、模型选择(choosing a machine learning method suitable for the problem at hand)、过拟合和欠拟合识别和处理(identifying">
<meta property="og:image" content="xtf615.com/picture/machine-learning/practice-advice1.jpg">
<meta property="og:image" content="xtf615.com/picture/machine-learning/practice-advice2.png">
<meta property="og:image" content="xtf615.com/picture/machine-learning/practice-advice3.png">
<meta property="og:image" content="xtf615.com/picture/machine-learning/machine-learning-method.png">
<meta property="og:image" content="xtf615.com/picture/machine-learning/practice-advice4.png">
<meta property="og:image" content="xtf615.com/picture/machine-learning/advice1.jpg">
<meta property="og:image" content="xtf615.com/picture/machine-learning/practice-advice5.png">
<meta property="og:image" content="xtf615.com/picture/machine-learning/practice-advice6.png">
<meta property="og:image" content="xtf615.com/picture/machine-learning/practice-advice7.png">
<meta property="og:image" content="xtf615.com/picture/machine-learning/practice-advice8.png">
<meta property="og:image" content="xtf615.com/picture/machine-learning/practice-advice9.png">
<meta property="og:image" content="xtf615.com/picture/machine-learning/practice-advice10.png">
<meta property="og:image" content="xtf615.com/picture/machine-learning/practice-advice11.png">
<meta property="og:image" content="xtf615.com/picture/machine-learning/practice-advice12.png">
<meta property="og:image" content="xtf615.com/picture/machine-learning/practice-advice13.png">
<meta property="og:image" content="xtf615.com/picture/machine-learning/practice-advice14.png">
<meta property="og:image" content="xtf615.com/picture/machine-learning/practice-advice15.png">
<meta property="og:image" content="xtf615.com/picture/machine-learning/practice-advice16.png">
<meta property="og:image" content="xtf615.com/picture/machine-learning/practice-advice17.png">
<meta property="og:image" content="xtf615.com/picture/machine-learning/practice-advice18.png">
<meta property="og:image" content="xtf615.com/picture/machine-learning/practice-advice19.png">
<meta property="og:image" content="xtf615.com/picture/machine-learning/practice-advice20.png">
<meta property="og:image" content="xtf615.com/picture/machine-learning/practice-advice21.png">
<meta property="og:image" content="xtf615.com/picture/machine-learning/practice-advice22.png">
<meta property="og:image" content="xtf615.com/picture/machine-learning/practice-advice23.png">
<meta property="og:image" content="xtf615.com/picture/machine-learning/practice-advice24.png">
<meta property="og:image" content="xtf615.com/picture/machine-learning/practice-advice25.png">
<meta property="og:image" content="xtf615.com/picture/machine-learning/practice-advice26.png">
<meta property="og:updated_time" content="2017-04-04T09:25:50.068Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Advice for applying Machine Learning(2)">
<meta name="twitter:description" content="本文对Advice for applying Machine Learning一文中提到的算法诊断等理论方法进行实践，使用Python工具，具体包括数据的可视化(data visualizing)、模型选择(choosing a machine learning method suitable for the problem at hand)、过拟合和欠拟合识别和处理(identifying">
<meta name="twitter:image" content="xtf615.com/picture/machine-learning/practice-advice1.jpg">



<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Muse',
    sidebar: {"position":"left","display":"post"},
    fancybox: true,
    motion: true,
    duoshuo: {
      userId: '0',
      author: '博主'
    },
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>



  <link rel="canonical" href="xtf615.com/2017/04/03/practice-ml-advice/"/>





  <title> Advice for applying Machine Learning(2) | 蘑菇先生学习记 </title>
</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="zh-Hans">

  










  
  
    
  

  <div class="container one-collumn sidebar-position-left page-post-detail ">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-meta ">
  

  <div class="custom-logo-site-title">
    <a href="/"  class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <span class="site-title">蘑菇先生学习记</span>
      <span class="logo-line-after"><i></i></span>
    </a>
  </div>
    
      <p class="site-subtitle"></p>
    
</div>

<div class="site-nav-toggle">
  <button>
    <span class="btn-bar"></span>
    <span class="btn-bar"></span>
    <span class="btn-bar"></span>
  </button>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-home"></i> <br />
            
            首页
          </a>
        </li>
      
        
        <li class="menu-item menu-item-categories">
          <a href="/categories" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-th"></i> <br />
            
            分类
          </a>
        </li>
      
        
        <li class="menu-item menu-item-about">
          <a href="/about" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-user"></i> <br />
            
            关于
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-archive"></i> <br />
            
            归档
          </a>
        </li>
      
        
        <li class="menu-item menu-item-tags">
          <a href="/tags" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-tags"></i> <br />
            
            标签
          </a>
        </li>
      

      
        <li class="menu-item menu-item-search">
          
            <a href="javascript:;" class="st-search-show-outputs">
          
            
              <i class="menu-item-icon fa fa-search fa-fw"></i> <br />
            
            搜索
          </a>
        </li>
      
    </ul>
  

  
    <div class="site-search">
      
  <form class="site-search-form">
  <input type="text" id="st-search-input" class="st-search-input st-default-search-input" />
</form>

<script type="text/javascript">
  (function(w,d,t,u,n,s,e){w['SwiftypeObject']=n;w[n]=w[n]||function(){
    (w[n].q=w[n].q||[]).push(arguments);};s=d.createElement(t);
    e=d.getElementsByTagName(t)[0];s.async=1;s.src=u;e.parentNode.insertBefore(s,e);
  })(window,document,'script','//s.swiftypecdn.com/install/v2/st.js','_st');

  _st('install', 'WgLy48WeXh1aXsWx1x7L','2.0.0');
</script>



    </div>
  
</nav>



 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            

  <div id="posts" class="posts-expand">
    

  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">
  <link itemprop="mainEntityOfPage" href="xtf615.com/2017/04/03/practice-ml-advice/">

  <span style="display:none" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <meta itemprop="name" content="xuetf">
    <meta itemprop="description" content="">
    <meta itemprop="image" content="https://avatars1.githubusercontent.com/u/11912425?v=3&u=11f9f5dc75aaf84f020a06c0b9cb2b6f401c586b&s=400">
  </span>

  <span style="display:none" itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
    <meta itemprop="name" content="蘑菇先生学习记">
    <span style="display:none" itemprop="logo" itemscope itemtype="http://schema.org/ImageObject">
      <img style="display:none;" itemprop="url image" alt="蘑菇先生学习记" src="">
    </span>
  </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
            
            
              
                Advice for applying Machine Learning(2)
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2017-04-03T15:21:13+08:00">
                2017-04-03
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/机器学习/" itemprop="url" rel="index">
                    <span itemprop="name">机器学习</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          

          
          

          
            <span class="post-meta-divider">|</span>
            <span class="page-pv"><i class="fa fa-file-o"></i> 阅读量 
            <span class="busuanzi-value" id="busuanzi_value_page_pv" ></span>
            </span>
          

          

        </div>
      </header>
    


    <div class="post-body" itemprop="articleBody">

      
      

      
        <p>　　本文对<a href="/2017/04/01/ml-advice/">Advice for applying Machine Learning</a>一文中提到的算法诊断等理论方法进行实践，使用Python工具，具体包括数据的可视化(data visualizing)、模型选择(choosing a machine learning method suitable for the problem at hand)、过拟合和欠拟合识别和处理(identifying and dealing with over and underfitting)、大数据集处理（dealing with large datasets）以及不同代价函数(pros and cons of different loss functions)优缺点等。<br><a id="more"></a></p>
<h1 id="数据可视化"><a href="#数据可视化" class="headerlink" title="数据可视化"></a>数据可视化</h1><h2 id="数据集获取"><a href="#数据集获取" class="headerlink" title="数据集获取"></a>数据集获取</h2><p>　　使用\(sklearn\)自带的\(make\_classification\)方法获取数据。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">from</span> sklearn.datasets <span class="keyword">import</span> make_classification</div><div class="line"><span class="keyword">from</span> pandas <span class="keyword">import</span> DataFrame</div><div class="line">X, y = make_classification(<span class="number">1000</span>, n_features=<span class="number">20</span>, n_informative=<span class="number">2</span>, </div><div class="line">                           n_redundant=<span class="number">2</span>, n_classes=<span class="number">2</span>, random_state=<span class="number">0</span>)</div><div class="line">columns = map(<span class="keyword">lambda</span> i:<span class="string">"col_"</span>+ str(i),range(<span class="number">20</span>)) + [<span class="string">"class"</span>]</div><div class="line">df = DataFrame(np.hstack((X, y[:, <span class="keyword">None</span>])), columns=columns)</div></pre></td></tr></table></figure>
<p>　　我们对二分类问题进行讨论，选取了1000个样本，20个特征。下表是部分数据：<br><img src="/picture/machine-learning/practice-advice1.jpg" alt="practice"><br>　　显然尽管维度很少，直接看这个数据很难得到关于问题的任何有用信息。我们通过可视化数据来发现规律。</p>
<h2 id="可视化"><a href="#可视化" class="headerlink" title="可视化"></a>可视化</h2><p>　　我们使用\(Seaborn\)开源库来进行可视化。<br>　　第一步我们使用pairplot方法来绘制任意两个维度和类别的关系，我们使用前100个数据，5个维度特征来进行绘图。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">_ = sns.pairplot(df[:<span class="number">100</span>], vars=[<span class="string">"col_8"</span>, <span class="string">"col_11"</span>, <span class="string">"col_12"</span>, <span class="string">"col_14"</span>, <span class="string">"col_19"</span>], hue=<span class="string">"class"</span>, size=<span class="number">1.5</span>)</div></pre></td></tr></table></figure></p>
<p><img src="/picture/machine-learning/practice-advice2.png" alt="practice"><br>　　上图25幅图，是5个维度特征两两组合的结果。对角线的柱状图反映了同一个维度不同类别之间取值的差异，从图中可以看出特征11和特征14取值在不同类别间差异显著。再观察散点图，散点图反映了任意两个维度组合特征和类别的关系，我们可以根据是否线性可分或者是否存在明显的相关来判断组合特征在类别判断中是否起到作用。如图特征11和特征14的散点图，我们发现基本上是线性可分的，而特征12和特征19则存在明显的反相关。对于相关性强的特征我们必须舍弃其一，对于和类别相关性强的特征必须保留。<br>　　我们继续观察特征与特征之间以及特征与类别之间的相关性：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">plt.figure(figsize=(<span class="number">12</span>, <span class="number">10</span>))</div><div class="line">plt.xticks(rotation=<span class="number">90</span>)</div><div class="line">_ = sns.heatmap(df.corr()) <span class="comment">#df.corr()是求相关系数函数</span></div></pre></td></tr></table></figure></p>
<p><img src="/picture/machine-learning/practice-advice3.png" alt="practice"><br>　　如上图，我们使用热力图来绘制不同特征之间以及特征与类别之间的相关性。首先看最后一行，反映了类别和不同特征之间的关系。可以看到，特征11和类别关系最密切，即特征11在类别判断中能起到很重要的作用。特征14、12次之。再看特征12和特征19，我们发现存在着明显的反相关，特征11和特征14正相关性也很强。因此存在一些冗余的特征。因为我们很多模型是假设在给定类别的情况下，特征取值之间是独立的，比如朴素贝叶斯。而剩余的其他特征大部分是噪声，既和其他特征不相关，也和类别不相关。</p>
<h1 id="模型初步选择"><a href="#模型初步选择" class="headerlink" title="模型初步选择"></a>模型初步选择</h1><p>　　一旦我们对数据进行可视化完，就可以快速使用模型来进行粗糙的学习(回顾前文提到的bulid-and-fixed方法)。由于机器学习模型多样，有的时候很难决定先用哪一种方法，根据一些总结的经验，我们使用如下图谱入手：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">from</span> IPython.display <span class="keyword">import</span> Image</div><div class="line">Image(filename=<span class="string">'machine-learning-method.png'</span>, width=<span class="number">800</span>, height=<span class="number">600</span>)</div></pre></td></tr></table></figure></p>
<p><img src="/picture/machine-learning/machine-learning-method.png" alt="practice"><br>　　因为我们有1000个样本，并且是有监督分类问题，根据图谱推荐使用\(LinearSVC\)，我们首先使用线性核函数的SVM来尝试建模。回顾一下\(SVM\)的目标函数：<br>$$\min_{\gamma,w,b} \frac{1}{2}{||w||}^2+C \sum_{i=1}^m \zeta_i \\\ 使得, y^{(i)}(w^T x^{(i)} + b) \geq 1-\zeta_i, 　　i=1,…,m \\\ \zeta_i \geq 0,　　i=1,…,m$$<br>　　上式使用的是L2-regularized,L1-loss(\(C \sum_{i=1}^m \zeta_i\))(具体含义参加<a href="/2017/03/28/SVM支持向量机/#软间隔分类器">SVM支持向量机-软间隔分类器一节</a>)。因此penalty=’l2’,loss=’hinge’,即：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line"><span class="comment">#http://scikit-learn.org/stable/modules/generated/sklearn.svm.LinearSVC.html</span></div><div class="line"><span class="keyword">from</span> sklearn.svm <span class="keyword">import</span> LinearSVC</div><div class="line"><span class="comment"># 二者间距较大，存在过拟合的嫌疑，即训练集拟合的很好，分数很高。但是测试集分数很低</span></div><div class="line">plot_learning_curve(LinearSVC(C=<span class="number">10.0</span>,penalty=<span class="string">'l2'</span>,loss=<span class="string">'hinge'</span>), <span class="string">"LinearSVC(C=10.0,penalty='l2',loss='hinge')"</span>,</div><div class="line">                    X, y, ylim=(<span class="number">0.8</span>, <span class="number">1.01</span>),</div><div class="line">                    train_sizes=np.linspace(<span class="number">.05</span>, <span class="number">0.2</span>, <span class="number">5</span>),baseline=<span class="number">0.9</span>)</div></pre></td></tr></table></figure></p>
<p><img src="/picture/machine-learning/practice-advice4.png" alt="practice"><br>　　上式是学习曲线，对应我们之前提到的诊断方法中的方差/误差分析图。我们在下一小节介绍该图的细节。我们现在先关注上图，我们只使用了20%(np.linspace第二个参数)，即200个数据进行训练测试。由图中可以看出，训练分数和泛化分数二者间距较大，并且训练分数处在一个很高的水准，根据之前介绍的偏差方差分析，我们可以得出，上述存在过拟合(over-fitting)的问题。注意，该学习曲线和之前偏差方差分析图存在区别：<br><img src="/picture/machine-learning/advice1.jpg" alt="advice"></p>
<p>　　区别在于，之前使用的是误差，这里使用的是得分。因此测试集和训练集分数曲线相对位置调换，训练集分数曲线在上，测试集分数曲线在下。随着样本的增多，误差曲线下降，这里分数曲线则是上升。但是相同点在于，过拟合图对应的学习曲线，训练分数(误差)和泛化分数(误差)二者间距较大，且训练分数(误差)处在一个高水准。</p>
<h2 id="学习曲线"><a href="#学习曲线" class="headerlink" title="学习曲线"></a>学习曲线</h2><p>　　这里我们先介绍下学习曲线绘制方法。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># http://scikit-learn.org/stable/modules/learning_curve.html#learning-curves</span></div><div class="line"><span class="keyword">from</span> sklearn.learning_curve <span class="keyword">import</span> learning_curve</div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">plot_learning_curve</span><span class="params">(estimator, title, X, y, ylim=None, cv=None,</span></span></div><div class="line">                        train_sizes=np.linspace<span class="params">(<span class="number">.1</span>, <span class="number">1.0</span>, <span class="number">5</span>)</span>,baseline=None):</div><div class="line">    <span class="string">"""</span></div><div class="line">    Generate a simple plot of the test and traning learning curve.</div><div class="line"></div><div class="line">    Parameters</div><div class="line">    ----------</div><div class="line">    estimator : object type that implements the "fit" and "predict" methods</div><div class="line">        An object of that type which is cloned for each validation.</div><div class="line"></div><div class="line">    title : string</div><div class="line">        Title for the chart.</div><div class="line"></div><div class="line">    X : array-like, shape (n_samples, n_features)</div><div class="line">        Training vector, where n_samples is the number of samples and</div><div class="line">        n_features is the number of features.</div><div class="line"></div><div class="line">    y : array-like, shape (n_samples) or (n_samples, n_features), optional</div><div class="line">        Target relative to X for classification or regression;</div><div class="line">        None for unsupervised learning.</div><div class="line"></div><div class="line">    ylim : tuple, shape (ymin, ymax), optional</div><div class="line">        Defines minimum and maximum yvalues plotted.</div><div class="line"></div><div class="line">    cv : integer, cross-validation generator, optional</div><div class="line">        If an integer is passed, it is the number of folds (defaults to 3).</div><div class="line">        Specific cross-validation objects can be passed, see</div><div class="line">        sklearn.cross_validation module for the list of possible objects</div><div class="line">    """</div><div class="line">    </div><div class="line">    plt.figure()</div><div class="line">    train_sizes, train_scores, test_scores = learning_curve(</div><div class="line">        estimator, X, y, cv=<span class="number">5</span>, n_jobs=<span class="number">1</span>, train_sizes=train_sizes)</div><div class="line">    <span class="keyword">print</span> train_sizes</div><div class="line">    <span class="keyword">print</span> <span class="string">'-------------'</span></div><div class="line">    <span class="keyword">print</span> train_scores</div><div class="line">    train_scores_mean = np.mean(train_scores, axis=<span class="number">1</span>)</div><div class="line">    train_scores_std = np.std(train_scores, axis=<span class="number">1</span>)</div><div class="line">    test_scores_mean = np.mean(test_scores, axis=<span class="number">1</span>)</div><div class="line">    test_scores_std = np.std(test_scores, axis=<span class="number">1</span>)</div><div class="line"></div><div class="line">    plt.fill_between(train_sizes, train_scores_mean - train_scores_std,</div><div class="line">                     train_scores_mean + train_scores_std, alpha=<span class="number">0.1</span>,</div><div class="line">                     color=<span class="string">"r"</span>)</div><div class="line">    plt.fill_between(train_sizes, test_scores_mean - test_scores_std,</div><div class="line">                     test_scores_mean + test_scores_std, alpha=<span class="number">0.1</span>, color=<span class="string">"g"</span>)</div><div class="line">    plt.plot(train_sizes, train_scores_mean, <span class="string">'o-'</span>, color=<span class="string">"r"</span>,</div><div class="line">             label=<span class="string">"Training score"</span>)</div><div class="line">    plt.plot(train_sizes, test_scores_mean, <span class="string">'o-'</span>, color=<span class="string">"b"</span>,</div><div class="line">             label=<span class="string">"Cross-validation score"</span>)</div><div class="line"></div><div class="line">    <span class="keyword">if</span> baseline:</div><div class="line">        plt.axhline(y=baseline,color=<span class="string">'red'</span>,linewidth=<span class="number">5</span>,label=<span class="string">'Desired Performance'</span>) <span class="comment">#baseline</span></div><div class="line">    plt.xlabel(<span class="string">"Training examples"</span>)</div><div class="line">    plt.ylabel(<span class="string">"Score"</span>)</div><div class="line">    plt.legend(loc=<span class="string">"best"</span>)</div><div class="line">    plt.grid(<span class="string">"on"</span>) </div><div class="line">    <span class="keyword">if</span> ylim:</div><div class="line">        plt.ylim(ylim)</div><div class="line">    plt.title(title)</div></pre></td></tr></table></figure></p>
<p>　　简要解释下几个重要点。首先是参数，estimator代表模型，title标题，X是样本数据集，y是标签集，ylim是学习曲线y轴的取值范围(min,max)，cv是交叉验证折数，train_sizes=np.linspace(.1, 1.0, 5)代表划分训练集，np.linspace(.1, 1.0, 5)返回的结果[ 0.1  ,  0.325,  0.55 ,  0.775,  1.   ]，即等间隔划分数据集，第一个参数是起始，第二个参数是终点，最后一个参数是划分份数。因为学习曲线的x轴代表样本的数量，即画出指标在训练集和验证集上样本数量变化的情况。我们不可能对每个样本量取值(从1一直递增到1000)都进行绘图，即不能画出平滑的曲线，而是取一些关键的点进行训练绘图，上述得到的train_sizes就是每次训练的样本占总样本的比例的数组。<br>　　接着是重要的一些代码。train_sizes, train_scores, test_scores = learning_curve(estimator, X, y, cv=5, n_jobs=1, train_sizes=train_sizes)返回的train_sizes是根据传入的train_sizes比例数组计算的实际训练样本数量数组。train_scores是训练集的得分，是一个二维数组，第一维等于train_sizes数组大小,即每次训练的分数，第二维等于交叉验证份数cv,即每次交叉验证的得分数组。test_scores是测试集的得分。因此可以取平均进行绘图，plt.fill_between方法是图中阴影的部分。</p>
<h1 id="过拟合处理"><a href="#过拟合处理" class="headerlink" title="过拟合处理"></a>过拟合处理</h1><p>　　有许多方法可以解决过拟合问题。</p>
<h2 id="增加样本数量"><a href="#增加样本数量" class="headerlink" title="增加样本数量"></a>增加样本数量</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">plot_learning_curve(LinearSVC(C=<span class="number">10.0</span>,penalty=<span class="string">'l2'</span>,loss=<span class="string">'hinge'</span>), <span class="string">"LinearSVC(C=10.0,penalty='l2',loss='hinge')"</span>,</div><div class="line">                    X, y, ylim=(<span class="number">0.8</span>, <span class="number">1.01</span>),</div><div class="line">                    train_sizes=np.linspace(<span class="number">.1</span>, <span class="number">1.0</span>, <span class="number">5</span>), baseline=<span class="number">0.9</span>)</div></pre></td></tr></table></figure>
<p><img src="/picture/machine-learning/practice-advice5.png" alt="practice"><br>　　这里修改linspace第二个参数为1，使用全部样本进行训练。我们发现泛化分数随着样本的增多不断增大，并且泛化分数和训练分数的间距不断缩小。但是高偏差的时候间距也是小的，我们继续进一步判断，发现训练分数和泛化分数都处在一个较高的水准，高于期望分数，而高偏差时，训练分数和泛化分数都比较低，低于理想分数。因此此时不存在过拟合或欠拟合的问题。</p>
<h2 id="减少特征"><a href="#减少特征" class="headerlink" title="减少特征"></a>减少特征</h2><p>　　根据前面的可视化分析，我们发现特征11和14和类别关联紧密，因此可以考虑先手动选择这两种特征进行训练。同样只在20%的样本上进行训练：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">plot_learning_curve(LinearSVC(C=<span class="number">10.0</span>,penalty=<span class="string">'l2'</span>,loss=<span class="string">'hinge'</span>), <span class="string">"LinearSVC(C=10.0,penalty='l2',loss='hinge') Features: 11&amp;14"</span>,</div><div class="line">                    df[[<span class="string">"col_11"</span>, <span class="string">"col_14"</span>]], y, ylim=(<span class="number">0.8</span>, <span class="number">1.0</span>),</div><div class="line">                    train_sizes=np.linspace(<span class="number">.05</span>, <span class="number">0.2</span>, <span class="number">5</span>),baseline=<span class="number">0.9</span>)</div></pre></td></tr></table></figure></p>
<p><img src="/picture/machine-learning/practice-advice6.png" alt="practice">　<br>　　和最早的那幅过拟合图相比，这里的结果已经好很多，基本上解决了过拟合的问题。但是这里的特征选择方法有点作弊嫌疑，首先是因为手动选择的，其次是因为我们是在1000个样本上进行选择的，而我们最终却只使用200个样本进行训练绘图。下面进行特征自动选择：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">from</span> sklearn.pipeline <span class="keyword">import</span> Pipeline</div><div class="line"><span class="keyword">from</span> sklearn.feature_selection <span class="keyword">import</span> SelectKBest, f_classif</div><div class="line"><span class="comment"># SelectKBest(f_classif, k=2) will select the k=2 best features according to their Anova F-value</span></div><div class="line">plot_learning_curve(Pipeline([(<span class="string">"fs"</span>, SelectKBest(f_classif, k=<span class="number">2</span>)), <span class="comment"># select two features</span></div><div class="line">                               (<span class="string">"svc"</span>, LinearSVC(C=<span class="number">10.0</span>,penalty=<span class="string">'l2'</span>,loss=<span class="string">'hinge'</span>))]),</div><div class="line">                    <span class="string">"SelectKBest(f_classif, k=2) + LinearSVC(C=10.0,penalty='l2',loss='hinge')"</span>,</div><div class="line">                    X, y, ylim=(<span class="number">0.8</span>, <span class="number">1.0</span>),</div><div class="line">                    train_sizes=np.linspace(<span class="number">.05</span>, <span class="number">0.2</span>, <span class="number">5</span>),baseline=<span class="number">0.9</span>)</div></pre></td></tr></table></figure></p>
<p><img src="/picture/machine-learning/practice-advice7.png" alt="practice">　<br>　　上述使用\(SelectKBest\)选择2个特征，我们发现在这个数据集上特征选择表现很好。注意，这种特征选择方法只是减少模型复杂度的一种方法。其他方法还包括，减少线性回归中多项式的阶数，减少神经网络中隐藏层的数量和节点数，增加高斯核函数的bandwidth(\(\sigma\)),或减小\(\gamma\)等(参考<a href="http://blog.csdn.net/wusecaiyun/article/details/49681431?locationNum=4" target="_blank" rel="external">SVM C和gamma参数理解</a>)。</p>
<h2 id="修改目标函数正则化项"><a href="#修改目标函数正则化项" class="headerlink" title="修改目标函数正则化项"></a>修改目标函数正则化项</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line"><span class="comment">#C表征了对离群点的重视程度，越大越重视，越大越容易过拟合。</span></div><div class="line"><span class="comment">#减小C可以一定程度上解决过拟合</span></div><div class="line">plot_learning_curve(LinearSVC(C=<span class="number">0.1</span>,penalty=<span class="string">'l2'</span>,loss=<span class="string">'hinge'</span>), <span class="string">"LinearSVC(C=0.1,penalty='l2',loss='hinge')"</span>, </div><div class="line">                    X, y, ylim=(<span class="number">0.8</span>, <span class="number">1.01</span>),</div><div class="line">                    train_sizes=np.linspace(<span class="number">.05</span>, <span class="number">0.2</span>, <span class="number">5</span>),baseline=<span class="number">0.9</span>)</div></pre></td></tr></table></figure>
<p><img src="/picture/machine-learning/practice-advice8.png" alt="practice">　<br>　　惩罚因子\(C\)决定了你有多重视离群点带来的损失，显然当所有离群点的松弛变量(\(\zeta\))的和一定时，你定的C越大，对目标函数的损失也越大，此时就暗示着你非常不愿意放弃这些离群点，最极端的情况是你把C定为无限大，这样只要稍有一个点离群，目标函数的值马上变成无限大，马上让问题变成无解，这就退化成了硬间隔问题，即C越大，你越希望在训练数据上少犯错误，而实际上这是不可能/没有意义的，于是就造成过拟合。<br>　　因此这里减少\(C\)能够一定程度上减少过拟合。<br>　　我们可以使用网格搜索来寻找最佳C。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line"><span class="comment">#使用网格搜索</span></div><div class="line"><span class="keyword">from</span> sklearn.grid_search <span class="keyword">import</span> GridSearchCV</div><div class="line">est = GridSearchCV(LinearSVC(penalty=<span class="string">'l2'</span>,loss=<span class="string">'hinge'</span>), </div><div class="line">                   param_grid=&#123;<span class="string">"C"</span>: [<span class="number">0.0001</span>,<span class="number">0.001</span>, <span class="number">0.01</span>, <span class="number">0.1</span>, <span class="number">1.0</span>, <span class="number">10.0</span>]&#125;)</div><div class="line">plot_learning_curve(est, <span class="string">"LinearSVC(C=AUTO)"</span>, </div><div class="line">                    X, y, ylim=(<span class="number">0.8</span>, <span class="number">1.0</span>),</div><div class="line">                    train_sizes=np.linspace(<span class="number">.05</span>, <span class="number">0.2</span>, <span class="number">5</span>),baseline=<span class="number">0.9</span>)</div><div class="line"><span class="keyword">print</span> <span class="string">"Chosen parameter on 100 datapoints: %s"</span> % est.fit(X[:<span class="number">100</span>], y[:<span class="number">100</span>]).best_params_</div></pre></td></tr></table></figure></p>
<p>输出结果：<strong>Chosen parameter on 100 datapoints: {‘C’: 0.01}</strong><br><img src="/picture/machine-learning/practice-advice9.png" alt="practice">　<br>　　特征选择看起来比修改正则化系数来的好。还有一种正则化方法，将LinearSVC的penalty设置为L1,官方文档解释为<strong>The ‘l1’ leads to coef_ vectors that are sparse</strong>,即L1可以导致稀疏参数矩阵，参数为0的特征不起作用，则相当于隐含的特征选择。不过注意,LinearSVC不支持L1-regularized和L1-loss,L1-regularized对应penalty=’l1’,L1-loss对应loss=’hinge’。可参考<a href="https://www.quora.com/Support-Vector-Machines/Liblinear-does-not-support-L1-regularized-L1-loss-hinge-loss-support-vector-classification-Why" target="_blank" rel="external">Liblinear does not support L1-regularized L1-loss ( hinge loss ) support vector classification. Why?</a>因此需要把loss改成’squared_hinge’。另外，此时不能用对偶问题来解决。故dual=False。<br><img src="/picture/machine-learning/practice-advice10.png" alt="practice">　<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">plot_learning_curve(LinearSVC(C=<span class="number">0.1</span>, penalty=<span class="string">'l1'</span>, loss=<span class="string">'squared_hinge'</span>,dual=<span class="keyword">False</span>), </div><div class="line">                    <span class="string">"LinearSVC(C=0.1, penalty='l1')"</span>, </div><div class="line">                    X, y, ylim=(<span class="number">0.8</span>, <span class="number">1.0</span>),</div><div class="line">                    train_sizes=np.linspace(<span class="number">.05</span>, <span class="number">0.2</span>, <span class="number">5</span>),baseline=<span class="number">0.9</span>)</div></pre></td></tr></table></figure></p>
<p><img src="/picture/machine-learning/practice-advice11.png" alt="practice">　<br>　　结果看起来不错。<br>　　学习到的参数如下：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">est = LinearSVC(C=<span class="number">0.1</span>, penalty=<span class="string">'l1'</span>, loss=<span class="string">'squared_hinge'</span>,dual=<span class="keyword">False</span>)</div><div class="line">est.fit(X[:<span class="number">150</span>], y[:<span class="number">150</span>])  <span class="comment"># fit on 150 datapoints</span></div><div class="line"><span class="keyword">print</span> <span class="string">"Coefficients learned: %s"</span> % est.coef_</div><div class="line"><span class="keyword">print</span> <span class="string">"Non-zero coefficients: %s"</span> % np.nonzero(est.coef_)[<span class="number">1</span>]</div></pre></td></tr></table></figure></p>
<p><img src="/picture/machine-learning/practice-advice12.png" alt="practice">　<br>　　可以看到特征11的权重最大，即最重要。</p>
<h1 id="欠拟合处理"><a href="#欠拟合处理" class="headerlink" title="欠拟合处理"></a>欠拟合处理</h1><p>　　之前使用的数据集分类结果都比较理想，我们尝试使用另一个二分类数据集。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">from</span> sklearn.datasets <span class="keyword">import</span> make_circles</div><div class="line">X, y = make_circles(n_samples=<span class="number">1000</span>, random_state=<span class="number">2</span>)<span class="comment">#只有2个特征</span></div><div class="line">plot_learning_curve(LinearSVC(C=<span class="number">0.25</span>), <span class="string">"LinearSVC(C=0.25)"</span>, </div><div class="line">                    X, y, ylim=(<span class="number">0.4</span>, <span class="number">1.0</span>),</div><div class="line">                    train_sizes=np.linspace(<span class="number">.1</span>, <span class="number">1.0</span>, <span class="number">5</span>))<span class="comment">#效果非常差</span></div></pre></td></tr></table></figure></p>
<p><img src="/picture/machine-learning/practice-advice13.png" alt="practice"><br>　　由上图可以看出，训练分数和泛化分数差距很小，并且训练分数明显低于期望分数。根据之前的方差/偏差分析可知，这里存在着明显的偏差，即欠拟合问题。<br>　　我们首先对数据进行可视化观察：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># 环形数据，外圈的数据是一种类别，内圈的数据是一种类别</span></div><div class="line">columns = map(<span class="keyword">lambda</span> i:<span class="string">"col_"</span>+ str(i),range(<span class="number">2</span>)) + [<span class="string">"class"</span>]</div><div class="line">df = DataFrame(np.hstack((X, y[:, <span class="keyword">None</span>])), </div><div class="line">               columns = columns)</div><div class="line">_ = sns.pairplot(df, vars=[<span class="string">"col_0"</span>, <span class="string">"col_1"</span>], hue=<span class="string">"class"</span>, size=<span class="number">3.5</span>)</div></pre></td></tr></table></figure></p>
<p><img src="/picture/machine-learning/practice-advice14.png" alt="practice"><br>　　根据上图，该数据集是环形数据，外圈的点代表一种类别，内圈的点代表另一种类别。显然上述数据是线性不可分的，使用再多数据或者减少特征都没用，我们的模型是错误的，需要进行欠拟合处理。</p>
<h2 id="增加或使用更好的特征"><a href="#增加或使用更好的特征" class="headerlink" title="增加或使用更好的特征"></a>增加或使用更好的特征</h2><p>　　我们尝试增加特征，根据散点图，显然不同类别距离原点的距离不同，我们可以增加到原点的距离这一特征。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line"><span class="comment">#解决欠拟合方法1：增加特征</span></div><div class="line"><span class="comment"># X[:, [0]]**2 + X[:, [1]]**2)计算的是离原点的距离</span></div><div class="line">X_orginal_distance = X[:, [<span class="number">0</span>]]**<span class="number">2</span> + X[:, [<span class="number">1</span>]]**<span class="number">2</span><span class="comment">#X[:, [0]]将得到的列数据变成二维的形式，[[  8.93841424e-01],[ -7.63891636e-01]...]</span></div><div class="line">df[<span class="string">'col_3'</span>] = X_orginal_distance </div><div class="line"><span class="comment">#可以看到完全线性可分</span></div><div class="line">_ = sns.pairplot(df, vars=[<span class="string">"col_0"</span>, <span class="string">"col_1"</span>,<span class="string">"col_3"</span>], hue=<span class="string">"class"</span>, size=<span class="number">3.5</span>)</div></pre></td></tr></table></figure></p>
<p><img src="/picture/machine-learning/practice-advice15.png" alt="practice"><br>　　由最后一幅图，我们发现根据col_3新特征，就能将类别完全线性分隔开，因此col_3特征在区分类别上能起决定性作用。不妨看看热力图：<br><img src="/picture/machine-learning/practice-advice16.png" alt="practice"><br>　　根据热力图，我们发现col_3和类别存在着非常强的负相关性。使用新增完的特征集进行预测：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">X_extra = np.hstack((X,X[:,[<span class="number">0</span>]]**<span class="number">2</span>+X[:,[<span class="number">1</span>]]**<span class="number">2</span>))</div><div class="line">plot_learning_curve(LinearSVC(C=<span class="number">10</span>,penalty=<span class="string">'l2'</span>,loss=<span class="string">'hinge'</span>), <span class="string">"LinearSVC(C=10,penalty='l2',loss='hinge')"</span>, </div><div class="line">                    X_extra, y, ylim=(<span class="number">0</span>, <span class="number">1.01</span>),</div><div class="line">                    train_sizes=np.linspace(<span class="number">.1</span>, <span class="number">1.0</span>, <span class="number">5</span>),baseline=<span class="number">0.9</span>)</div></pre></td></tr></table></figure></p>
<p><img src="/picture/machine-learning/practice-advice17.png" alt="practice"><br>　　根据结果，完全分开了样本。我们可以进一步思考，是否可以让模型进行自动生成新特征？</p>
<h2 id="使用更复杂的模型"><a href="#使用更复杂的模型" class="headerlink" title="使用更复杂的模型"></a>使用更复杂的模型</h2><p>　　<strong>使用复杂的模型，相当于更换了目标函数</strong>。根据上面数据集非线性可分的特点，我们可尝试非线性分类器，使用RBF核的SVM进行分类。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">from</span> sklearn.svm <span class="keyword">import</span> SVC</div><div class="line"><span class="comment"># note: we use the original X without the extra feature</span></div><div class="line"><span class="comment"># 使用RBF核，最小间隔gamma设为1.</span></div><div class="line">plot_learning_curve(SVC(C=<span class="number">10</span>, kernel=<span class="string">"rbf"</span>, gamma=<span class="number">1.0</span>),</div><div class="line">                    <span class="string">"SVC(C=10, kernel='rbf', gamma=1.0)"</span>,</div><div class="line">                    X, y, ylim=(<span class="number">0.5</span>, <span class="number">1.1</span>), </div><div class="line">                    train_sizes=np.linspace(<span class="number">.1</span>, <span class="number">1.0</span>, <span class="number">5</span>),baseline=<span class="number">0.9</span>)</div></pre></td></tr></table></figure></p>
<p><img src="/picture/machine-learning/practice-advice18.png" alt="practice"><br>　　注意上述建模使用的是原始数据集X，而没有用新的特征。可以发现结果很理想，RBF核会将特征映射到高维空间，因此得到的非线性模型效果很好。</p>
<h1 id="大数据集和高维特征处理"><a href="#大数据集和高维特征处理" class="headerlink" title="大数据集和高维特征处理"></a>大数据集和高维特征处理</h1><h2 id="SGDClassfier增量学习"><a href="#SGDClassfier增量学习" class="headerlink" title="SGDClassfier增量学习"></a>SGDClassfier增量学习</h2><p>　　如果数据集增大，特征增多，那么上述SVM运行会变慢很多。根据之前的图谱推荐，此时可以使用\(SGDClassifier\)，该分类器也是一个线性模型,但是使用随机梯度下降法(stochastic gradient descent),\(SGDClassifier\)对特征缩放很敏感，因此可以考虑标准化数据集，使特征均值为0，方差为1.<br>　　\(SGDClassifier\)允许增量学习，会在线学习，在数据量很大的时候很有用。此时不适合采用交叉验证，我们采取progressive validation方法，即将数据集等分成块，每次在前一块训练，在后一块验证，并且使用增量学习，后面块的学习是在前面块学习的基础上继续学习的。　<br>　　首先生成数据，20万+200特征+10个类别。　<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">X, y = make_classification(<span class="number">200000</span>, n_features=<span class="number">200</span>, n_informative=<span class="number">25</span>, </div><div class="line">                           n_redundant=<span class="number">0</span>, n_classes=<span class="number">10</span>, class_sep=<span class="number">2</span>,</div><div class="line">                           random_state=<span class="number">0</span>)</div></pre></td></tr></table></figure></p>
<p>　　建模和验证：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span> SGDClassifier</div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">sgd_score</span><span class="params">(X,y)</span>:</span></div><div class="line">    est = SGDClassifier(penalty=<span class="string">"l2"</span>, alpha=<span class="number">0.001</span>)</div><div class="line">    progressive_validation_score = []</div><div class="line">    train_score = []</div><div class="line">    <span class="keyword">for</span> datapoint <span class="keyword">in</span> range(<span class="number">0</span>, <span class="number">199000</span>, <span class="number">1000</span>):</div><div class="line">        X_batch = X[datapoint:datapoint+<span class="number">1000</span>]</div><div class="line">        y_batch = y[datapoint:datapoint+<span class="number">1000</span>]</div><div class="line">        <span class="keyword">if</span> datapoint &gt; <span class="number">0</span>:</div><div class="line">            progressive_validation_score.append(est.score(X_batch, y_batch))</div><div class="line">        est.partial_fit(X_batch, y_batch, classes=range(<span class="number">10</span>)) <span class="comment">#增量学习或称为在线学习</span></div><div class="line">        <span class="keyword">if</span> datapoint &gt; <span class="number">0</span>:</div><div class="line">            train_score.append(est.score(X_batch, y_batch))</div><div class="line">            </div><div class="line">    plt.plot(train_score, label=<span class="string">"train score"</span>,color=<span class="string">'blue'</span>)</div><div class="line">    plt.plot(progressive_validation_score, label=<span class="string">"progressive validation score"</span>,color=<span class="string">'red'</span>)</div><div class="line">    plt.xlabel(<span class="string">"Mini-batch"</span>)</div><div class="line">    plt.ylabel(<span class="string">"Score"</span>)</div><div class="line">    plt.axhline(y=<span class="number">0.8</span>,color=<span class="string">'red'</span>,linewidth=<span class="number">5</span>,label=<span class="string">'Desired Performance'</span>) <span class="comment">#baseline</span></div><div class="line">    plt.legend(loc=<span class="string">'best'</span>)</div><div class="line">sgd_score(X,y)</div></pre></td></tr></table></figure></p>
<p><img src="/picture/machine-learning/practice-advice19.png" alt="practice"><br>　　上图表明，在50次mini-batches滞后，分数提高就很少了，因此我们可以提前停止训练。由于训练分数和泛化分数差距很小，其训练分数较低，因此可能存在欠拟合的可能。<br>然而SGDClassifier不支持核技巧，根据图谱可以使用kernel approximation。<br>　　The advantage of using approximate explicit feature maps compared to the kernel trick, which makes use of feature maps implicitly, is that explicit mappings can be better suited for online learning and can significantly reduce the cost of learning with very large datasets. The combination of kernel map approximations with SGDClassifier can make non-linear learning on large datasets possible.<br>　　相较于核函数隐示的映射，kernel approximation使用显示的映射方法，这对在线学习非常重要，可以减少超大数据集的学习代价。使用SGDClassifier配合kernel approximation可以在大数据集上实现非线性学习的目的。</p>
<h2 id="手写体数字识别"><a href="#手写体数字识别" class="headerlink" title="手写体数字识别"></a>手写体数字识别</h2><p>　　现在尝试对手写体数字问题进行建模。</p>
<h3 id="可视化-1"><a href="#可视化-1" class="headerlink" title="可视化"></a>可视化</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># http://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_digits.html#sklearn.datasets.load_digits</span></div><div class="line"><span class="keyword">from</span> sklearn.datasets <span class="keyword">import</span> load_digits</div><div class="line">digits = load_digits(n_class=<span class="number">6</span>)</div><div class="line">X = digits.data</div><div class="line">y = digits.target</div><div class="line">n_samples, n_features = X.shape</div><div class="line"><span class="keyword">print</span> <span class="string">"Dataset consist of %d samples with %d features each"</span> % (n_samples, n_features)</div><div class="line"></div><div class="line"><span class="comment"># Plot images of the digits</span></div><div class="line">n_img_per_row = <span class="number">20</span> <span class="comment">#最大为32，即展示1024个样本</span></div><div class="line">img = np.zeros((<span class="number">10</span> * n_img_per_row, <span class="number">10</span> * n_img_per_row)) <span class="comment"># 200*200规格的像素矩阵</span></div><div class="line"></div><div class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(n_img_per_row):</div><div class="line">    ix = <span class="number">10</span> * i + <span class="number">1</span> <span class="comment">#空1个像素点</span></div><div class="line">    <span class="keyword">for</span> j <span class="keyword">in</span> range(n_img_per_row):</div><div class="line">        iy = <span class="number">10</span> * j + <span class="number">1</span></div><div class="line">        img[ix:ix + <span class="number">8</span>, iy:iy + <span class="number">8</span>] = X[i * n_img_per_row + j].reshape((<span class="number">8</span>, <span class="number">8</span>))<span class="comment">#1行64个特征是通过8*8展平的,存入分块矩阵</span></div><div class="line"></div><div class="line">plt.imshow(img, cmap=plt.cm.binary)</div><div class="line">plt.xticks([])</div><div class="line">plt.yticks([])</div><div class="line">_ = plt.title(<span class="string">'A selection from the 8*8=64-dimensional digits dataset'</span>)</div></pre></td></tr></table></figure>
<p><img src="/picture/machine-learning/practice-advice20.png" alt="practice"><br>　　手写体数字的64维特征就是一个8*8数字图片每个像素点平铺开来的。因此我们可以通过上面代码进行重建图片。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">print</span> digits.images.shape <span class="comment">#三维数组(1083L, 8L, 8L)，1083个样本</span></div><div class="line"><span class="keyword">print</span> img.shape <span class="comment">#二维数组(200L,200L),每个样本占8*8小分块矩阵。每8行20个样本，一共可以放400个样本。</span></div><div class="line"><span class="comment">#可以扩大该二维数组，例如(320L,320L), 每个样本占8*8小分块矩阵， 每8行展示32个样本，最大可以展示1024个样本。即32*32</span></div><div class="line"><span class="comment"># digits.images[0] == img[1:9,1:9]</span></div><div class="line"><span class="comment"># digits.images[1] == img[1:9,11:19]</span></div><div class="line">plt.matshow(digits.images[<span class="number">1</span>],cmap=plt.cm.gray)  <span class="comment">#第二个样本为数字1</span></div><div class="line">plt.matshow(img[<span class="number">1</span>:<span class="number">9</span>,<span class="number">11</span>:<span class="number">19</span>],cmap=plt.cm.gray)  <span class="comment">#第二个样本数字1</span></div></pre></td></tr></table></figure></p>
<p><img src="/picture/machine-learning/practice-advice21.png" alt="practice"><br>　　上述代码展示一个数字的结果，可以发现digits.images<a href="/2017/04/01/ml-advice/">1</a>和img[1:9,11:19]都是代表第二个样本，我们可以从图中看出第二个样本数字是1。<br>　　进一步可视化：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># Helper function based on </span></div><div class="line"><span class="comment"># http://scikit-learn.org/stable/auto_examples/manifold/plot_lle_digits.html#example-manifold-plot-lle-digits-py</span></div><div class="line"><span class="comment"># 我们之前已经讨论过手写数字的数据，每个手写的阿拉伯数字被表达为一个8*8的像素矩阵，</span></div><div class="line"><span class="comment"># 我们曾经使用每个像素点，也就是64个特征，使用logistic和knn的方法（分类器）去根据训练集判别测试集中的数字。</span></div><div class="line"><span class="comment"># 在这种做法中，我们使用了尚未被降维的数据。其实我们还可以使用降维后的数据来训练分类器。</span></div><div class="line"><span class="comment"># 现在，就让我们看一下对这个数据集采取各种方式降维的效果。</span></div><div class="line"><span class="keyword">from</span> matplotlib <span class="keyword">import</span> offsetbox</div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">plot_embedding</span><span class="params">(X, title=None)</span>:</span></div><div class="line">    x_min, x_max = np.min(X, <span class="number">0</span>), np.max(X, <span class="number">0</span>)</div><div class="line">    X = (X - x_min) / (x_max - x_min)</div><div class="line"></div><div class="line">    plt.figure(figsize=(<span class="number">10</span>, <span class="number">10</span>))</div><div class="line">    ax = plt.subplot(<span class="number">111</span>)</div><div class="line">    </div><div class="line">    <span class="comment"># 绘制每个样本这两个维度的值以及实际的数字</span></div><div class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(X.shape[<span class="number">0</span>]):</div><div class="line">        plt.text(X[i, <span class="number">0</span>], X[i, <span class="number">1</span>], str(digits.target[i]),</div><div class="line">                 color=plt.cm.Set1(y[i] / <span class="number">10.</span>),</div><div class="line">                 fontdict=&#123;<span class="string">'weight'</span>: <span class="string">'bold'</span>, <span class="string">'size'</span>: <span class="number">12</span>&#125;)</div><div class="line"></div><div class="line">    <span class="keyword">if</span> hasattr(offsetbox, <span class="string">'AnnotationBbox'</span>):</div><div class="line">        <span class="comment"># only print thumbnails with matplotlib &gt; 1.0</span></div><div class="line">        shown_images = np.array([[<span class="number">1.</span>, <span class="number">1.</span>]])  <span class="comment"># 定义一个标准点</span></div><div class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(digits.data.shape[<span class="number">0</span>]):<span class="comment">#样本数</span></div><div class="line">            dist = np.sum((X[i] - shown_images) ** <span class="number">2</span>,axis=<span class="number">1</span>)<span class="comment">#计算要展示的点和目前所有的点的距离，</span></div><div class="line">            <span class="comment">#axis=1代表横着加，即每个样本x^2+y^2; 得到该样本和所有的点的距离的数组;axis=0，按列加，就变成了把每个样本的x^2全加起来，y^2全部加起来。</span></div><div class="line">            <span class="keyword">if</span> np.min(dist) &lt; <span class="number">4e-3</span>: <span class="comment">#选择最近的距离</span></div><div class="line">                <span class="keyword">continue</span> <span class="comment"># don't show points that are too close</span></div><div class="line">            shown_images = np.r_[shown_images, [X[i]]] <span class="comment"># 纵向合并</span></div><div class="line">            imagebox = offsetbox.AnnotationBbox(</div><div class="line">                offsetbox.OffsetImage(digits.images[i], cmap=plt.cm.gray_r),</div><div class="line">                X[i])<span class="comment">#X[i]代表每个样本的两个维度的值，即横轴和纵轴的值，即两个维度决定的位置画出灰度图</span></div><div class="line">            ax.add_artist(imagebox)</div><div class="line">    plt.xticks([]), plt.yticks([])</div><div class="line">    <span class="keyword">if</span> title <span class="keyword">is</span> <span class="keyword">not</span> <span class="keyword">None</span>:</div><div class="line">        plt.title(title)</div></pre></td></tr></table></figure></p>
<h3 id="降维"><a href="#降维" class="headerlink" title="降维"></a>降维</h3><p><strong>随机降维</strong><br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line"><span class="comment">#降维——随机投影</span></div><div class="line"><span class="comment">#把64维数据随机地投影到二维上</span></div><div class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> (manifold, datasets, decomposition, ensemble,</div><div class="line">                     discriminant_analysis, random_projection)</div><div class="line">rp = random_projection.SparseRandomProjection(n_components=<span class="number">2</span>, random_state=<span class="number">42</span>)<span class="comment">#随机投影到两个维度</span></div><div class="line">stime = time.time()</div><div class="line">X_projected = rp.fit_transform(X)</div><div class="line">plot_embedding(X_projected, <span class="string">"Random Projection of the digits (time: %.3fs)"</span> % (time.time() - stime))</div></pre></td></tr></table></figure></p>
<p><img src="/picture/machine-learning/practice-advice22.png" alt="practice"></p>
<p><strong>PCA降维</strong><br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># PCA降维</span></div><div class="line"><span class="comment"># linear线性降维</span></div><div class="line"><span class="comment"># TruncatedSVD是pca的一种方式，不需要计算协方差矩阵，适用于稀疏矩阵</span></div><div class="line"><span class="comment"># PCA for dense data or TruncatedSVD for sparse data</span></div><div class="line"><span class="comment">#implemented using a TruncatedSVD which does not require constructing the covariance matrix</span></div><div class="line"><span class="comment"># LSA的基本思想就是，将document从稀疏的高维Vocabulary空间映射到一个低维的向量空间，我们称之为隐含语义空间(Latent Semantic Space).</span></div><div class="line">X_pca = decomposition.TruncatedSVD(n_components=<span class="number">2</span>).fit_transform(X)</div><div class="line">stime = time.time()</div><div class="line">plot_embedding(X_pca,<span class="string">"Principal Components projection of the digits (time: %.3fs)"</span> % (time.time() - stime))</div></pre></td></tr></table></figure></p>
<p><img src="/picture/machine-learning/practice-advice23.png" alt="practice"></p>
<p><strong>LDA线性变换</strong><br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line">print(<span class="string">"Computing Linear Discriminant Analysis projection"</span>)</div><div class="line">X2 = X.copy()</div><div class="line">X2.flat[::X.shape[<span class="number">1</span>] + <span class="number">1</span>] += <span class="number">0.01</span>  <span class="comment"># Make X invertible</span></div><div class="line">stime = time.time()</div><div class="line">X_lda = discriminant_analysis.LinearDiscriminantAnalysis(n_components=<span class="number">2</span>).fit_transform(X2, y)</div><div class="line">plot_embedding(X_lda,</div><div class="line">               <span class="string">"Linear Discriminant projection of the digits (time %.2fs)"</span> %</div><div class="line">               (time.time() - stime))</div></pre></td></tr></table></figure></p>
<p><img src="/picture/machine-learning/practice-advice24.png" alt="practice"></p>
<p><strong>t-SNE非线性变换</strong><br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line"><span class="comment">#http://scikit-learn.org/stable/modules/generated/sklearn.manifold.TSNE.html#sklearn.manifold.TSNE</span></div><div class="line"><span class="comment">#非线性的变换</span></div><div class="line"><span class="comment">#最小化KL距离，Kullback-Leibler </span></div><div class="line">tsne = manifold.TSNE(n_components=<span class="number">2</span>, init=<span class="string">'pca'</span>, random_state=<span class="number">0</span>)</div><div class="line">stime = time.time()</div><div class="line">X_tsne = tsne.fit_transform(X)</div><div class="line">plot_embedding(X_tsne,</div><div class="line">               <span class="string">"t-SNE embedding of the digits (time: %.3fs)"</span> % (time.time() - stime))</div></pre></td></tr></table></figure></p>
<p><img src="/picture/machine-learning/practice-advice25.png" alt="practice"><br>　　可以发现，在该数据集上，非线性变换的结果比线性变换的结果更理想。</p>
<h1 id="目标函数选择"><a href="#目标函数选择" class="headerlink" title="目标函数选择"></a>目标函数选择</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># adapted from http://scikit-learn.org/stable/auto_examples/linear_model/plot_sgd_loss_functions.html</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">modified_huber_loss</span><span class="params">(y_true, y_pred)</span>:</span></div><div class="line">    z = y_pred * y_true</div><div class="line">    loss = <span class="number">-4</span> * z</div><div class="line">    loss[z &gt;= <span class="number">-1</span>] = (<span class="number">1</span> - z[z &gt;= <span class="number">-1</span>]) ** <span class="number">2</span></div><div class="line">    loss[z &gt;= <span class="number">1.</span>] = <span class="number">0</span></div><div class="line">    <span class="keyword">return</span> loss</div><div class="line">xmin, xmax = <span class="number">-4</span>, <span class="number">4</span></div><div class="line">xx = np.linspace(xmin, xmax, <span class="number">100</span>)</div><div class="line">lw = <span class="number">2</span></div><div class="line">plt.plot([xmin, <span class="number">0</span>, <span class="number">0</span>, xmax], [<span class="number">1</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>], color=<span class="string">'gold'</span>, lw=lw,</div><div class="line">         label=<span class="string">"Zero-one loss"</span>)</div><div class="line">plt.plot(xx, np.where(xx &lt; <span class="number">1</span>, <span class="number">1</span> - xx, <span class="number">0</span>), color=<span class="string">'teal'</span>, lw=lw,</div><div class="line">         label=<span class="string">"Hinge loss"</span>)</div><div class="line">plt.plot(xx, -np.minimum(xx, <span class="number">0</span>), color=<span class="string">'yellowgreen'</span>, lw=lw,</div><div class="line">         label=<span class="string">"Perceptron loss"</span>)</div><div class="line">plt.plot(xx, np.log2(<span class="number">1</span> + np.exp(-xx)), color=<span class="string">'cornflowerblue'</span>, lw=lw,</div><div class="line">         label=<span class="string">"Log loss"</span>)</div><div class="line">plt.plot(xx, np.where(xx &lt; <span class="number">1</span>, <span class="number">1</span> - xx, <span class="number">0</span>) ** <span class="number">2</span>, color=<span class="string">'orange'</span>, lw=lw,</div><div class="line">         label=<span class="string">"Squared hinge loss"</span>)</div><div class="line">plt.plot(xx, np.exp(-xx), color=<span class="string">'red'</span>,lw=lw,linestyle=<span class="string">'--'</span>,</div><div class="line">         label=<span class="string">"Exponential loss"</span>)</div><div class="line">plt.plot(xx, modified_huber_loss(xx, <span class="number">1</span>), color=<span class="string">'darkorchid'</span>, lw=lw,</div><div class="line">         linestyle=<span class="string">'--'</span>, label=<span class="string">"Modified Huber loss"</span>)</div><div class="line">plt.ylim((<span class="number">0</span>, <span class="number">8</span>))</div><div class="line">plt.legend(loc=<span class="string">"upper right"</span>)</div><div class="line">plt.xlabel(<span class="string">r"Decision function $f(x)$"</span>)</div><div class="line">plt.ylabel(<span class="string">"$L(y, f(x))$"</span>)</div><div class="line">plt.show()</div></pre></td></tr></table></figure>
<p><img src="/picture/machine-learning/practice-advice26.png" alt="practice"><br>　　不同的代价函数有不同的优点：</p>
<ul>
<li>0-1 loss:在分类问题中使用。这是ERM用的代价函数，然而是非凸的，因此必须使用其他代价函数来近似替代。</li>
<li>hinge loss:在SVM中使用，体现最大间隔思想，不容易受离群点影响，有很好的鲁棒性，然后不能提供较好的概率解释。</li>
<li>log loss:在逻辑回归使用，能提供较好的概率解释，然而容易受离群点影响。</li>
<li>Exponential loss: 指数代价，在Boost中使用，容易受离群点影响，在AdaBoost中能够实现简单有效的算法。</li>
<li>perceptron loss:在感知机算法中使用。类似hinge loss，左移了一下。不同于hinge loss,percptron loss不对离超平面近的点进行惩罚。</li>
<li>squared hinge loss: 对hinge loss进行改进，平方损失。</li>
<li>modified huber loss: 对squared hinge loss进一步改进，是一种平滑损失，能够容忍离群点的影响。</li>
</ul>
<h1 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h1><p><a href="https://see.stanford.edu/materials/aimlcs229/ML-advice.pdf" target="_blank" rel="external">斯坦福机器学习：Advice for applying Machine Learning</a><br><a href="https://jmetzen.github.io/2015-01-29/ml_advice.html" target="_blank" rel="external">Advice for applying Machine Learning</a></p>

      
    </div>

    <div>
      
        

      
    </div>

    <div>
      
        
  <div style="padding: 10px 0; margin: 20px auto; width: 90%; text-align: center;">
    <div>坚持原创技术分享，您的支持将鼓励我继续创作！</div>
    <button id="rewardButton" disable="enable" onclick="var qr = document.getElementById('QR'); if (qr.style.display === 'none') {qr.style.display='block';} else {qr.style.display='none'}">
      <span>赏</span>
    </button>
    <div id="QR" style="display: none;">
      
        <div id="wechat" style="display: inline-block">
          <img id="wechat_qr" src="/picture/wechatpay.JPG" alt="xuetf WeChat Pay"/>
          <p>微信打赏</p>
        </div>
      
      
        <div id="alipay" style="display: inline-block">
          <img id="alipay_qr" src="/picture/alipay.JPG" alt="xuetf Alipay"/>
          <p>支付宝打赏</p>
        </div>
      
    </div>
  </div>


      
    </div>


    <footer class="post-footer">
      
        <div class="post-tags">
          
            <a href="/tags/机器学习/" rel="tag"># 机器学习</a>
          
            <a href="/tags/算法诊断/" rel="tag"># 算法诊断</a>
          
            <a href="/tags/偏差方差分析/" rel="tag"># 偏差方差分析</a>
          
            <a href="/tags/学习曲线/" rel="tag"># 学习曲线</a>
          
            <a href="/tags/目标函数/" rel="tag"># 目标函数</a>
          
        </div>
      

      
        <div class="post-nav">
          <div class="post-nav-next post-nav-item">
            
              <a href="/2017/04/01/ml-advice/" rel="next" title="Advice for applying Machine Learning">
                <i class="fa fa-chevron-left"></i> Advice for applying Machine Learning
              </a>
            
          </div>

          <span class="post-nav-divider"></span>

          <div class="post-nav-prev post-nav-item">
            
              <a href="/2017/04/07/聚类算法/" rel="prev" title="K-means和混合高斯模型">
                K-means和混合高斯模型 <i class="fa fa-chevron-right"></i>
              </a>
            
          </div>
        </div>
      

      
      
    </footer>
  </article>



    <div class="post-spread">
      
    </div>
  </div>


          </div>
          


          
  <div class="comments" id="comments">
    
         <div id="uyan_frame"></div>
    
  </div>

        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    <div class="sidebar-inner">

      

      
        <ul class="sidebar-nav motion-element">
          <li class="sidebar-nav-toc sidebar-nav-active" data-target="post-toc-wrap" >
            文章目录
          </li>
          <li class="sidebar-nav-overview" data-target="site-overview">
            站点概览
          </li>
        </ul>
      

      <section class="site-overview sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
          <img class="site-author-image" itemprop="image"
               src="https://avatars1.githubusercontent.com/u/11912425?v=3&u=11f9f5dc75aaf84f020a06c0b9cb2b6f401c586b&s=400"
               alt="xuetf" />
          <p class="site-author-name" itemprop="name">xuetf</p>
          <p class="site-description motion-element" itemprop="description"></p>
        </div>
        <nav class="site-state motion-element">
        
          
            <div class="site-state-item site-state-posts">
              <a href="/archives">
                <span class="site-state-item-count">36</span>
                <span class="site-state-item-name">日志</span>
              </a>
            </div>
          

          
            <div class="site-state-item site-state-categories">
              <a href="/categories">
                <span class="site-state-item-count">14</span>
                <span class="site-state-item-name">分类</span>
              </a>
            </div>
          

          
            <div class="site-state-item site-state-tags">
              <a href="/tags">
                <span class="site-state-item-count">72</span>
                <span class="site-state-item-name">标签</span>
              </a>
            </div>
          

        </nav>

        
          <div class="feed-link motion-element">
            <a href="/atom.xml" rel="alternate">
              <i class="fa fa-rss"></i>
              RSS
            </a>
          </div>
        

        <div class="links-of-author motion-element">
          
        </div>

        
        

        
        
          <div class="links-of-blogroll motion-element links-of-blogroll-inline">
            <div class="links-of-blogroll-title">
              <i class="fa  fa-fw fa-globe"></i>
              链接
            </div>
            <ul class="links-of-blogroll-list">
              
                <li class="links-of-blogroll-item">
                  <a href="http://lsxj615.com/" title="小王子" target="_blank">小王子</a>
                </li>
              
                <li class="links-of-blogroll-item">
                  <a href="https://github.com/xuetf/" title="My Github" target="_blank">My Github</a>
                </li>
              
            </ul>
          </div>
        

        


      </section>

      
      <!--noindex-->
        <section class="post-toc-wrap motion-element sidebar-panel sidebar-panel-active">
          <div class="post-toc">

            
              
            

            
              <div class="post-toc-content"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#数据可视化"><span class="nav-number">1.</span> <span class="nav-text">数据可视化</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#数据集获取"><span class="nav-number">1.1.</span> <span class="nav-text">数据集获取</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#可视化"><span class="nav-number">1.2.</span> <span class="nav-text">可视化</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#模型初步选择"><span class="nav-number">2.</span> <span class="nav-text">模型初步选择</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#学习曲线"><span class="nav-number">2.1.</span> <span class="nav-text">学习曲线</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#过拟合处理"><span class="nav-number">3.</span> <span class="nav-text">过拟合处理</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#增加样本数量"><span class="nav-number">3.1.</span> <span class="nav-text">增加样本数量</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#减少特征"><span class="nav-number">3.2.</span> <span class="nav-text">减少特征</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#修改目标函数正则化项"><span class="nav-number">3.3.</span> <span class="nav-text">修改目标函数正则化项</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#欠拟合处理"><span class="nav-number">4.</span> <span class="nav-text">欠拟合处理</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#增加或使用更好的特征"><span class="nav-number">4.1.</span> <span class="nav-text">增加或使用更好的特征</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#使用更复杂的模型"><span class="nav-number">4.2.</span> <span class="nav-text">使用更复杂的模型</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#大数据集和高维特征处理"><span class="nav-number">5.</span> <span class="nav-text">大数据集和高维特征处理</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#SGDClassfier增量学习"><span class="nav-number">5.1.</span> <span class="nav-text">SGDClassfier增量学习</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#手写体数字识别"><span class="nav-number">5.2.</span> <span class="nav-text">手写体数字识别</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#可视化-1"><span class="nav-number">5.2.1.</span> <span class="nav-text">可视化</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#降维"><span class="nav-number">5.2.2.</span> <span class="nav-text">降维</span></a></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#目标函数选择"><span class="nav-number">6.</span> <span class="nav-text">目标函数选择</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#参考"><span class="nav-number">7.</span> <span class="nav-text">参考</span></a></li></ol></div>
            

          </div>
        </section>
      <!--/noindex-->
      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright" >
  
  &copy; 
  <span itemprop="copyrightYear">2017</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">xuetf</span>
</div>




<script type="text/x-mathjax-config">
 MathJax.Hub.Config({"HTML-CSS": { preferredFont: "TeX", availableFonts: ["STIX","TeX"], linebreaks: { automatic:true }, EqnChunk: (MathJax.Hub.Browser.isMobile ? 10 : 50) },
 tex2jax: { inlineMath: [ ["$", "$"], ["\\(","\\)"] ], processEscapes: true, ignoreClass: "tex2jax_ignore|dno",skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']},
 TeX: { noUndefined: { attributes: { mathcolor: "red", mathbackground: "#FFEEEE", mathsize: "90%" } }, Macros: { href: "{}" } },
 messageStyle: "none"
 });
</script>
<script type="text/x-mathjax-config">
 MathJax.Hub.Queue(function() {
 var all = MathJax.Hub.getAllJax(), i;
 for(i=0; i < all.length; i += 1) {
 all[i].SourceElement().parentNode.className += ' has-jax';
 }
 });
</script>
<script type="text/x-mathjax-config">
 MathJax.Hub.Queue(function() {
 var all = MathJax.Hub.getAllJax(), i;
 for(i=0; i < all.length; i += 1) {
 all[i].SourceElement().parentNode.className += ' has-jax';
 }
 });
</script>
<script charset="utf-8" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>

        

<div class="busuanzi-count">

  <script async src="https://dn-lbstatics.qbox.me/busuanzi/2.3/busuanzi.pure.mini.js"></script>

  
    <span class="site-uv"><i class="fa fa-user"></i><span class="busuanzi-value" id="busuanzi_value_site_uv"></span></span>
  

  
    <span class="site-pv"><i class="fa fa-eye"></i><span class="busuanzi-value" id="busuanzi_value_site_pv"></span></span>
  
  
</div>



        
      </div>
    </footer>

    <div class="back-to-top">
      <i class="fa fa-arrow-up"></i>
    </div>
  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  



  
  <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>

  
  <script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script>

  
  <script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>

  
  <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>

  
  <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>

  
  <script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.1.0"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.1.0"></script>



  
  

  
  <script type="text/javascript" src="/js/src/scrollspy.js?v=5.1.0"></script>
<script type="text/javascript" src="/js/src/post-details.js?v=5.1.0"></script>



  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.0"></script>



  



  



  
    
  
 
      <!-- UY BEGIN -->
      <script type="text/javascript" src="http://v2.uyan.cc/code/uyan.js?uid=2122877"></script>
      <!-- UY END -->
  



	





  




  
  
  <script type="text/javascript">
    // Popup Window;
    var isfetched = false;
    // Search DB path;
    var search_path = "search.xml";
    if (search_path.length == 0) {
      search_path = "search.xml";
    }
    var path = "/" + search_path;
    // monitor main search box;

    function proceedsearch() {
      $("body").append('<div class="popoverlay">').css('overflow', 'hidden');
      $('.popup').toggle();
    }
    // search function;
    var searchFunc = function(path, search_id, content_id) {
      'use strict';
      $.ajax({
        url: path,
        dataType: "xml",
        async: true,
        success: function( xmlResponse ) {
          // get the contents from search data
          isfetched = true;
          $('.popup').detach().appendTo('.header-inner');
          var datas = $( "entry", xmlResponse ).map(function() {
            return {
              title: $( "title", this ).text(),
              content: $("content",this).text(),
              url: $( "url" , this).text()
            };
          }).get();
          var $input = document.getElementById(search_id);
          var $resultContent = document.getElementById(content_id);
          $input.addEventListener('input', function(){
            var matchcounts = 0;
            var str='<ul class=\"search-result-list\">';
            var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
            $resultContent.innerHTML = "";
            if (this.value.trim().length > 1) {
              // perform local searching
              datas.forEach(function(data) {
                var isMatch = false;
                var content_index = [];
                var data_title = data.title.trim().toLowerCase();
                var data_content = data.content.trim().replace(/<[^>]+>/g,"").toLowerCase();
                var data_url = decodeURIComponent(data.url);
                var index_title = -1;
                var index_content = -1;
                var first_occur = -1;
                // only match artiles with not empty titles and contents
                if(data_title != '') {
                  keywords.forEach(function(keyword, i) {
                    index_title = data_title.indexOf(keyword);
                    index_content = data_content.indexOf(keyword);
                    if( index_title >= 0 || index_content >= 0 ){
                      isMatch = true;
                      if (i == 0) {
                        first_occur = index_content;
                      }
                    }

                  });
                }
                // show search results
                if (isMatch) {
                  matchcounts += 1;
                  str += "<li><a href='"+ data_url +"' class='search-result-title'>"+ data_title +"</a>";
                  var content = data.content.trim().replace(/<[^>]+>/g,"");
                  if (first_occur >= 0) {
                    // cut out 100 characters
                    var start = first_occur - 20;
                    var end = first_occur + 80;
                    if(start < 0){
                      start = 0;
                    }
                    if(start == 0){
                      end = 50;
                    }
                    if(end > content.length){
                      end = content.length;
                    }
                    var match_content = content.substring(start, end);
                    // highlight all keywords
                    keywords.forEach(function(keyword){
                      var regS = new RegExp(keyword, "gi");
                      match_content = match_content.replace(regS, "<b class=\"search-keyword\">"+keyword+"</b>");
                    });

                    str += "<p class=\"search-result\">" + match_content +"...</p>"
                  }
                  str += "</li>";
                }
              })};
            str += "</ul>";
            if (matchcounts == 0) { str = '<div id="no-result"><i class="fa fa-frown-o fa-5x" /></div>' }
            if (keywords == "") { str = '<div id="no-result"><i class="fa fa-search fa-5x" /></div>' }
            $resultContent.innerHTML = str;
          });
          proceedsearch();
        }
      });}

    // handle and trigger popup window;
    $('.popup-trigger').click(function(e) {
      e.stopPropagation();
      if (isfetched == false) {
        searchFunc(path, 'local-search-input', 'local-search-result');
      } else {
        proceedsearch();
      };
    });

    $('.popup-btn-close').click(function(e){
      $('.popup').hide();
      $(".popoverlay").remove();
      $('body').css('overflow', '');
    });
    $('.popup').click(function(e){
      e.stopPropagation();
    });
  </script>


  

  <!-- custom analytics part create by xiamo -->
<script src="https://cdn1.lncld.net/static/js/av-core-mini-0.6.1.js"></script>
<script>AV.initialize("DFlRFg5OyISCpmUurUC3Vk4s-gzGzoHsz", "0ayDjXz6ELVOVmPMjLQH3llQ");</script>
<script>
function showTime(Counter) {
  var query = new AV.Query(Counter);
  $(".leancloud_visitors").each(function() {
    var url = $(this).attr("id").trim();
    query.equalTo("url", url);
    query.find({
      success: function(results) {
        if (results.length == 0) {
          var content = '0 ' + $(document.getElementById(url)).text();
          $(document.getElementById(url)).text(content);
          return;
        }
        for (var i = 0; i < results.length; i++) {
          var object = results[i];
          var content = object.get('time') + ' ' + $(document.getElementById(url)).text();
          $(document.getElementById(url)).text(content);
        }
      },
      error: function(object, error) {
        console.log("Error: " + error.code + " " + error.message);
      }
    });

  });
}

function addCount(Counter) {
  var Counter = AV.Object.extend("Counter");
  url = $(".leancloud_visitors").attr('id').trim();
  title = $(".leancloud_visitors").attr('data-flag-title').trim();
  var query = new AV.Query(Counter);
  query.equalTo("url", url);
  query.find({
    success: function(results) {
      if (results.length > 0) {
        var counter = results[0];
        counter.fetchWhenSave(true);
        counter.increment("time");
        counter.save(null, {
          success: function(counter) {
            var content =  counter.get('time') + ' ' + $(document.getElementById(url)).text();
            $(document.getElementById(url)).text(content);
          },
          error: function(counter, error) {
            console.log('Failed to save Visitor num, with error message: ' + error.message);
          }
        });
      } else {
        var newcounter = new Counter();
        newcounter.set("title", title);
        newcounter.set("url", url);
        newcounter.set("time", 1);
        newcounter.save(null, {
          success: function(newcounter) {
              console.log("newcounter.get('time')="+newcounter.get('time'));
            var content = newcounter.get('time') + ' ' + $(document.getElementById(url)).text();
            $(document.getElementById(url)).text(content);
          },
          error: function(newcounter, error) {
            console.log('Failed to create');
          }
        });
      }
    },
    error: function(error) {
      console.log('Error:' + error.code + " " + error.message);
    }
  });
}
$(function() {
  var Counter = AV.Object.extend("Counter");
  if ($('.leancloud_visitors').length == 1) {
    addCount(Counter);
  } else if ($('.post-title-link').length > 1) {
    showTime(Counter);
  }
}); 
</script>
  
<script>
(function(){
    var bp = document.createElement('script');
    var curProtocol = window.location.protocol.split(':')[0];
    if (curProtocol === 'https') {
        bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';        
    }
    else {
        bp.src = 'http://push.zhanzhang.baidu.com/push.js';
    }
    var s = document.getElementsByTagName("script")[0];
    s.parentNode.insertBefore(bp, s);
})();
</script>


  


</body>
</html>
