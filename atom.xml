<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>蘑菇先生学习记</title>
  
  <link href="/atom.xml" rel="self"/>
  
  <link href="xtf615.com/"/>
  <updated>2017-02-13T14:43:53.235Z</updated>
  <id>xtf615.com/</id>
  
  <author>
    <name>xuetf</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>神经网络(系列1)</title>
    <link href="xtf615.com/2017/02/13/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/"/>
    <id>xtf615.com/2017/02/13/神经网络/</id>
    <published>2017-02-13T10:23:23.000Z</published>
    <updated>2017-02-13T14:43:53.235Z</updated>
    
    <content type="html"><![CDATA[<h1 id="非线性假设"><a href="#非线性假设" class="headerlink" title="非线性假设"></a>非线性假设</h1><p>我们之前学的，无论是线性回归还是逻辑回归都有这样一个缺点，即：当特征太多时，计算的负荷会非常大。<br>下面是一个例子：<br><img src="/picture/machine-learning/network1.jpg" alt="network"><br>当我们使用\(x_1,x_2\)的多次项式进行预测时，我们可以应用得很好。<br>之前我们已经看到过，使用非线性的多项式项，能够帮助我们建立更好的分类模型。假设我们有非常多的特征，例如大于100个变量，我们希望用这100个特征来构建一个非线性的多项式模型，结果将是数量非常惊人的特征组合，即便我们只采用两两特征的组合\(x_1x_2+x_1x_3+x_1x_4+…+x_2x_3+x_2x_4+…+x_{99}x_{100}\),我们也会有接近5000个组合而成的特征。这对于一般的逻辑回归来说需要计算的特征太多了。<br>假设我们希望训练一个模型来识别视觉对象（例如识别一张图片上是否是一辆汽车）。<br>我们怎样才能这么做呢？一种方法是我们利用很多汽车的图片和很多非汽车的图片，然后利用这些图片上一个个像素的值（饱和度或亮度）来作为特征。<br>假如我们只选用灰度图片，每个像素则只有一个值（而非RGB值），我们可以选取图片上的两个不同位置上的两个像素，然后训练一个逻辑回归算法利用这两个像素的值来判断图片上是否是汽车：<br><img src="/picture/machine-learning/network2.jpg" alt="network"><br>假使我们采用的都是  50x50像素的小图片，并且我们将所有的像素视为特征，则会有2500个特征，如果我们要进一步将两两特征组合构成一个多项式模型，则会有约\(\frac{2500^2}{2}\)个（接近3百万个）特征。普通的逻辑回归模型，不能有效地处理这么多的特征，这时候我们就需要神经网络。</p>
<h1 id="神经元和大脑"><a href="#神经元和大脑" class="headerlink" title="神经元和大脑"></a>神经元和大脑</h1><p>神经网络是一种很古老的算法，它最初产生的目的是制造能模拟大脑的机器。<br>接下来我将介绍神经网络。它能很好地解决不同的机器学习问题。首先介绍一些神经网络的背景知识，由此我们能知道可以用它们来做什么。不管是将其应用到现代的机器学习问题上，还是应用到那些你可能会感兴趣的问题中。也许，这一伟大的人工智能梦想在未来能制造出真正的智能机器。另外，我们还将讲解神经网络是怎么涉及这些问题的，神经网络产生的原因是人们想尝试设计出模仿大脑的算法，从某种意义上说如果我们想要建立学习系统，那为什么不去模仿我们所认识的最神奇的学习机器——人类的大脑呢？</p>
<h2 id="起源"><a href="#起源" class="headerlink" title="起源"></a>起源</h2><p>神经网络逐渐兴起于二十世纪八九十年代，应用得非常广泛。但由于各种原因，在90年代的后期应用减少了。但是最近，神经网络又东山再起了。其中一个原因是：神经网络是计算量有些偏大的算法。近些年计算机的运行速度变快，才足以真正运行起大规模的神经网络。正是由于这个原因和其他一些我们后面会讨论到的技术因素，如今的神经网络对于许多应用来说是最先进的技术。当你想模拟大脑时，是指想制造出与人类大脑作用效果相同的机器。大脑可以学会去以看而不是听的方式处理图像，学会处理我们的触觉我们能学习数学，学着做微积分，而且大脑能处理各种不同的令人惊奇的事情。似乎如果你想要模仿它，你得写很多不同的软件来模拟所有这些五花八门的奇妙的事情。不过能不能假设大脑做所有这些，不同事情的方法，不需要用上千个不同的程序去实现。相反的，大脑处理的方法，只需要一个单一的学习算法就可以了？</p>
<h2 id="例子"><a href="#例子" class="headerlink" title="例子"></a>例子</h2><ul>
<li><p>如图，大脑的这一部分这一小片红色区域是你的听觉皮层，你现在正在理解我的话，这靠的是耳朵。耳朵接收到声音信号，并把声音信号传递给你的听觉皮层.<br><img src="/picture/machine-learning/network3.jpg" alt="network"></p>
</li>
<li><p>神经系统科学家做了下面这个有趣的实验，把耳朵到听觉皮层的神经切断。在这种情况下，将其重新接到一个动物的大脑上，这样从眼睛到视神经的信号最终将传到听觉皮层。如果这样做了。那么结果表明听觉皮层将会学会“看”。这里的“看”代表了我们所知道的每层含义。所以，如果你对动物这样做，那么动物就可以完成视觉辨别任务，它们可以看图像，并根据图像做出适当的决定。它们正是通过脑组织中的这个部分完成的。</p>
</li>
<li>下面再举另一个例子，这块红色的脑组织是你的躯体感觉皮层，这是你用来处理触觉的，如果你做一个和刚才类似的重接实验，那么躯体感觉皮层也能学会“看”。这个实验和其它一些类似的实验，被称为神经重接实验，从这个意义上说，如果人体有同一块脑组织可以处理光、声或触觉信号，那么也许存在一种学习算法，可以同时处理视觉、听觉和触觉，而不是需要运行上千个不同的程序，或者上千个不同的算法来做这些大脑所完成的成千上万的美好事情。也许我们需要做的就是找出一些近似的或实际的大脑学习算法，然后实现它大脑通过自学掌握如何处理这些不同类型的数据。在很大的程度上，可以猜想如果我们把几乎任何一种传感器接入到大脑的几乎任何一个部位的话，大脑就会学会处理它。</li>
<li>这张图是用舌头学会“看”的一个例子。它的原理是：这实际上是一个名为BrainPort的系统，它现在正在FDA(美国食品和药物管理局的临床试验阶段，它能帮助失明人士看见事物。它的原理是，你在前额上带一个灰度摄像头，面朝前，它就能获取你面前事物的低分辨率的灰度图像。你连一根线到舌头上安装的电极阵列上，那么每个像素都被映射到你舌头<br>的某个位置上，可能电压值高的点对应一个暗像素电压值低的点。对应于亮像素，即使依靠它现在的功能，使用这种系统就能让你我在几十分钟里就学会用我们的舌头“看”东西。<br><img src="/picture/machine-learning/network4.jpg" alt="network"></li>
<li>这是关于人体回声定位或者说人体声纳。你有两种方法可以实现：你可以弹响指，或者咂舌头。不过现在有失明人士，确实在学校里接受这样的培训，并学会解读从环境反弹回来的声波模式—这就是声纳。如果你搜索 YouTube之后，就会发现有些视频讲述了一个令人称奇的孩子，他因为癌症眼球惨遭移除，虽然失去了眼球，但是通过打响指，他可以四处走动而不撞到任何东西，他能滑滑板，他可以将篮球投入篮框中。注意这是一个没有眼球的孩子。<br><img src="/picture/machine-learning/network5.jpg" alt="network"></li>
<li>这是触觉皮带，如果你把它戴在腰上，蜂鸣器会响，而且总是朝向北时发出嗡嗡声。它可以使人拥有方向感，用类似于鸟类感知方向的方式。<br><img src="/picture/machine-learning/network6.jpg" alt="network"></li>
<li>还有一些离奇的例子。如果你在青蛙身上插入第三只眼，青蛙也能学会使用那只眼睛。因此，这将会非常令人惊奇。如果你能把几乎任何传感器接入到大脑中，大脑的学习算法就能找出学习数据的方法，并处理这些数据。从某种意义上来说，如果我们能找出大脑的学习算法，然后在计算机上执行大脑学习算法或与之相似的算法，也许这将是我们向人工智能迈进做出的最好的尝试。人工智能的梦想就是：有一天能制造出真正的智能机器。<br><img src="/picture/machine-learning/network7.jpg" alt="network"><br>神经网络可能为我们打开一扇进入遥远的人工智能梦的窗户。</li>
</ul>
<h1 id="模型表示（1）"><a href="#模型表示（1）" class="headerlink" title="模型表示（1）"></a>模型表示（1）</h1><p>为了构建神经网络模型，我们需要首先思考大脑中的神经网络是怎样的？每一个神经元都可以被认为是一个处理单元 /神经核（processing unit/Nucleus），它含有许多输入/树突（input/Dendrite），并且有一个输出/轴突（output/Axon）。神经网络是大量神经元相互链接并通过电脉冲来交流的一个网络。<br><img src="/picture/machine-learning/network8.jpg" alt="network"><br>下面是一组神经元的示意图，神经元利用微弱的电流进行沟通。这些弱电流也称作动作电位，其实就是一些微弱的电流。所以如果神经元想要传递一个消息，它就会就通过它的轴突，发送一段微弱电流给其他神经元，这就是轴突。<br>这里是一条连接到输入神经，或者连接另一个神经元树突的神经，接下来这个神经元接收这条消息，做一些计算，它有可能会反过来将在轴突上的自己的消息传给其他神经元。这就是所有人类思考的模型：我们的神经元把自己的收到的消息进行计算，并向其他神经元传递消息。这也是我们的感觉和肌肉运转的原理。如果你想活动一块肌肉，就会触发一个神经元给你的肌肉发送脉冲，并引起你的肌肉收缩。如果一些感官：比如说眼睛想要给大脑传递一个消息，那么它就像这样发送电脉冲给大脑的。<br><img src="/picture/machine-learning/network9.jpg" alt="network"><br>神经网络模型建立在很多神经元之上，每一个神经元又是一个个学习模型。这些神经元（也叫激活单元，activation unit）采纳一些特征作为输出，并且根据本身的模型提供一个输出。下图是一个以逻辑回归模型作为自身学习模型的神经元示例，在神经网络中，参数又可被称为权重（weight）。<br><img src="/picture/machine-learning/network10.jpg" alt="network"><br>我们设计出了类似于神经元的神经网络，效果如下：<br><img src="/picture/machine-learning/network11.jpg" alt="network"><br>其中\(x_1,x_2,x_3\)是输入单元（input units），我们将原始数据输入给它们。\(a_1,a_2,a_3\)是中间单元，它们负责将数据进行处理，然后呈递到下一层。最后是输出单元，它负责计算\(h_θ(x)\)。<br>神经网络模型是许多逻辑单元按照不同层级组织起来的网络，每一层的输出变量都是下一层的输入变量。下图为一个 3层的神经网络，第一层成为输入层（Input Layer），最后一层称为输出层（Output Layer），中间一层成为隐藏层（Hidden Layers）。我们为每一层都增加一个偏差单位（bias unit）：<br><img src="/picture/machine-learning/network12.jpg" alt="network"><br>下面引入一些标记法来帮助描述模型：<br>\(a_i^{(j)})\)代表第j层的第i个激活单元。\(\Theta^{(j)}\)代表从第j层映射到第j+1层时的权重的矩阵。例如\(\Theta^{(i)}\)代表从第一层映射到第二层的权重矩阵。其尺寸为：以第<strong>j+1</strong>层的激活单元数量为<strong>行数</strong>，以第<strong>j</strong>层的激活单元数加1为<strong>列数</strong>的矩阵。<br>例如：上图所示的神经网络中\(θ^{(1)}的尺寸为3*4。<br>对于上图所示的模型，激活单元和输出分别表达为：</p>
<p>$$a_1^{(2)}=g(\Theta_{10}^{(1)}x_0+\Theta_{11}^{(1)}x_1+\Theta_{12}^{(1)}x_2+\Theta_{13}^{(1)}x_3) \\\\<br>a_2^{(2)}=g(\Theta_{20}^{(1)}x_0+\Theta_{21}^{(1)}x_1+\Theta_{22}^{(1)}x_2+\Theta_{23}^{(1)}x_3) \\\\<br>a_3^{(2)}=g(\Theta_{30}^{(1)}x_0+\Theta_{31}^{(1)}x_1+\Theta_{32}^{(1)}x_2+\Theta_{33}^{(1)}x_3)$$<br>上面进行的讨论中只是将特征矩阵中的一行(一个训练实例)喂给了神经网络，我们需要将整个训练集都喂给我们的神经网络算法来学习模型。<br>我们可以知道：每一个a都是由上一层所有的x和每一个x所对应的决定的。<br>（我们把这样从左到右的算法称为前向传播算法(FORWARD PROPAGATION)）<br>把X，a分别用矩阵表示：<br>$$X=\begin{bmatrix} x_0 \\\ x_1 \\\ x_2 \\\ x_3\end{bmatrix}\\\\<br>\Theta=\begin{bmatrix} \Theta_{10} \ \Theta_{11} \ \Theta_{12} \ \Theta_{13}\\\ \Theta_{20} \ \Theta_{21} \ \Theta_{22} \ \Theta_{23}\\\ \Theta_{30} \ \Theta_{31} \ \Theta_{32} \ \Theta_{33}\end{bmatrix} \\\\<br>a=\begin{bmatrix} a_1 \\\ a_2 \\\ a_3\end{bmatrix} \\\\<br>我们可以得到\Theta X=a。<br>$$</p>
<h1 id="模型表示（2）"><a href="#模型表示（2）" class="headerlink" title="模型表示（2）"></a>模型表示（2）</h1><p>( FORWARD PROPAGATION  )相对与使用循环来编码，利用向量化的方法会使得计算更为简便。以上面的神经网络为例，试着计算第二层的值：</p>
<p>$$x=\begin{bmatrix} x_0 \\\ x_1 \\\ x_2 \\\ x_3\end{bmatrix}\\\\<br>z^{(2)}=\begin{bmatrix} z_1^{(2)} \\\ z_2^{(2)} \\\ z_3^{(2)}\end{bmatrix}$$<br><img src="/picture/machine-learning/network13.jpg" alt="network"><br>我们令：\(z^{(2)}=\Theta^{(1)}x\),则\(a^{(2)}=g(z^{(2)})\),计算后添加\(a_0^{(2)}=I\)。计算输出值为：<br><img src="/picture/machine-learning/network14.jpg" alt="network"><br>令：\(z^{(3)}=\Theta^{(2)}a^{(2)}\),则\(h_\theta(x)=a^{(3)}=g(z^{(3)})\)<br>这只是针对训练集中一个训练实例所进行的计算。如果我们要对整个训练集进行计算，我们需要将训练集特征矩阵进行转置，使得同一个实例的特征都在同一列里。即：<br>\(z^{(2)}=\Theta^{(1)}*X^T\) , \(a^{(2)}=g(z^{(2)})\)<br>为了更好地了解Neuron Networks的工作原理，我们先把左半部分遮住：<br><img src="/picture/machine-learning/network15.jpg" alt="network"><br>右半部分其实就是以\(a_0,a_1,a_2,a_3\)按照Logistic Regression的方式输出h(x)：<br><img src="/picture/machine-learning/network16.jpg" alt="network"><br>其实神经网络就像是logistic  regression，只不过我们把logistic regression中的输入向量\([x1~x3]\)变成了中间层的\([a_1^{(2)}~a_3^{(2)}]\)<br>即：<br>$$h(x)=g(\theta_0^{(2)}a_0^{(2)}+\theta_1^{(2)}a_1^{(2)}+\theta_2^{(2)}a_2^{(2)}+\theta_3^{(2)}a_3^{(2)})$$<br>我们可以把\(a_0,a_1,a_2,a_3\)看成更为高级的特征值，也就是\(x_0,x_1,x_2,x_3\)的进化体，并且它们是由x决定的，因为是梯度下降的，所以a是变化的，并且变得越来越厉害，所以<strong>这些更高级的特征值远比仅仅将x次方厉害</strong>，也能更好的预测新数据。<br>这就是神经网络相比于逻辑回归和线性回归的优势。</p>
<h1 id="例子和直观理解（1）"><a href="#例子和直观理解（1）" class="headerlink" title="例子和直观理解（1）"></a>例子和直观理解（1）</h1><p>从本质上讲，神经网络能够通过学习得出其自身的一系列特征。在普通的逻辑回归中，我们被限制为使用数据中的原始特征\(x_1,x_2,…,x_n\)，我们虽然可以使用一些二项式项来组合这些特征，但是我们仍然受到这些原始特征的限制。在神经网络中，原始特征只是输入层，在我们上面三层的神经网络例子中，第三层也就是输出层做出的预测利用的是第二层的特征，而非输入层中的原始特征，我们可以认为第二层中的特征是神经网络通过学习后自己得出的一系列用于预测输出变量的新特征。<br>神经网络中，单层神经元（无中间层）的计算可用来表示逻辑运算，比如逻辑AND、逻辑或OR。<br>未完待续…</p>
<a id="more"></a>
]]></content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;非线性假设&quot;&gt;&lt;a href=&quot;#非线性假设&quot; class=&quot;headerlink&quot; title=&quot;非线性假设&quot;&gt;&lt;/a&gt;非线性假设&lt;/h1&gt;&lt;p&gt;我们之前学的，无论是线性回归还是逻辑回归都有这样一个缺点，即：当特征太多时，计算的负荷会非常大。&lt;br&gt;下面是一个例子：&lt;br&gt;&lt;img src=&quot;/picture/machine-learning/network1.jpg&quot; alt=&quot;network&quot;&gt;&lt;br&gt;当我们使用\(x_1,x_2\)的多次项式进行预测时，我们可以应用得很好。&lt;br&gt;之前我们已经看到过，使用非线性的多项式项，能够帮助我们建立更好的分类模型。假设我们有非常多的特征，例如大于100个变量，我们希望用这100个特征来构建一个非线性的多项式模型，结果将是数量非常惊人的特征组合，即便我们只采用两两特征的组合\(x_1x_2+x_1x_3+x_1x_4+…+x_2x_3+x_2x_4+…+x_{99}x_{100}\),我们也会有接近5000个组合而成的特征。这对于一般的逻辑回归来说需要计算的特征太多了。&lt;br&gt;假设我们希望训练一个模型来识别视觉对象（例如识别一张图片上是否是一辆汽车）。&lt;br&gt;我们怎样才能这么做呢？一种方法是我们利用很多汽车的图片和很多非汽车的图片，然后利用这些图片上一个个像素的值（饱和度或亮度）来作为特征。&lt;br&gt;假如我们只选用灰度图片，每个像素则只有一个值（而非RGB值），我们可以选取图片上的两个不同位置上的两个像素，然后训练一个逻辑回归算法利用这两个像素的值来判断图片上是否是汽车：&lt;br&gt;&lt;img src=&quot;/picture/machine-learning/network2.jpg&quot; alt=&quot;network&quot;&gt;&lt;br&gt;假使我们采用的都是  50x50像素的小图片，并且我们将所有的像素视为特征，则会有2500个特征，如果我们要进一步将两两特征组合构成一个多项式模型，则会有约\(\frac{2500^2}{2}\)个（接近3百万个）特征。普通的逻辑回归模型，不能有效地处理这么多的特征，这时候我们就需要神经网络。&lt;/p&gt;
&lt;h1 id=&quot;神经元和大脑&quot;&gt;&lt;a href=&quot;#神经元和大脑&quot; class=&quot;headerlink&quot; title=&quot;神经元和大脑&quot;&gt;&lt;/a&gt;神经元和大脑&lt;/h1&gt;&lt;p&gt;神经网络是一种很古老的算法，它最初产生的目的是制造能模拟大脑的机器。&lt;br&gt;接下来我将介绍神经网络。它能很好地解决不同的机器学习问题。首先介绍一些神经网络的背景知识，由此我们能知道可以用它们来做什么。不管是将其应用到现代的机器学习问题上，还是应用到那些你可能会感兴趣的问题中。也许，这一伟大的人工智能梦想在未来能制造出真正的智能机器。另外，我们还将讲解神经网络是怎么涉及这些问题的，神经网络产生的原因是人们想尝试设计出模仿大脑的算法，从某种意义上说如果我们想要建立学习系统，那为什么不去模仿我们所认识的最神奇的学习机器——人类的大脑呢？&lt;/p&gt;
&lt;h2 id=&quot;起源&quot;&gt;&lt;a href=&quot;#起源&quot; class=&quot;headerlink&quot; title=&quot;起源&quot;&gt;&lt;/a&gt;起源&lt;/h2&gt;&lt;p&gt;神经网络逐渐兴起于二十世纪八九十年代，应用得非常广泛。但由于各种原因，在90年代的后期应用减少了。但是最近，神经网络又东山再起了。其中一个原因是：神经网络是计算量有些偏大的算法。近些年计算机的运行速度变快，才足以真正运行起大规模的神经网络。正是由于这个原因和其他一些我们后面会讨论到的技术因素，如今的神经网络对于许多应用来说是最先进的技术。当你想模拟大脑时，是指想制造出与人类大脑作用效果相同的机器。大脑可以学会去以看而不是听的方式处理图像，学会处理我们的触觉我们能学习数学，学着做微积分，而且大脑能处理各种不同的令人惊奇的事情。似乎如果你想要模仿它，你得写很多不同的软件来模拟所有这些五花八门的奇妙的事情。不过能不能假设大脑做所有这些，不同事情的方法，不需要用上千个不同的程序去实现。相反的，大脑处理的方法，只需要一个单一的学习算法就可以了？&lt;/p&gt;
&lt;h2 id=&quot;例子&quot;&gt;&lt;a href=&quot;#例子&quot; class=&quot;headerlink&quot; title=&quot;例子&quot;&gt;&lt;/a&gt;例子&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;&lt;p&gt;如图，大脑的这一部分这一小片红色区域是你的听觉皮层，你现在正在理解我的话，这靠的是耳朵。耳朵接收到声音信号，并把声音信号传递给你的听觉皮层.&lt;br&gt;&lt;img src=&quot;/picture/machine-learning/network3.jpg&quot; alt=&quot;network&quot;&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;神经系统科学家做了下面这个有趣的实验，把耳朵到听觉皮层的神经切断。在这种情况下，将其重新接到一个动物的大脑上，这样从眼睛到视神经的信号最终将传到听觉皮层。如果这样做了。那么结果表明听觉皮层将会学会“看”。这里的“看”代表了我们所知道的每层含义。所以，如果你对动物这样做，那么动物就可以完成视觉辨别任务，它们可以看图像，并根据图像做出适当的决定。它们正是通过脑组织中的这个部分完成的。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;下面再举另一个例子，这块红色的脑组织是你的躯体感觉皮层，这是你用来处理触觉的，如果你做一个和刚才类似的重接实验，那么躯体感觉皮层也能学会“看”。这个实验和其它一些类似的实验，被称为神经重接实验，从这个意义上说，如果人体有同一块脑组织可以处理光、声或触觉信号，那么也许存在一种学习算法，可以同时处理视觉、听觉和触觉，而不是需要运行上千个不同的程序，或者上千个不同的算法来做这些大脑所完成的成千上万的美好事情。也许我们需要做的就是找出一些近似的或实际的大脑学习算法，然后实现它大脑通过自学掌握如何处理这些不同类型的数据。在很大的程度上，可以猜想如果我们把几乎任何一种传感器接入到大脑的几乎任何一个部位的话，大脑就会学会处理它。&lt;/li&gt;
&lt;li&gt;这张图是用舌头学会“看”的一个例子。它的原理是：这实际上是一个名为BrainPort的系统，它现在正在FDA(美国食品和药物管理局的临床试验阶段，它能帮助失明人士看见事物。它的原理是，你在前额上带一个灰度摄像头，面朝前，它就能获取你面前事物的低分辨率的灰度图像。你连一根线到舌头上安装的电极阵列上，那么每个像素都被映射到你舌头&lt;br&gt;的某个位置上，可能电压值高的点对应一个暗像素电压值低的点。对应于亮像素，即使依靠它现在的功能，使用这种系统就能让你我在几十分钟里就学会用我们的舌头“看”东西。&lt;br&gt;&lt;img src=&quot;/picture/machine-learning/network4.jpg&quot; alt=&quot;network&quot;&gt;&lt;/li&gt;
&lt;li&gt;这是关于人体回声定位或者说人体声纳。你有两种方法可以实现：你可以弹响指，或者咂舌头。不过现在有失明人士，确实在学校里接受这样的培训，并学会解读从环境反弹回来的声波模式—这就是声纳。如果你搜索 YouTube之后，就会发现有些视频讲述了一个令人称奇的孩子，他因为癌症眼球惨遭移除，虽然失去了眼球，但是通过打响指，他可以四处走动而不撞到任何东西，他能滑滑板，他可以将篮球投入篮框中。注意这是一个没有眼球的孩子。&lt;br&gt;&lt;img src=&quot;/picture/machine-learning/network5.jpg&quot; alt=&quot;network&quot;&gt;&lt;/li&gt;
&lt;li&gt;这是触觉皮带，如果你把它戴在腰上，蜂鸣器会响，而且总是朝向北时发出嗡嗡声。它可以使人拥有方向感，用类似于鸟类感知方向的方式。&lt;br&gt;&lt;img src=&quot;/picture/machine-learning/network6.jpg&quot; alt=&quot;network&quot;&gt;&lt;/li&gt;
&lt;li&gt;还有一些离奇的例子。如果你在青蛙身上插入第三只眼，青蛙也能学会使用那只眼睛。因此，这将会非常令人惊奇。如果你能把几乎任何传感器接入到大脑中，大脑的学习算法就能找出学习数据的方法，并处理这些数据。从某种意义上来说，如果我们能找出大脑的学习算法，然后在计算机上执行大脑学习算法或与之相似的算法，也许这将是我们向人工智能迈进做出的最好的尝试。人工智能的梦想就是：有一天能制造出真正的智能机器。&lt;br&gt;&lt;img src=&quot;/picture/machine-learning/network7.jpg&quot; alt=&quot;network&quot;&gt;&lt;br&gt;神经网络可能为我们打开一扇进入遥远的人工智能梦的窗户。&lt;/li&gt;
&lt;/ul&gt;
&lt;h1 id=&quot;模型表示（1）&quot;&gt;&lt;a href=&quot;#模型表示（1）&quot; class=&quot;headerlink&quot; title=&quot;模型表示（1）&quot;&gt;&lt;/a&gt;模型表示（1）&lt;/h1&gt;&lt;p&gt;为了构建神经网络模型，我们需要首先思考大脑中的神经网络是怎样的？每一个神经元都可以被认为是一个处理单元 /神经核（processing unit/Nucleus），它含有许多输入/树突（input/Dendrite），并且有一个输出/轴突（output/Axon）。神经网络是大量神经元相互链接并通过电脉冲来交流的一个网络。&lt;br&gt;&lt;img src=&quot;/picture/machine-learning/network8.jpg&quot; alt=&quot;network&quot;&gt;&lt;br&gt;下面是一组神经元的示意图，神经元利用微弱的电流进行沟通。这些弱电流也称作动作电位，其实就是一些微弱的电流。所以如果神经元想要传递一个消息，它就会就通过它的轴突，发送一段微弱电流给其他神经元，这就是轴突。&lt;br&gt;这里是一条连接到输入神经，或者连接另一个神经元树突的神经，接下来这个神经元接收这条消息，做一些计算，它有可能会反过来将在轴突上的自己的消息传给其他神经元。这就是所有人类思考的模型：我们的神经元把自己的收到的消息进行计算，并向其他神经元传递消息。这也是我们的感觉和肌肉运转的原理。如果你想活动一块肌肉，就会触发一个神经元给你的肌肉发送脉冲，并引起你的肌肉收缩。如果一些感官：比如说眼睛想要给大脑传递一个消息，那么它就像这样发送电脉冲给大脑的。&lt;br&gt;&lt;img src=&quot;/picture/machine-learning/network9.jpg&quot; alt=&quot;network&quot;&gt;&lt;br&gt;神经网络模型建立在很多神经元之上，每一个神经元又是一个个学习模型。这些神经元（也叫激活单元，activation unit）采纳一些特征作为输出，并且根据本身的模型提供一个输出。下图是一个以逻辑回归模型作为自身学习模型的神经元示例，在神经网络中，参数又可被称为权重（weight）。&lt;br&gt;&lt;img src=&quot;/picture/machine-learning/network10.jpg&quot; alt=&quot;network&quot;&gt;&lt;br&gt;我们设计出了类似于神经元的神经网络，效果如下：&lt;br&gt;&lt;img src=&quot;/picture/machine-learning/network11.jpg&quot; alt=&quot;network&quot;&gt;&lt;br&gt;其中\(x_1,x_2,x_3\)是输入单元（input units），我们将原始数据输入给它们。\(a_1,a_2,a_3\)是中间单元，它们负责将数据进行处理，然后呈递到下一层。最后是输出单元，它负责计算\(h_θ(x)\)。&lt;br&gt;神经网络模型是许多逻辑单元按照不同层级组织起来的网络，每一层的输出变量都是下一层的输入变量。下图为一个 3层的神经网络，第一层成为输入层（Input Layer），最后一层称为输出层（Output Layer），中间一层成为隐藏层（Hidden Layers）。我们为每一层都增加一个偏差单位（bias unit）：&lt;br&gt;&lt;img src=&quot;/picture/machine-learning/network12.jpg&quot; alt=&quot;network&quot;&gt;&lt;br&gt;下面引入一些标记法来帮助描述模型：&lt;br&gt;\(a_i^{(j)})\)代表第j层的第i个激活单元。\(\Theta^{(j)}\)代表从第j层映射到第j+1层时的权重的矩阵。例如\(\Theta^{(i)}\)代表从第一层映射到第二层的权重矩阵。其尺寸为：以第&lt;strong&gt;j+1&lt;/strong&gt;层的激活单元数量为&lt;strong&gt;行数&lt;/strong&gt;，以第&lt;strong&gt;j&lt;/strong&gt;层的激活单元数加1为&lt;strong&gt;列数&lt;/strong&gt;的矩阵。&lt;br&gt;例如：上图所示的神经网络中\(θ^{(1)}的尺寸为3*4。&lt;br&gt;对于上图所示的模型，激活单元和输出分别表达为：&lt;/p&gt;
&lt;p&gt;$$a_1^{(2)}=g(\Theta_{10}^{(1)}x_0+\Theta_{11}^{(1)}x_1+\Theta_{12}^{(1)}x_2+\Theta_{13}^{(1)}x_3) \\\\&lt;br&gt;a_2^{(2)}=g(\Theta_{20}^{(1)}x_0+\Theta_{21}^{(1)}x_1+\Theta_{22}^{(1)}x_2+\Theta_{23}^{(1)}x_3) \\\\&lt;br&gt;a_3^{(2)}=g(\Theta_{30}^{(1)}x_0+\Theta_{31}^{(1)}x_1+\Theta_{32}^{(1)}x_2+\Theta_{33}^{(1)}x_3)$$&lt;br&gt;上面进行的讨论中只是将特征矩阵中的一行(一个训练实例)喂给了神经网络，我们需要将整个训练集都喂给我们的神经网络算法来学习模型。&lt;br&gt;我们可以知道：每一个a都是由上一层所有的x和每一个x所对应的决定的。&lt;br&gt;（我们把这样从左到右的算法称为前向传播算法(FORWARD PROPAGATION)）&lt;br&gt;把X，a分别用矩阵表示：&lt;br&gt;$$X=\begin{bmatrix} x_0 \\\ x_1 \\\ x_2 \\\ x_3\end{bmatrix}\\\\&lt;br&gt;\Theta=\begin{bmatrix} \Theta_{10} \ \Theta_{11} \ \Theta_{12} \ \Theta_{13}\\\ \Theta_{20} \ \Theta_{21} \ \Theta_{22} \ \Theta_{23}\\\ \Theta_{30} \ \Theta_{31} \ \Theta_{32} \ \Theta_{33}\end{bmatrix} \\\\&lt;br&gt;a=\begin{bmatrix} a_1 \\\ a_2 \\\ a_3\end{bmatrix} \\\\&lt;br&gt;我们可以得到\Theta X=a。&lt;br&gt;$$&lt;/p&gt;
&lt;h1 id=&quot;模型表示（2）&quot;&gt;&lt;a href=&quot;#模型表示（2）&quot; class=&quot;headerlink&quot; title=&quot;模型表示（2）&quot;&gt;&lt;/a&gt;模型表示（2）&lt;/h1&gt;&lt;p&gt;( FORWARD PROPAGATION  )相对与使用循环来编码，利用向量化的方法会使得计算更为简便。以上面的神经网络为例，试着计算第二层的值：&lt;/p&gt;
&lt;p&gt;$$x=\begin{bmatrix} x_0 \\\ x_1 \\\ x_2 \\\ x_3\end{bmatrix}\\\\&lt;br&gt;z^{(2)}=\begin{bmatrix} z_1^{(2)} \\\ z_2^{(2)} \\\ z_3^{(2)}\end{bmatrix}$$&lt;br&gt;&lt;img src=&quot;/picture/machine-learning/network13.jpg&quot; alt=&quot;network&quot;&gt;&lt;br&gt;我们令：\(z^{(2)}=\Theta^{(1)}x\),则\(a^{(2)}=g(z^{(2)})\),计算后添加\(a_0^{(2)}=I\)。计算输出值为：&lt;br&gt;&lt;img src=&quot;/picture/machine-learning/network14.jpg&quot; alt=&quot;network&quot;&gt;&lt;br&gt;令：\(z^{(3)}=\Theta^{(2)}a^{(2)}\),则\(h_\theta(x)=a^{(3)}=g(z^{(3)})\)&lt;br&gt;这只是针对训练集中一个训练实例所进行的计算。如果我们要对整个训练集进行计算，我们需要将训练集特征矩阵进行转置，使得同一个实例的特征都在同一列里。即：&lt;br&gt;\(z^{(2)}=\Theta^{(1)}*X^T\) , \(a^{(2)}=g(z^{(2)})\)&lt;br&gt;为了更好地了解Neuron Networks的工作原理，我们先把左半部分遮住：&lt;br&gt;&lt;img src=&quot;/picture/machine-learning/network15.jpg&quot; alt=&quot;network&quot;&gt;&lt;br&gt;右半部分其实就是以\(a_0,a_1,a_2,a_3\)按照Logistic Regression的方式输出h(x)：&lt;br&gt;&lt;img src=&quot;/picture/machine-learning/network16.jpg&quot; alt=&quot;network&quot;&gt;&lt;br&gt;其实神经网络就像是logistic  regression，只不过我们把logistic regression中的输入向量\([x1~x3]\)变成了中间层的\([a_1^{(2)}~a_3^{(2)}]\)&lt;br&gt;即：&lt;br&gt;$$h(x)=g(\theta_0^{(2)}a_0^{(2)}+\theta_1^{(2)}a_1^{(2)}+\theta_2^{(2)}a_2^{(2)}+\theta_3^{(2)}a_3^{(2)})$$&lt;br&gt;我们可以把\(a_0,a_1,a_2,a_3\)看成更为高级的特征值，也就是\(x_0,x_1,x_2,x_3\)的进化体，并且它们是由x决定的，因为是梯度下降的，所以a是变化的，并且变得越来越厉害，所以&lt;strong&gt;这些更高级的特征值远比仅仅将x次方厉害&lt;/strong&gt;，也能更好的预测新数据。&lt;br&gt;这就是神经网络相比于逻辑回归和线性回归的优势。&lt;/p&gt;
&lt;h1 id=&quot;例子和直观理解（1）&quot;&gt;&lt;a href=&quot;#例子和直观理解（1）&quot; class=&quot;headerlink&quot; title=&quot;例子和直观理解（1）&quot;&gt;&lt;/a&gt;例子和直观理解（1）&lt;/h1&gt;&lt;p&gt;从本质上讲，神经网络能够通过学习得出其自身的一系列特征。在普通的逻辑回归中，我们被限制为使用数据中的原始特征\(x_1,x_2,…,x_n\)，我们虽然可以使用一些二项式项来组合这些特征，但是我们仍然受到这些原始特征的限制。在神经网络中，原始特征只是输入层，在我们上面三层的神经网络例子中，第三层也就是输出层做出的预测利用的是第二层的特征，而非输入层中的原始特征，我们可以认为第二层中的特征是神经网络通过学习后自己得出的一系列用于预测输出变量的新特征。&lt;br&gt;神经网络中，单层神经元（无中间层）的计算可用来表示逻辑运算，比如逻辑AND、逻辑或OR。&lt;br&gt;未完待续…&lt;/p&gt;
    
    </summary>
    
      <category term="机器学习" scheme="xtf615.com/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="机器学习" scheme="xtf615.com/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="人工智能" scheme="xtf615.com/tags/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/"/>
    
      <category term="神经网络" scheme="xtf615.com/tags/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/"/>
    
  </entry>
  
  <entry>
    <title>逻辑回归</title>
    <link href="xtf615.com/2017/02/11/%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92/"/>
    <id>xtf615.com/2017/02/11/逻辑回归/</id>
    <published>2017-02-11T07:30:53.000Z</published>
    <updated>2017-02-13T03:22:26.236Z</updated>
    
    <content type="html"><![CDATA[<h1 id="分类问题"><a href="#分类问题" class="headerlink" title="分类问题"></a>分类问题</h1><p>在分类问题中，你要预测的变量  y是离散的值，我们将学习一种叫做逻辑回归(Logistic Regression)的算法，这是目前最流行使用最广泛的一种学习算法。</p>
<p>在分类问题中，我们尝试预测的是结果是否属于某一个类（例如正确或错误）。分类问题的例子有：判断一封电子邮件是否是垃圾邮件；判断一次金融交易是否是欺诈；之前我们也谈到了肿瘤分类问题的例子，区别一个肿瘤是恶性的还是良性的。</p>
<p>我们从二元的分类问题开始讨论。我们将因变量(dependant variable)可能属于的两个类分别称为负向类（negative class）和正向类（positive class），则因变量 \(y\in{0,1}\) 其中0表示负向类，1表示正向类。<br><img src="/picture/machine-learning/logistic_regression1.jpg" alt="logistic_regression"><br><img src="/picture/machine-learning/logistic_regression2.jpg" alt="logistic_regression2"><br>如果我们要用线性回归算法来解决一个分类问题，对于分类，y取值为     0或者1，但如果你使用的是线性回归，那么假设函数的输出值可能远大于  1，或者远小于0，即使所有训练样本的标签y都等于0或1。尽管我们知道标签应该取值0或者1，但是如果算法得到的值远大于1或者远小于0的话，就会感觉很奇怪。所以我们在接下来的要研究的算法就叫做逻辑回归算法，这个算法的性质是：它的输出值永远在0到1之间。</p>
<p>顺便说一下，逻辑回归算法是分类算法，我们将它作为分类算法使用。有时候可能因为这个算法的名字中出现了“回归”使你感到困惑，但逻辑回归算法实际上是一种分类算法.<br><a id="more"></a></p>
<h1 id="假设表示"><a href="#假设表示" class="headerlink" title="假设表示"></a>假设表示</h1><p>在分类问题中，要用什么样的函数来表示我们的假设。此前我们说过，希望我们的分类器的输出值在0和1之间，因此，我们希望想出一个满足某个性质的假设函数，这个性质是它的预测值要在0和1之间。<br>回顾在一开始提到的乳腺癌分类问题，我们可以用线性回归的方法求出适合数据的一条直线：<br><img src="/picture/machine-learning/logistic_regression3.jpg" alt="logistic_regression3"><br>根据线性回归模型我们只能预测连续的值，然而对于分类问题，我们需要输出0或1，我们可以预测：<br>当\(h_θ&gt;=0.5\)时，预测y=1。<br>当\(h_θ&lt;0.5\)时，预测y=0。<br>对于上图所示的数据，这样的一个线性模型似乎能很好地完成分类任务。假使我们又观测到一个非常大尺寸的恶性肿瘤，将其作为实例加入到我们的训练集中来，这将使得我们获得一条新的直线。<br><img src="/picture/machine-learning/logistic_regression4.jpg" alt="logistic_regression4"><br>这时，再使用 0.5作为阀值来预测肿瘤是良性还是恶性便不合适了。可以看出，线性回归模型，因为其预测的值可以超越[0,1]的范围，并不适合解决这样的问题。<br>我们引入一个新的模型，逻辑回归，该模型的输出变量范围始终在0和 1之间。逻辑回归模型的假设是：<br>$$h_\theta(x)=g(\theta^TX) \\\\<br>其中，X代表特征向量 \\\\<br>g代表逻辑函数(logistic function), 也叫S形函数（Sigmoid \ function), \\\\<br>公式为：g(z)=\frac{1}{1+e^{-z}}<br>$$<br>该函数的图像：<br><img src="/picture/machine-learning/logistic_regression5.jpg" alt="logistic_regression5"><br>合起来，我们得到逻辑回归模型的假设：<br>$$h_\theta(x)=\frac{1}{1+e^{-\theta^TX}}$$<br>\(h_\theta(x)\)的作用是，对于给定的输入变量，根据选择的参数计算输出变量=1的可能性(estimated probablity)，即:<br>$$h_\theta(x)=P(y=1|x;\theta)$$<br>例如，如果对于给定的 x，通过已经确定的参数计算得出\(h_θ(x)=0.7\)，则表示有70%的几率y为正向类，相应地y为负向类的几率为1-0.7=0.3。</p>
<h1 id="判定边界"><a href="#判定边界" class="headerlink" title="判定边界"></a>判定边界</h1><p>现在讲下决策边界(decision  boundary)的概念。这个概念能更好地帮助我们理解逻辑回归的假设函数在计算什么。<br><img src="/picture/machine-learning/logistic_regression6.jpg" alt="logistic_regression6"><br>在逻辑回归中，我们预测：</p>
<ul>
<li>当\(h_θ\)大于等于0.5时，预测y=1;</li>
<li>当\(h_θ\)小于0.5时，预测y=0;<br>根据上面绘制出的S形函数图像，我们知道:</li>
<li>当z=0时, g(z)=0.5;</li>
<li>当z&gt;0时, g(z)&gt;0.5;</li>
<li>当z&lt;0时, g(z)&lt;0.5.</li>
</ul>
<p>又\(z=\theta^TX\),即：<br>\(\theta^TX \ge 0\),预测y=1;<br>\(\theta^TX &lt; 0\),预测y=0;<br>现在假设我们有一个模型：<br><img src="/picture/machine-learning/logistic_regression7.jpg" alt="logistic_regression7"><br>并且参数θ是向量[-3 1 1]。则当\( (-3+x_1+x_2)\ge 0\)，即 \(x_1+x_2 \ge 3\)时,模型将预测y=1。<br>我们可以绘制直线\(x_1+x_2=3\)，这条线便是我们模型的分界线，将预测为1的区域和预测为0的区域分隔开。<br><img src="/picture/machine-learning/logistic_regression8.jpg" alt="logistic_regression8"><br>假使我们的数据呈现这样的分布情况，怎样的模型才能适合呢？<br><img src="/picture/machine-learning/logistic_regression9.jpg" alt="logistic_regression9"><br>因为需要用曲线才能分隔y=0的区域和y=1的区域，我们需要二次方特征：假设参数：\(h_θ(x)=g(θ_0+θ_1x_1+θ_2x_2+θ_3x_1^2+θ_4x_2^2)\)是[-1 0 0 1 1],则我们得到的判定边界恰好是圆点在原点且半径为1的圆形。<br>我们可以用非常复杂的模型来适应非常复杂形状的判定边界。</p>
<h1 id="代价函数"><a href="#代价函数" class="headerlink" title="代价函数"></a>代价函数</h1><p>接下来我们要介绍如何拟合逻辑回归模型的参数\(θ\)。具体来说，我要定义用来拟合参数的优化目标或者叫代价函数，这便是监督学习问题中的逻辑回归模型的拟合问题。<br><img src="/picture/machine-learning/logistic_regression10.jpg" alt="logistic_regression10"><br>对于线性回归模型，我们定义的代价函数是所有模型误差的平方和。理论上来说，我们也可以对逻辑回归模型沿用这个定义，但是问题在于:当我们将\(h_θ(x)=\frac{1}{1+e^{-θ^TX}}\)代入到这样定义了的代价函数中时，我们得到的代价函数将是一个非凸函数（no-convex function).<br><img src="/picture/machine-learning/logistic_regression11.jpg" alt="logistic_regression11"><br>这意味着我们的代价函数有许多局部最小值，<strong>这将影响梯度下降算法寻找全局最小值。</strong><br>线性回归的代价函数为：<br>$$J(\theta)=\frac{1}{2m}\sum_{i=1}^m\left(h_{\theta}(x^{(i)})-y^{(i)}\right)^2$$<br>我们重新定义逻辑回归的代价函数为:<br>$$J(\theta)=\frac{1}{m}\sum_{i=1}^mCost\left(h_{\theta}(x^{(i)}),y^{(i)}\right)$$<br>其中：<br>$$<br>\begin{eqnarray}<br>Cost(h_\theta(x),y)=<br>\begin{cases}<br>-log(h_\theta(x)), y=1 \cr -log(1-h_\theta(x)), y=0<br>\end{cases}<br>\end{eqnarray}<br>$$<br>\(h_θ(x)\)与\(Cost(h_θ(x),y)\)之间的关系如下图所示：<br><img src="/picture/machine-learning/logistic_regression12.jpg" alt="logistic_regression12"><br>这样构建的\(Cost(h_θ(x),y)\)函数的特点是：当实际的y=1且\(h_θ\)也为1时误差为 0，当y=1但\(h_θ\)不为1时误差随着\(h_θ\)的变小而变大；当实际的y=0且\(h_θ\)也为0时,代价为0，当y=0但\(h_θ\)不为0时误差随着\(h_θ\)的变大而变大。<br>将构建的Cost(hθ(x),y)简化如下:<br>$$Cost(h_θ(x),y)=-y*log(h_θ(x))-(1-y)*log(1-h_θ(x))$$</p>
<p>代入代价函数得到：<br>$$J(θ)=-\frac{1}{m}\sum_{i=1}^m\left(y^{(i)}log(h_θ(x^{(i)}))+(1-y^{(i)})log(1-h_θ(x^{(i)}))\right)$$<br>在得到这样一个代价函数以后，我们便可以用梯度下降算法来求得能使代价函数最小的参数了。算法为：<br><strong>repeat until convergence</strong>{<br>   $$\theta_j:=\theta_j-\alpha\frac{\partial}{\partial\theta_j}J(\theta) $$<br>}<br>求导后得到：<br><strong>repeat until convergence</strong>{<br>   $$\theta_j:=\theta_j-\alpha\sum_{i=1}^m\left((h_\theta(x^{(i)})-y^{(i)})x_j^{(i)}\right) $$<br>}</p>
<h2 id="推导过程"><a href="#推导过程" class="headerlink" title="推导过程"></a>推导过程</h2><p>所给的代价函数：<br>$$J(θ)=-\frac{1}{m}\sum_{i=1}^m\left(y^{(i)}log(h_θ(x^{(i)}))+(1-y^{(i)})log(1-h_θ(x^{(i)}))\right)$$<br>其中：<br>\(h_θ(x^{(i)})=\frac{1}{1+e^{-θ^TX}}\)<br>令\(z=θ^Tx=θ_0+θ_1x_1+θ_2x_2+…+θ_nx_n\)<br>则\(\frac{\partial z}{\partial(θ_j)}=x_j^{(i)}\)<br>\(h_θ(x^{(i)})=\frac{1}{1+e^{-z}}=\frac{e^z}{1+e^z}\) </p>
<p>$$J(θ)=-\frac{1}{m}\sum_{i=1}^m\left(y^{(i)}log(h_θ(x^{(i)}))+(1-y^{(i)})log(1-h_θ(x^{(i)}))\right)\\\\<br>=-\frac{1}{m}\sum_{i=1}^m\left(y^{(i)}log(h_θ(x^{(i)}))-y^{(i)}log(1-h_θ(x^{(i)}))+log(1-h_θ(x^{(i)}))\right) \\\\<br>= -\frac{1}{m}\sum_{i=1}^m\left(y^{(i)}log\left(\frac{h_θ(x^{(i)})}{1-h_θ(x^{(i)})}\right)+log(1-h_θ(x^{(i)}))\right) \\\\<br>= -\frac{1}{m}\sum_{i=1}^m\left(y^{(i)}log\left(\frac{\frac{e^{z^{(i)}}}{1+e^{z^{(i)}}}}{1-\frac{e^{z^{(i)}}}{1+e^{z^{(i)}}}}\right)+log(1-\frac{e^{z^{(i)}}}{1+e^{z^{(i)}}})\right) \\\\<br>= -\frac{1}{m}\sum_{i=1}^m\left(y^{(i)}z^{(i)}-log(1+e^{z^{(i)}})\right)<br>$$<br>$$<br>J’(θ)= -\frac{1}{m}\sum_{i=1}^m\left(y^{(i)}\frac{\partial z^{(i)}}{\partialθ_j}-\frac{e^{z^{(i)}}}{1+e^{z^{(i)}}}\frac{\partial z^{(i)}}{\partialθ_j}\right) \\\\<br>= -\frac{1}{m}\sum_{i=1}^m\left(y^{(i)}-h_θ(x^{(i)})\right)\frac{\partial z^{(i)}}{\partialθ_j} \\\\<br>= \frac{1}{m}\sum_{i=1}^m\left((h_θ(x^{(i)})-y^{(i)})x_j^{(i)}\right)<br>$$</p>
<p>注：虽然得到的梯度下降算法表面上看上去与线性回归的梯度下降算法一样，但是这里的\(h_θ(x)=g(θ^TX)\)与线性回归中不同，所以实际上是不一样的。另外，在运行梯度下降算法之T前，进行特征缩放依旧是非常必要的。<br>一些梯度下降算法之外的选择：除了梯度下降算法以外，还有一些常被用来令代价函数最小的算法，这些算法更加复杂和优越，而且通常不需要人工选择学习率，通常比梯度下降算法要更加快速。这些算法有：共轭梯度（Conjugate Gradient），局部优化法(Broyden fletcher goldfarb shann,BFGS)和有限内存局部优化法(LBFGS)。</p>
<h1 id="多类别分类-1对多"><a href="#多类别分类-1对多" class="headerlink" title="多类别分类 1对多"></a>多类别分类 1对多</h1><p>我们将谈到如何使用逻辑回归(logistic  regression)来解决多类别分类问题，具体来说，我想通过一个叫做”一对多” (one-vs-all)的分类算法。<br>先看这样一些例子。</p>
<ul>
<li>第一个例子：假如说你现在需要一个学习算法能自动地将邮件归类到不同的文件夹里，或者说可以自动地加上标签，那么，你也许需要一些不同的文件夹，或者不同的标签来完成<br>这件事，来区分开来自工作的邮件、来自朋友的邮件、来自家人的邮件或者是有关兴趣爱好的邮件，那么，我们就有了这样一个分类问题：其类别有四个，分别用y=1、y=2、y=3、y=4来代表。</li>
<li>第二个例子是有关药物诊断的，如果一个病人因为鼻塞来到你的诊所，他可能并没有生病，用y=1这个类别来代表；或者患了感冒，用y=2来代表；或者得了流感用y=3来代表。</li>
<li>第三个例子：如果你正在做有关天气的机器学习分类问题，那么你可能想要区分哪些天是晴天、多云、雨天、或者下雪天，对上述所有的例子，y可以取一个很小的数值，一个相对”谨慎”的数值，比如1到3、1到4或者其它数值。<br>以上说的都是多类分类问题。<br>然而对于之前的一个二元分类问题，我们的数据看起来可能是像这样：<br><img src="/picture/machine-learning/logistic_regression13.jpg" alt="logistic"><br>对于一个多类分类问题，我们的数据集或许看起来像这样：<br><img src="/picture/machine-learning/logistic_regression14.jpg" alt="logistic"><br>我用三种不同的符号来代表三个类别，问题就是给出三个类型的数据集，我们如何得到一个学习算法来进行分类呢？<br>我们现在已经知道如何进行二元分类，可以使用逻辑回归，对于直线或许你也知道，可以将数据集一分为二为正类和负类。用一对多的分类思想，我们可以将其用在多类分类问题上。<br>下面将介绍如何进行一对多的分类工作，有时这个方法也被称为”一对余”方法。<br>现在我们有一个训练集，好比上图表示的有三个类别，我们用三角形表示y=1，方框表示  y=2，叉叉表示y=3。我们下面要做的就是使用一个训练集，将其分成三个二元分类问题。我们先从用三角形代表的类别1开始，实际上我们可以创建一个，新的”伪”训练集，类型2和类型3定为负类，类型1设定为正类，我们创建一个新的训练集，如下图所示的那样，我们要拟合出一个合适的分类器。<br><img src="/picture/machine-learning/logistic_regression15.jpg" alt="logistic_regression15"></li>
</ul>
<p>这里的三角形是正样本，而圆形代表负样本。可以这样想，设置三角形的值为1，圆形的值为0，下面我们来训练一个标准的逻辑回归分类器，这样我们就得到一个正边界。<br>为了能实现这样的转变，我们将多个类中的一个类标记为正向类（y=1），然后将其他所有类都标记为负向类，这个模型记作\(h_\theta^{(1)}(x)\)。<br>接着，类似地第我们选择另一个类标记为正向类（y=2），再将其它类都标记为负向类，将这个模型记作\(h_\theta^{(2)}(x)\),依此类推。<br>最后我们得到一系列的模型简记为：\(h_\theta^{(i)}(x)=p(y=i|x;\theta)\)<br><img src="/picture/machine-learning/logistic_regression16.jpg" alt="logistic_regression16"><br>最后，在我们需要做预测时，我们将所有的分类机都运行一遍，然后对每一个输入变量，都选择最高可能性的输出变量。<br>总之，我们已经把要做的做完了，现在要做的就是训练这个逻辑回归分类器：\(h_\theta^{(i)}(x)\)其中i对应每一个可能的y=i，最后,为了做出预测，我们给出输入一个新的x值，用这个做预测。我们要做的就是在我们三个分类器里面输入x，然后我们选择一个让\(h_\theta^{(i)}(x)\)最大的i,即\(\max_{i}h_\theta^{(i)}(x)\)<br>现在知道了基本的挑选分类器的方法，选择出哪一个分类器是可信度最高效果最好的,那么就可认为得到一个正确的分类，无论i值是多少，我们都有最高的概率值，我们预测y就是那个值。这就是多类别分类问题，以及一对多的方法，通过这个小方法，现在也可以将逻辑回归分类器用在多类分类的问题上。</p>
<h1 id="正则化"><a href="#正则化" class="headerlink" title="正则化"></a>正则化</h1><p>到现在为止，我们已经学习了几种不同的学习算法，包括线性回归和逻辑回归，它们能够有效地解决许多问题，但是当将它们应用到某些特定的机器学习应用时，会遇到过度拟合 (over-fitting)的问题，可能会导致它们效果很差。<br>接下来，我们将谈论一种称为正则化(regularization)的技术，它可以改善或者减少过度拟合问题。<br>下图是一个回归问题的例子：<br><img src="/picture/machine-learning/logistic_regression17.jpg" alt="logistic_regression17"><br>第一个模型是一个线性模型，欠拟合，不能很好地适应我们的训练集；第三个模型是一个四次方的模型，过于强调拟合原始数据，而丢失了算法的本质：预测新数据。我们可以看出，若给出一个新的值使之预测，它将表现的很差，是过拟合，虽然能非常好地适应我们的训练集但在新输入变量进行预测时可能会效果不好；而中间的模型似乎最合适。<br>分类问题中也存在这样的问题：<br><img src="/picture/machine-learning/logistic_regression18.jpg" alt="logistic_regression18"><br>就以多项式理解，x的次数越高，拟合的越好，但相应的预测的能力就可能变差。<br>问题是，如果我们发现了过拟合问题，应该如何处理？</p>
<ul>
<li>丢弃一些不能帮助我们正确预测的特征。可以是手工选择保留哪些特征，或者使用一些模型选择的算法来帮忙（例如PCA）</li>
<li>正则化。保留所有的特征，但是减少参数的大小（magnitude）。<h2 id="代价函数-1"><a href="#代价函数-1" class="headerlink" title="代价函数"></a>代价函数</h2>上面的回归问题中如果我们的模型是：<br>$$h_\theta(x)=\theta_0+\theta_1x_1+\theta_2x_2^2+\theta_3x_3^3+\theta_4x_4^4$$<br>我们可以从之前的事例中看出，正是那些高次项导致了过拟合的产生，所以如果我们能让这些高次项的系数接近于0的话，我们就能很好的拟合了。<br>所以我们要做的就是在一定程度上减小这些参数θ的值，这就是正则化的基本方法。我们决定要减少\(θ_3\)和\(θ_4\)的大小，我们要做的便是修改代价函数，在其中\(θ_3\)和\(θ_4\)设置一点惩罚。这样做的话，我们在尝试最小化代价时也需要将这个惩罚纳入考虑中，并最终导致选择较小一些的\(θ_3\)和\(θ_4\)。修改后的代价函数如下：<br>$$\min_θ\frac{1}{2m}\sum_{i=1}^m(h_θ(x^{(i)})-y^{(i)})^2+1000θ_3^2+10000θ_4^2$$<br>通过这样的代价函数选择出的\(θ_3\)和\(θ_4\)对预测结果的影响就比之前要小许多。假如我们有非常多的特征，我们并不知道其中哪些特征我们要惩罚，我们将对所有的特征进行惩罚，并且让代价函数最优化的软件来选择这些惩罚的程度。这样的结果是得到了一个较为简单的能防止过拟合问题的假设：<br>$$J(θ)=\frac{1}{2m}\sum_{i=1}^m(h_θ(x^{(i)})-y^{(i)})^2+\lambda\sum_{j=1}^nθ_j^2$$<br>其中λ又称为正则化参数（Regularization Parameter）。注：根据惯例，我们不对\(θ_0\)进行惩罚。经过正则化处理的模型与原模型的可能对比如下图所示：<br><img src="/picture/machine-learning/logistic_regression19.jpg" alt="regularization"><br>如果选择的正则化参数λ过大，则会把所有的参数都最小化了，导致模型变成\(h_θ(x)=θ_0\).也就是上图中红色直线所示的情况，造成欠拟合。<br>那为什么增加的一项\(\lambda\sum_{j=1}^nθ_j^2\),可以使θ的值减小呢？<br>因为如果我们令λ的值很大的话,为了使Cost Function尽可能的小，所有的θ的值（不包括 \(θ_0\)）都会在一定程度上减小。<br>但若λ的值太大了,那么θ（不包括\(θ_0\)）都会趋近于0，这样我们所得到的只能是一条平行于x轴的直线。<br>所以对于正则化，我们要取一个合理的λ的值，这样才能更好的应用正则化。<br>回顾一下代价函数，为了使用正则化，让我们把这些概念应用到到线性回归和逻辑回归中去，那么我们就可以让他们避免过度拟合了。<h2 id="正则化线性回归"><a href="#正则化线性回归" class="headerlink" title="正则化线性回归"></a>正则化线性回归</h2>对于线性回归的求解，我们之前推导了两种学习算法：一种基于梯度下降，一种基于正规方程。<br>正则化线性回归的代价函数为：<br>$$J(θ)=\frac{1}{2m}\sum_{i=1}^m(h_θ(x^{(i)})-y^{(i)})^2+\lambda\sum_{j=1}^nθ_j^2$$<br>如果我们要使用梯度下降法令这个代价函数最小化，因为我们未对\(θ_0\)进行正则化，所以梯度下降算法将分两种情形：<br><strong>repeat until convergence</strong>{<br> $$\theta_0:=\theta_0-\alpha\frac{1}{m}\sum_{i=1}^m((h_{\theta}(x^{(i)})-y^{(i)}).x_0^{(i)}) \\\ <br> \theta_j:=\theta_j-\alpha\frac{1}{m}\sum_{i=1}^m((h_{\theta}(x^{(i)})-y^{(i)})*x_j^{(i)}+\frac{\lambda}{m}\theta_j) \\\\<br> for j=1,2,…,n$$<br>}<br>对上面j=1,2…n时的更新式子进行调整可得：<br>$$\theta_j:=\theta_j(1-\alpha\frac{\lambda}{m})-\alpha\frac{1}{m}\sum_{i=1}^m(h_\theta(x^{(i)})-y^{(i)})x_j^{(i)}$$<br>可以看出，正则化线性回归的梯度下降算法的变化在于，每次都在原有算法更新规则的基础上令θ值减少了一个额外的值。<br>我们同样也可以利用正规方程来求解正则化线性回归模型，方法如下所示：<br>$$θ=\left(X^TX+\lambda I\right)^{-1}X^Ty \\\\<br>其中I为对角矩阵,并且第一个元素为0,其余对角元素都为1$$<h2 id="正则化的逻辑回归模型"><a href="#正则化的逻辑回归模型" class="headerlink" title="正则化的逻辑回归模型"></a>正则化的逻辑回归模型</h2><img src="/picture/machine-learning/logistic_regression20.jpg" alt="regularization"><br>同样对于逻辑回归，我们也给代价函数增加一个正则化的表达式，得到代价函数：<br>$$J(θ)=-\frac{1}{m}\sum_{i=1}^m\left(y^{(i)}log(h_θ(x^{(i)}))+(1-y^{(i)})log(1-h_θ(x^{(i)}))\right)+\frac{\lambda}{2m}\sum_{j=1}^nθ_j^2$$<br>要最小化该代价函数，通过求导，得出梯度下降算法为：<br><strong>repeat until convergence</strong>{<br> $$\theta_0:=\theta_0-\alpha\frac{1}{m}\sum_{i=1}^m((h_{\theta}(x^{(i)})-y^{(i)}).x_0^{(i)}) \\\ <br> \theta_j:=\theta_j-\alpha\frac{1}{m}\sum_{i=1}^m((h_{\theta}(x^{(i)})-y^{(i)})*x_j^{(i)}+\frac{\lambda}{m}\theta_j) \\\\<br> for j=1,2,…,n$$<br>}</li>
</ul>
<p><strong>注：看上去同线性回归一样，但是逻辑回归\(h_θ(x)=g(θ^TX)\)，而线性回归\(h_θ(x)=θ^TX\),所以二者不同。</strong></p>
<h1 id="广义线性模型与逻辑回归"><a href="#广义线性模型与逻辑回归" class="headerlink" title="广义线性模型与逻辑回归"></a>广义线性模型与逻辑回归</h1><p>广义线性模型的介绍见：<a href="/2017/02/09/线性回归/">线性回归-拓展 广义线性模型与线性回归</a><br>实际上，逻辑回归中：我们有如下假设：<br>$$y|x;θ \sim Bernoulli(\phi)$$<br>即，假设y的条件概率服从伯努利分布，伯努利分布也是广义线性模型的一种特例。</p>
<h2 id="Bernoulli分布的指数分布簇形式"><a href="#Bernoulli分布的指数分布簇形式" class="headerlink" title="Bernoulli分布的指数分布簇形式"></a>Bernoulli分布的指数分布簇形式</h2><p>广义线性模型的指数分布簇形式：<br>$$p(y;\eta)=b(y)exp(\eta^{T}T(y)-a(\eta))$$<br>伯努利分布：<br>$$p(y=1;\phi)=\phi; \ p(y=0;\phi)=1-\phi$$<br>=&gt;<br>$$p(y;\phi)=\phi^y(1-\phi)^{1-y} \\\\<br>=exp(ylog\phi+(1-y)log(1-\phi)) \\\\<br>=exp\left(  \left(log\left(\frac{\phi}{1-\phi}\right)\right)y+log(1-\phi)\right)<br>$$<br>即：在如下参数下，广义线性模型是伯努利分布<br>$$\eta=log(\frac{\phi}{1-\phi}) =&gt; \phi = \frac{1}{1+e^{-\eta}} \\\\<br>T(y)=y \\\\<br>a(\eta)=-log(1-\eta)=log(1+e^\eta) \\\\<br>b(y)=1<br>$$</p>
<h2 id="广义线性模型推导逻辑回归"><a href="#广义线性模型推导逻辑回归" class="headerlink" title="广义线性模型推导逻辑回归"></a>广义线性模型推导逻辑回归</h2><ul>
<li>step1: \(y|x;\theta \sim Bernoulli(\phi)\)</li>
<li>step2: 由假设2: \(h(x)=E[y|x]\) 得到：<br>$$h_\theta(x)=E[y|x;\Theta]\\\\<br> =\phi \\\\<br> =\frac{1}{1+e^{-\eta}}  \\\\<br> =\frac{1}{1+e^{-\Theta^Tx}}  \\\\<br> 其中, E[y|x;\Theta]=\phi由假设1得到；\\\\<br>\phi=\frac{1}{1+e^{-\eta}}由伯努利分布对应的广义线性模型参数得到；\\\\<br>\eta = \Theta^Tx由假设3得到。<br>$$</li>
</ul>
<p><strong>注：可以看出Sigmoid function得到的实际上就是\(\phi\)的值，也就是预测y=1的概率。</strong></p>
<h1 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h1><p><a href="http://open.163.com/special/opencourse/machinelearning.html" target="_blank" rel="external">斯坦福大学机器学习视频教程</a><br><a href="https://www.zhihu.com/question/47637500?sort=created" target="_blank" rel="external">知乎：为什么广义线性模型GLM要求被解释变量属于指数分布簇</a><br><a href="https://zhuanlan.zhihu.com/p/22876460" target="_blank" rel="external">知乎：广义线性模型</a>  </p>
]]></content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;分类问题&quot;&gt;&lt;a href=&quot;#分类问题&quot; class=&quot;headerlink&quot; title=&quot;分类问题&quot;&gt;&lt;/a&gt;分类问题&lt;/h1&gt;&lt;p&gt;在分类问题中，你要预测的变量  y是离散的值，我们将学习一种叫做逻辑回归(Logistic Regression)的算法，这是目前最流行使用最广泛的一种学习算法。&lt;/p&gt;
&lt;p&gt;在分类问题中，我们尝试预测的是结果是否属于某一个类（例如正确或错误）。分类问题的例子有：判断一封电子邮件是否是垃圾邮件；判断一次金融交易是否是欺诈；之前我们也谈到了肿瘤分类问题的例子，区别一个肿瘤是恶性的还是良性的。&lt;/p&gt;
&lt;p&gt;我们从二元的分类问题开始讨论。我们将因变量(dependant variable)可能属于的两个类分别称为负向类（negative class）和正向类（positive class），则因变量 \(y\in{0,1}\) 其中0表示负向类，1表示正向类。&lt;br&gt;&lt;img src=&quot;/picture/machine-learning/logistic_regression1.jpg&quot; alt=&quot;logistic_regression&quot;&gt;&lt;br&gt;&lt;img src=&quot;/picture/machine-learning/logistic_regression2.jpg&quot; alt=&quot;logistic_regression2&quot;&gt;&lt;br&gt;如果我们要用线性回归算法来解决一个分类问题，对于分类，y取值为     0或者1，但如果你使用的是线性回归，那么假设函数的输出值可能远大于  1，或者远小于0，即使所有训练样本的标签y都等于0或1。尽管我们知道标签应该取值0或者1，但是如果算法得到的值远大于1或者远小于0的话，就会感觉很奇怪。所以我们在接下来的要研究的算法就叫做逻辑回归算法，这个算法的性质是：它的输出值永远在0到1之间。&lt;/p&gt;
&lt;p&gt;顺便说一下，逻辑回归算法是分类算法，我们将它作为分类算法使用。有时候可能因为这个算法的名字中出现了“回归”使你感到困惑，但逻辑回归算法实际上是一种分类算法.&lt;br&gt;
    
    </summary>
    
      <category term="机器学习" scheme="xtf615.com/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="机器学习" scheme="xtf615.com/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="广义线性模型" scheme="xtf615.com/tags/%E5%B9%BF%E4%B9%89%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8B/"/>
    
      <category term="逻辑回归" scheme="xtf615.com/tags/%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92/"/>
    
  </entry>
  
  <entry>
    <title>基于jieba分词和nltk的情感分析</title>
    <link href="xtf615.com/2017/02/10/%E5%9F%BA%E4%BA%8Ejieba%E5%88%86%E8%AF%8D%E5%92%8Cnltk%E7%9A%84%E6%83%85%E6%84%9F%E5%88%86%E6%9E%90/"/>
    <id>xtf615.com/2017/02/10/基于jieba分词和nltk的情感分析/</id>
    <published>2017-02-10T10:45:56.000Z</published>
    <updated>2017-02-11T07:33:21.302Z</updated>
    
    <content type="html"><![CDATA[<h1 id="自然语言处理NLP"><a href="#自然语言处理NLP" class="headerlink" title="自然语言处理NLP"></a>自然语言处理NLP</h1><p>情感分析作为自然语言处理的一个部分，让我们首先看一下自然语言处理。</p>
<h2 id="相关技术及运用"><a href="#相关技术及运用" class="headerlink" title="相关技术及运用"></a>相关技术及运用</h2><p>自动问答（Question Answering，QA）：它是一套可以理解复杂问题，并以充分的准确度、可信度和速度给出答案的计算系统，以IBM‘s Waston为代表；<br>信息抽取（Information Extraction，IE）：其目的是将非结构化或半结构化的自然语言描述文本转化结构化的数据，如自动根据邮件内容生成Calendar；<br>情感分析（Sentiment Analysis，SA）：又称倾向性分析和意见挖掘，它是对带有情感色彩的主观性文本进行分析、处理、归纳和推理的过程，如从大量网页文本中分析用户对“数码相机”的“变焦、价格、大小、重量、闪光、易用性”等属性的情感倾向；<br>机器翻译（Machine Translation，MT）：将文本从一种语言转成另一种语言，如中英机器翻译。</p>
<h2 id="发展现状"><a href="#发展现状" class="headerlink" title="发展现状"></a>发展现状</h2><p>基本解决：词性标注、命名实体识别、Spam识别<br>取得长足进展：情感分析、共指消解、词义消歧、句法分析、机器翻译、信息抽取<br>挑战：自动问答、复述、文摘、会话机器人<br><img src="/picture/machine-learning/nlp_process.png" alt="nlp_process"><br><a id="more"></a>  </p>
<h2 id="NLP主要难点——歧义问题"><a href="#NLP主要难点——歧义问题" class="headerlink" title="NLP主要难点——歧义问题"></a>NLP主要难点——歧义问题</h2><ul>
<li>词法分析歧义：<br>1）分词， 如“严守一把手机关了”，可能的分词结果“严守一/ 把/ 手机/ 关/ 了” 和“严守/ 一把手/ 机关/ 了”。2）词性标注， 如“计划”在不同上下文中有不同的词性：“我/ 计划/v 考/ 研/”和“我/ 完成/ 了/ 计划/n”</li>
<li>语法分析歧义：<br>“那只狼咬死了猎人的狗”。 ”咬死了猎人的狗失踪了”。</li>
<li>语义分析歧义：<br>计算机会像你的母亲那样很好的理解你（的语言）： 1）计算机理解你喜欢你的母亲。2）计算机会像很好的理解你的母亲那样理解你</li>
<li>NLP应用中的歧义：<br>音字转换：拼音串“ji qi fan yi ji qi ying yong ji qi le ren men ji qi nong hou de xing qu”中的“ji qi”如何转换成正确的词条<h2 id="为什么自然语言理解如此困难？"><a href="#为什么自然语言理解如此困难？" class="headerlink" title="为什么自然语言理解如此困难？"></a>为什么自然语言理解如此困难？</h2></li>
<li>用户生成内容中存在大量口语化、成语、方言等非标准的语言描述</li>
<li>分词问题</li>
<li>新词不断产生</li>
<li>基本常识与上下文知识</li>
<li>各式各样的实体词</li>
</ul>
<p>为了解决以上难题，我们需要掌握较多的语言学知识，构建知识库资源，并找到一种融合各种知识、资源的方法，目前使用较多是概率模型 （probabilistic model）或称为统计模型（statistical model），或者称为“经验主义模型”，其建模过程基于大规模真实语料库，从中各级语言单位上的统计信息，并且，依据较低级语言单位上的统计信息，运行 相关的统计、推理等技术计算较高级语言单位上的统计信息。与其相对的“理想主义模型”，即基于Chomsky形式语言的确定性语言模型，它建立在人脑中先 天存在语法规则这一假设基础上，认为语言是人脑语言能力推导出来的，建立语言模型就是通过建立人工编辑的语言规则集来模拟这种先天的语言能力。</p>
<h1 id="情感分析概念"><a href="#情感分析概念" class="headerlink" title="情感分析概念"></a>情感分析概念</h1><p>情感分析（Sentiment analysis），又称倾向性分析，意见抽取（Opinion extraction），意见挖掘（Opinion mining），情感挖掘（Sentiment mining），主观分析（Subjectivity analysis），它是对带有情感色彩的主观性文本进行分析、处理、归纳和推理的过程，如从评论文本中分析用户对“数码相机”的“变焦、价格、大小、重 量、闪光、易用性”等属性的情感倾向。</p>
<h2 id="示例"><a href="#示例" class="headerlink" title="示例"></a>示例</h2><ul>
<li><p><strong>从电影评论中识别用户对电影的褒贬评价</strong><br><img src="/picture/machine-learning/movie_comment.png" alt="movie_comment"></p>
</li>
<li><p>Google Product Search识别用户对产品各种属性的评价，并从评论中选择代表性评论展示给用户<br><img src="/picture/machine-learning/product_comment.png" alt="product_comment"></p>
</li>
<li><p>微信新闻识别用户对新闻的各种评价，并从评论中选择代表性评论展示给用户</p>
</li>
<li><p>Twitter sentiment versus Gallup Poll of Consumer Confidence：挖掘Twitter中的用户情感发现，其与传统的调查、投票等方法结果有高度的一致性。<br>下图中2008年到2009年初，网民情绪低谷是金融危机导致，从2009年5月份开始慢慢恢复。<br><img src="/picture/machine-learning/mental.png" alt="mental"></p>
</li>
<li><p>Twitter sentiment: 通过Twitter用户情感预测股票走势。<br>2012年5月，世界首家基于社交媒体的对冲基金 Derwent Capital Markets 在屡次跳票后终于上线。它会即时关注Twitter 中的公众情绪指导投资。正如基金创始人保罗•郝汀（Paul Hawtin）表示：“长期以来，投资者已经广泛地认可金融市场由恐惧和贪婪驱使，但我们从未拥有一种技术或数据来量化人们的情感。”一直为金融市场非理性举动所困惑的投资者，终于有了一扇可以了解心灵世界的窗户——那便是 Twitter 每天浩如烟海的推文，在一份八月份的报道中显示，利用 Twitter 的对冲基金 Derwent Capital Markets 在首月的交易中已经盈利，它以1.85%的收益率，让平均数只有0.76%的其他对冲基金相形见绌。类似的工作还有预测电影票房、选举结果等，均是将公众 情绪与社会事件对比，发现一致性，并用于预测，如将“冷静CLAM”情绪指数后移3天后和道琼斯工业平均指数DIJA惊人一致。<br><img src="/picture/machine-learning/stock_predict.png" alt="stock"></p>
</li>
<li><p>Target Sentiment on Twitter（Twitter Sentiment App）：对Twitter中包含给定query的tweets进行情感分类。对于公司了解用户对公司、产品的喜好，用于指导改善产品和服务，公司还可以 据此发现竞争对手的优劣势，用户也可以根据网友甚至亲友评价决定是否购买特定产品。详细见论文：Alec Go, Richa Bhayani, Lei Huang. 2009. Twitter Sentiment Classification using Distant Supervision.<br><img src="/picture/machine-learning/search_sentiment.png" alt="search"></p>
</li>
</ul>
<h2 id="情感分析内容"><a href="#情感分析内容" class="headerlink" title="情感分析内容"></a>情感分析内容</h2><h3 id="任务"><a href="#任务" class="headerlink" title="任务"></a>任务</h3><p>情感分析主要目的就是识别用户对事物或人的看法、态度。<br>参与主体主要包括：</p>
<ul>
<li>Holder (source) of attitude：观点持有者</li>
<li>Target (aspect) of attitude：评价对象</li>
<li>Type of attitude：评价观点</li>
<li>From a set of types：观点类型：Like, love, hate, value, desire, etc.<br>Or (more commonly) simple weighted polarity: positive, negative, neutral,together with strength</li>
<li>Text containing the attitude：评价文本，一般是句子或整篇文档。</li>
<li>更细更深入的还包括评价属性，情感词/极性词，评价搭配等。</li>
</ul>
<p>通常，我们面临的情感分析任务包括如下几类：</p>
<ul>
<li>是正面还是反面情绪？<br>Simplest task: Is the attitude of this text positive or negative?</li>
<li>排序态度：<br>More complex: Rank the attitude of this text from 1 to 5</li>
<li>检测目的、观点等：<br>Advanced: Detect the target, source, or complex attitude types</li>
</ul>
<h2 id="词典匹配VS机器学习"><a href="#词典匹配VS机器学习" class="headerlink" title="词典匹配VS机器学习"></a>词典匹配VS机器学习</h2><p>不是有词典匹配的方法了吗？怎么还搞多个机器学习方法。因为词典方法和机器学习方法各有千秋。<br>机器学习的方法精确度更高，因为词典匹配会由于语义表达的丰富性而出现很大误差，而机器学习方法不会。而且它可使用的场景更多样。无论是主客观分类还是正负面情感分类，机器学习都可以完成任务。而无需像词典匹配那样要深入到词语、句子、语法这些层面。<br>而词典方法适用的语料范围更广，无论是手机、电脑这些商品，还是书评、影评这些语料，都可以适用。但机器学习则极度依赖语料，把手机语料训练出来的的分类器拿去给书评分类，那是注定要失败的。<br>使用机器学习进行情感分析，可以换一个相同意思的说法，就是用有监督的（需要人工标注类别）机器学习方法来对文本进行分类。<br>这点与词典匹配有着本质的区别。<strong>词典匹配是直接计算文本中的情感词，得出它们的情感倾向分值</strong>。而<strong>机器学习方法的思路是先选出一部分表达积极情感的文本和一部分表达消极情感的文本，用机器学习方法进行训练，获得一个情感分类器。再通过这个情感分类器对所有文本进行积极和消极的二分分类</strong>。最终的分类可以为文本给出0或1这样的类别，也可以给出一个概率值，比如”这个文本的积极概率是90%，消极概率是10%“。</p>
<h2 id="NLTK"><a href="#NLTK" class="headerlink" title="NLTK"></a>NLTK</h2><p>Python 有良好的程序包可以进行情感分类，那就是Python 自然语言处理包，Natural Language Toolkit ，简称NLTK 。NLTK 当然不只是处理情感分析，NLTK 有着整套自然语言处理的工具，从分词到实体识别，从情感分类到句法分析，完整而丰富，功能强大。实乃居家旅行，越货杀人之必备良药。<br>另外，<strong>NLTK 新增的scikit-learn 的接口</strong>，使得它的分类功能更为强大好用了，可以用很多高端冷艳的分类算法了。<br>有了scikit-learn 的接口，NLTK 做分类变得比之前更简单快捷，但是相关的结合NLTK 和 sciki-learn 的文章实在少。</p>
<h1 id="构建情感分析工具流程"><a href="#构建情感分析工具流程" class="headerlink" title="构建情感分析工具流程"></a>构建情感分析工具流程</h1><h2 id="人工标注"><a href="#人工标注" class="headerlink" title="人工标注"></a>人工标注</h2><p>有监督意味着需要人工标注，需要人为的给文本一个类标签。比如我有5000条商品评论，如果我要把这些评论分成积极和消极两类。那我就可以先从里面选2000条评论，然后对这2000条数据进行人工标注，把这2000条评论标为“积极”或“消极”。这“积极”和“消极”就是类标签。 假设有1000条评论被标为“积极”，有1000条评论被标为“消极”。（两者数量相同对训练分类器是有用的，如果实际中数量不相同，应该减少和增加数据以使得它们数量相同）</p>
<h2 id="特征选择"><a href="#特征选择" class="headerlink" title="特征选择"></a>特征选择</h2><p>特征就是分类对象所展现的部分特点，是实现分类的依据。我们经常会做出分类的行为，那我们依据些什么进行分类呢？ 举个例子，如果我看到一个年轻人，穿着新的正装，提着崭新的公文包，快步行走，那我就会觉得他是一个刚入职的职场新人。在这里面，“崭新”，“正装”，“公文包”，“快步行走”都是这个人所展现出的特点，也是我用来判断这个人属于哪一类的依据。这些特点和依据就是特征。可能有些特征对我判断更有用，有些对我判断没什么用，有些可能会让我判断错误，但这些都是我分类的依据。<br>我们没办法发现一个人的所有特点，所以我们没办法客观的选择所有特点，我们只能主观的选择一部分特点来作为我分类的依据。这也是特征选择的特点，需要人为的进行一定选择。<br><strong>而在情感分类中，一般从“词”这个层次来选择特征。</strong><br>比如这句话“手机非常好用！”，我给了它一个类标签“Positive”。里面有四个词（把感叹号也算上），“手机”，“非常”，“好用”，“！”。我可以认为这4个词都对分类产生了影响，都是分类的依据。也就是无论什么地方出现了这四个词的其中之一，文本都可以被分类为“积极”。这个是把所有词都作为分类特征。<br>同样的，对这句话，我也可以选择它的双词搭配（Bigrams）作为特征。比如“手机 非常”，“非常 好用”，“好用 ！”这三个搭配作为分类的特征。以此类推，三词搭配（Trigrams），四词搭配都是可以被作为特征的。</p>
<h2 id="特征降维"><a href="#特征降维" class="headerlink" title="特征降维"></a>特征降维</h2><p>特征降维说白了就是减少特征的数量。这有两个意义，一个是特征数量减少了之后可以加快算法计算的速度（数量少了当然计算就快了），另一个是如果用一定的方法选择信息量丰富的特征，可以减少噪音，有效提高分类的准确率。<br>所谓信息量丰富，可以看回上面这个例子“手机非常好用！”，很明显，其实不需要把“手机”，“非常”，“好用”，“！”这4个都当做特征，因为“好用”这么一个词，或者“非常 好用”这么一个双词搭配就已经决定了这个句子是“积极”的。这就是说，“好用”这个词的信息量非常丰富。<br><strong>那要用什么方法来减少特征数量呢？</strong><br>答案是通过一定的统计方法找到信息量丰富的特征。<br>统计方法包括：词频（Term Frequency）、文档频率（Document Frequency）、互信息（Pointwise Mutual Information）、信息熵（Information Entropy）、卡方统计（Chi-Square）等等。<br>在情感分类中，用词频选择特征，也就是选在语料库中出现频率高的词。比如我可以选择语料库中词频最高的2000个词作为特征。用文档频率选特征，是选在语料库的不同文档中出现频率最高的词。其他类似，都是要通过某个统计方法选择信息量丰富的特征。特征可以是词，可以是词组合。</p>
<h2 id="文本特征化"><a href="#文本特征化" class="headerlink" title="文本特征化"></a>文本特征化</h2><p>在使用分类算法进行分类之前，第一步是要把所有原始的语料文本转化为特征表示的形式。<br>还是以上面那句话做例子，“手机非常好用！”<br>如果在NLTK 中，如果选择所有词作为特征，其形式是这样的：[ {“手机”: True, “非常”: True, “好用”: True, “！”: True} , positive]<br>如果选择双词作为特征，其形式是这样的：[ {“手机 非常”: True, “非常 好用”: True, “好用 ！”: True} , positive ]<br>如果选择信息量丰富的词作为特征，其形式是这样的：[ {“好用”: True} , positive ]<br>无论使用什么特征选择方法，其形式都是一样的。都是[ {“特征1”: True, “特征2”: True, “特征N”: True, }, 类标签 ]</p>
<h2 id="划分数据集"><a href="#划分数据集" class="headerlink" title="划分数据集"></a>划分数据集</h2><p>把用特征表示之后的文本分成开发集和测试集，把开发集分成训练集和验证集。机器学习分类必须有数据给分类算法训练，这样才能得到一个（基于训练数据的）分类器。有了分类器之后，就需要检测这个分类器的准确度。</p>
<h2 id="分类算法学习"><a href="#分类算法学习" class="headerlink" title="分类算法学习"></a>分类算法学习</h2><p>这个时候终于可以使用各种高端冷艳的机器学习算法啦！<br>我们的目标是：找到最佳的机器学习算法。<br>可以使用朴素贝叶斯（NaiveBayes），决策树（Decision Tree）等NLTK 自带的机器学习方法。也可以更进一步，使用NLTK 的scikit-learn 接口，这样就可以调用scikit-learn 里面的所有。</p>
<h2 id="预测"><a href="#预测" class="headerlink" title="预测"></a>预测</h2><p>在终于得到最佳分类算法和特征维度（数量）之后，就可以动用测试集。<br>直接用最优的分类算法对测试集进行分类，得出分类结果。对比分类器的分类结果和人工标注的正确结果，给出分类器的最终准确度。</p>
<h1 id="开发实践"><a href="#开发实践" class="headerlink" title="开发实践"></a>开发实践</h1><p>本次实践只是简单的进行文本评论正反面的预测。选取的材料是京东商城酒类商品的评论。</p>
<h2 id="准备工作"><a href="#准备工作" class="headerlink" title="准备工作"></a>准备工作</h2><p>准备人工标注好的好评和差评文本. good.txt  bad.txt</p>
<ul>
<li>好评<br><img src="/picture/machine-learning/good.jpg" alt="good"></li>
<li>差评<br><img src="/picture/machine-learning/bad.jpg" alt="bad"></li>
<li>停用词（上网查找的）<br><img src="/picture/machine-learning/stop.jpg" alt="stop"><h2 id="特征提取和选择"><a href="#特征提取和选择" class="headerlink" title="特征提取和选择"></a>特征提取和选择</h2></li>
<li>使用jieba分词对文本进行分词<br><img src="/picture/machine-learning/jieba_cut.jpg" alt="jieba"></li>
<li>构建特征<br><img src="/picture/machine-learning/feature_selection_nlp.jpg" alt="feature_selection_nlp"></li>
<li>选择特征<br><img src="/picture/machine-learning/feature_selection_nlp2.jpg" alt="feature_selection_nlp"><br><img src="/picture/machine-learning/feature_selection_nlp3.jpg" alt="feature_selection_nlp"><h2 id="划分数据集-1"><a href="#划分数据集-1" class="headerlink" title="划分数据集"></a>划分数据集</h2><img src="/picture/machine-learning/split_data.jpg" alt="split_data"><h2 id="构建分类器"><a href="#构建分类器" class="headerlink" title="构建分类器"></a>构建分类器</h2><img src="/picture/machine-learning/classifier_sentiment.jpg" alt="classifier_sentiment"><br><img src="/picture/machine-learning/classifier_sentiment2.jpg" alt="classifier_sentiment"><h2 id="预测-1"><a href="#预测-1" class="headerlink" title="预测"></a>预测</h2><img src="/picture/machine-learning/predict_sentiment.jpg" alt="predict_sentiment"><br>下图是朴素贝叶斯得到的结果：可以看到正确率达到了90%<br><img src="/picture/machine-learning/predict_sentiment2.jpg" alt="predict_sentiment"><br>下图是其他算法得到的结果：可以看到逻辑回归和线性SVM正确率都在96%以上，效果不错。<br><img src="/picture/machine-learning/predict_sentiment3.jpg" alt="predict_sentiment"></li>
</ul>
<h1 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h1><p><a href="http://www.open-open.com/lib/view/open1421114964515.html" target="_blank" rel="external">大数据文摘：斯坦福大学怎样讲“情感分析”</a><br><a href="http://www.nltk.org/" target="_blank" rel="external">NLTK官网</a><br><a href="http://streamhacker.com/" target="_blank" rel="external">StreamHacker</a><br><a href="http://andybromberg.com/sentiment-analysis-python/" target="_blank" rel="external">Andybromberg</a></p>
]]></content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;自然语言处理NLP&quot;&gt;&lt;a href=&quot;#自然语言处理NLP&quot; class=&quot;headerlink&quot; title=&quot;自然语言处理NLP&quot;&gt;&lt;/a&gt;自然语言处理NLP&lt;/h1&gt;&lt;p&gt;情感分析作为自然语言处理的一个部分，让我们首先看一下自然语言处理。&lt;/p&gt;
&lt;h2 id=&quot;相关技术及运用&quot;&gt;&lt;a href=&quot;#相关技术及运用&quot; class=&quot;headerlink&quot; title=&quot;相关技术及运用&quot;&gt;&lt;/a&gt;相关技术及运用&lt;/h2&gt;&lt;p&gt;自动问答（Question Answering，QA）：它是一套可以理解复杂问题，并以充分的准确度、可信度和速度给出答案的计算系统，以IBM‘s Waston为代表；&lt;br&gt;信息抽取（Information Extraction，IE）：其目的是将非结构化或半结构化的自然语言描述文本转化结构化的数据，如自动根据邮件内容生成Calendar；&lt;br&gt;情感分析（Sentiment Analysis，SA）：又称倾向性分析和意见挖掘，它是对带有情感色彩的主观性文本进行分析、处理、归纳和推理的过程，如从大量网页文本中分析用户对“数码相机”的“变焦、价格、大小、重量、闪光、易用性”等属性的情感倾向；&lt;br&gt;机器翻译（Machine Translation，MT）：将文本从一种语言转成另一种语言，如中英机器翻译。&lt;/p&gt;
&lt;h2 id=&quot;发展现状&quot;&gt;&lt;a href=&quot;#发展现状&quot; class=&quot;headerlink&quot; title=&quot;发展现状&quot;&gt;&lt;/a&gt;发展现状&lt;/h2&gt;&lt;p&gt;基本解决：词性标注、命名实体识别、Spam识别&lt;br&gt;取得长足进展：情感分析、共指消解、词义消歧、句法分析、机器翻译、信息抽取&lt;br&gt;挑战：自动问答、复述、文摘、会话机器人&lt;br&gt;&lt;img src=&quot;/picture/machine-learning/nlp_process.png&quot; alt=&quot;nlp_process&quot;&gt;&lt;br&gt;
    
    </summary>
    
      <category term="机器学习" scheme="xtf615.com/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="机器学习" scheme="xtf615.com/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="自然语言处理" scheme="xtf615.com/tags/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/"/>
    
      <category term="分词" scheme="xtf615.com/tags/%E5%88%86%E8%AF%8D/"/>
    
      <category term="NLTK" scheme="xtf615.com/tags/NLTK/"/>
    
      <category term="情感分析" scheme="xtf615.com/tags/%E6%83%85%E6%84%9F%E5%88%86%E6%9E%90/"/>
    
  </entry>
  
  <entry>
    <title>线性回归</title>
    <link href="xtf615.com/2017/02/09/%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/"/>
    <id>xtf615.com/2017/02/09/线性回归/</id>
    <published>2017-02-09T14:27:47.000Z</published>
    <updated>2017-02-13T03:22:22.528Z</updated>
    
    <content type="html"><![CDATA[<h1 id="模型表示"><a href="#模型表示" class="headerlink" title="模型表示"></a>模型表示</h1><h2 id="房价预测例子"><a href="#房价预测例子" class="headerlink" title="房价预测例子"></a>房价预测例子</h2><p>  让我们通过一个例子来开始：这个例子是预测住房价格的，我们要使用一个数据集，数据集包含俄勒冈州波特兰市的住房价格。在这里，我要根据不同房屋尺寸所售出的价格，画出我的数据集。比方说，如果你朋友的房子是1250平方尺大小，你要告诉他们这房子能卖多少钱。那么，你可以做的一件事就是构建一个模型，也许是条直线，从这个数据模型上来看，也许你可以告诉你的朋友，他能以大约 220000(美元)左右的价格卖掉这个房子。这就是监督学习算法的一个例子<br><img src="/picture/machine-learning/house_price.jpg" alt="house_price"><br>   它被称作监督学习是因为对于每个数据来说，我们给出了“正确的答案”，即告诉我们：根据我们的数据来说，房子实际的价格是多少，而且，更具体来说，这是一个回归问题。回归一词指的是，我们根据之前的数据预测出一个准确的输出值，对于这个例子就是价格，同时，还有另一种最常见的监督学习方式，叫做分类问题，当我们想要预测离散的输出值，例如，我们正在寻找癌症肿瘤，并想要确定肿瘤是良性的还是恶性的，这就是 0/1离散输出的问题。更进一步来说，在监督学习中我们有一个数据集，这个数据集被称训练集。下图是房价预测数据格式：<br>  <img src="/picture/machine-learning/train_set_representation.jpg" alt="train_set_representation"><br><a id="more"></a>  </p>
<h2 id="标记"><a href="#标记" class="headerlink" title="标记"></a>标记</h2><p>我们将要用来描述这个回归问题的标记如下:<br>    m代表训练集中实例的数量<br>    x代表特征/输入变量<br>    y代表目标变量/输出变量<br>    (x,y)代表训练集中的实例<br>    \((x^{(i)},y^{(i)})\) 代表第i个观察实例<br>    h代表学习算法的解决方案或函数也称为假设(hypothesis)<br><img src="/picture/machine-learning/supervised_learning.jpg" alt="supervised_learning"><br>  这就是一个监督学习算法的工作方式，我们可以看到这里有我们的训练集里房屋价格我们把它喂给我们的学习算法，学习算法的工作了，然后输出一个函数，通常表示为小写h表示。h代表   hypothesis(假设)，h表示一个函数，输入是房屋尺寸大小，就像你朋友想出售的房屋，因此h根据输入的x值来得出y值，y值对应房子的价格因此，h是一个从x到y的函数映射。<br>   我将选择最初的使用规则h代表hypothesis，因而，要解决房价预测问题，我们实际上是要将训练集“喂”给我们的学习算法，进而学习得到一个假设 h，然后将我们要预测的房屋的尺寸作为输入变量输入给h，预测出该房屋的交易价格作为输出变量输出为结果。那么，对于我们的房价预测问题，我们该如何表达h？<br>   一种可能的表达方式为：\(h_θ(x)=θ_0+θ_1x\)，因为只含有一个特征/输入变量，因此这样的问题叫作单变量线性回归问题。</p>
<h1 id="代价函数"><a href="#代价函数" class="headerlink" title="代价函数"></a>代价函数</h1><p>我们将定义代价函数的概念，这有助于我们弄清楚如何把最有可能的直线与我们的数据相拟合。如图：<br><img src="/picture/machine-learning/cost_function.jpg" alt="cost_function"><br>在线性回归中我们有一个像这样的训练集，m代表了训练样本的数量，比如m=47.而我们的假设函数，也就是用来进行预测的函数，是这样的线性函数形式：\(h_θ(x)=θ_0+θ_1x\)。接下来我们会引入一些术语我们现在要做的便是为我们的模型选择合适的参数（parameters）θ0和θ1，在房价问题这个例子中便是直线的斜率和在  y轴上的截距。我们选择的参数决定了我们得到的直线相对于我们的训练集的准确程度，模型所预测的值与训练集中实际值之间的差距（下图中蓝线所指）就是建模误差（modeling error）。<br><img src="/picture/machine-learning/modeling_error.jpg" alt="modeling_error"><br>我们的目标便是选择出可以使得建模误差的平方和能够最小的模型参数。即使得代价函数最小：<br>$$J(\theta_{0},\theta_{1})=\frac{1}{2m}\sum_{i=1}^m\left(h_{\theta}(x^{(i)})-y^{(i)}\right)^2$$<br>我们绘制一个等高线图，三个坐标分别为 \(θ_0\) 和\(θ_1\) 和 \(J(θ_0,θ_1)\) ：<br><img src="/picture/machine-learning/contour.jpg" alt="contour"><br>可以看出在三维空间中存在一个使得 \(J(θ_0,θ_1)\) 最小的点.<br>代价函数也被称作平方误差函数，有时也被称为平方误差代价函数。我们之所以要求出<br>误差的平方和，是因为误差平方代价函数，对于大多数问题，特别是回归问题，都是一个合<br>理的选择。还有其他的代价函数也能很好地发挥作用，但是平方误差代价函数可能是解决回<br>归问题最常用的手段了。</p>
<h2 id="代价函数的理解（1）"><a href="#代价函数的理解（1）" class="headerlink" title="代价函数的理解（1）"></a>代价函数的理解（1）</h2><h3 id="Hypothesis"><a href="#Hypothesis" class="headerlink" title="Hypothesis:"></a>Hypothesis:</h3><p>$$h_\theta(x)=\theta_0+\theta_1x$$</p>
<h3 id="Parameters"><a href="#Parameters" class="headerlink" title="Parameters:"></a>Parameters:</h3><p>$$\theta_0,  \theta_1$$</p>
<h3 id="Cost-Function"><a href="#Cost-Function" class="headerlink" title="Cost Function:"></a>Cost Function:</h3><p>$$J(\theta_{0},\theta_{1})=\frac{1}{2m}\sum_{i=1}^m\left(h_{\theta}(x^{(i)})-y^{(i)}\right)^2$$</p>
<h3 id="Goal"><a href="#Goal" class="headerlink" title="Goal:"></a>Goal:</h3><p>$$\min_{\theta_0,\theta_1}J(\theta_0,\theta_1)$$<br><img src="/picture/machine-learning/mini_.jpg" alt="mini"><br>如上图所示，左图为 \(\theta_0=0,\theta_1=0\) 时的代价,右图为 \(\theta_0=0\),代价函数随 \(\theta_1\) 变化的情况。可以看出当 \(\theta_1=1\) 时，代价损失最低。</p>
<h2 id="代价函数的理解（2）"><a href="#代价函数的理解（2）" class="headerlink" title="代价函数的理解（2）"></a>代价函数的理解（2）</h2><p><img src="/picture/machine-learning/contour2.jpg" alt="contour2"><br>如图是代价函数的样子，等高线图，可以看出在三维空间中存在一个使得 \(J(θ_0,θ_1)\) 最小的点。<br><img src="/picture/machine-learning/contour3.jpg" alt="contour3"><br>上图右边图形为三维图的二维等高线图。<br>通过这些图形，能更好地理解这些代价函数J所表达的值是什么样的，它们对应的假设是什么样的，以及什么样的假设对应的点，更接近于代价函数J的最小值。<br>当然，我们真正需要的是一种有效的算法，能够自动地找出这些使代价函数J取最小值的参数\(θ_0\)和 \(θ_1\)来。<br>我们也不希望编个程序把这些点画出来，然后人工的方法来读出这些点的数值，这很明显不是一个好办法。我们会遇到更复杂、更高维度、更多参数的情况，而这些情况是很难画出图的，因此更无法将其可视化，因此我们真正需要的是编写程序来找出这些最小化代价函数的 \(θ_0\)和 \(θ_1\)的值.</p>
<h1 id="梯度下降Gradient-Descent"><a href="#梯度下降Gradient-Descent" class="headerlink" title="梯度下降Gradient Descent"></a>梯度下降Gradient Descent</h1><p>梯度下降是一个用来求函数最小值的算法，我们将使用梯度下降算法来求出代价函数\(J(θ_0,θ_1)\)的最小值。<br>梯度下降背后的思想是：开始时我们随机选择一个参数的组合 \((θ_0,θ_1,…,θ_n)\) ，计算代价<br>函数，然后我们寻找下一个能让代价函数值下降最多的参数组合。我们持续这么做直到到到一个局部最小值（local minimum），因为我们并没有尝试完所有的参数组合，所以不能确定我们得到的局部最小值是否便是全局最小值（global minimum），选择不同的初始参数组合，可能会找到不同的局部最小值。<br><img src="/picture/machine-learning/hill.jpg" alt="hill"><br>想象一下你正站立在山的这一点上，站立在你想象的公园这座红色山上，在梯度下降算法中，我们要做的就是旋转 360度，看看我们的周围，并问自己要在某个方向上，用小碎步尽快下山。这些小碎步需要朝什么方向？如果我们站在山坡上的这一点，你看一下周围，你会发现最佳的下山方向，你再看看周围，然后再一次想想，我应该从什么方向迈着小碎步下山？然后你按照自己的判断又迈出一步，重复上面的步骤，从这个新的点，你环顾四周，并决定从什么方向将会最快下山，然后又迈进了一小步，并依此类推，直到你接近局部最低点的位置。<br>批量梯度下降（batch gradient descent）算法的公式为：<br><strong>repeat until convergence</strong>{<br>   $$\theta_j:=\theta_j-\alpha\frac{\partial}{\partial\theta_j}J(\theta_0,\theta_1) \quad(for \ j = 0\ and\ j = 1)$$<br>}<br>其中 \(\alpha\)是学习率（learning rate），它决定了我们沿着能让代价函数下降程度最大的方向向下迈出的步子有多大，在批量梯度下降中，我们每一次都同时让所有的参数减去学习速<br>率乘以代价函数的导数。<br><img src="/picture/machine-learning/gradient_descent.jpg" alt="gradient"><br>在梯度下降算法中，还有一个更微妙的问题，梯度下降中，我们要更新\(θ_0\)和 \(θ_1\)，当j=0和j=1时，会产生更新，所以你将更新 \(J_{θ_0}\)和\(J_{θ_0}\)。实现梯度下降算法的微妙之处是，在这个表达式中，如果你要更新这个等式，你需要同时更新\(θ_0\)和 \(θ_1\).<br>让我进一步阐述这个过程：<br>$$temp_0:=\theta_0-\alpha\frac{\partial}{\partial\theta_0}J(\theta_0,\theta_1)\\\\temp_1:=\theta_1-\alpha\frac{\partial}{\partial\theta_1}J(\theta_0,\theta_1) \\\ \theta_0:=temp_0 \\\ \theta_1:=temp_1$$<br>在梯度下降算法中，这是正确实现同时更新的方法。我不打算解释为什么你需要同时更新，同时更新是梯度下降中的一种常用方法。我们之后会讲到，同步更新是更自然的实现方法。当人们谈到梯度下降时，他们的意思就是同步更新。</p>
<h2 id="梯度下降理解"><a href="#梯度下降理解" class="headerlink" title="梯度下降理解"></a>梯度下降理解</h2><p>$$\theta_j:=\theta_j-\alpha\frac{\partial}{\partial\theta_j}J(\theta_0,\theta_1)$$<br>描述：对θ赋值，使得J(θ)按梯度下降最快方向进行，一直迭代下去，最终得到局部最小值。其中 α是学习率（learning  rate），它决定了我们沿着能让代价函数下降程度最大的方向向下迈出的步子有多大。<br><img src="/picture/machine-learning/gradient_descent2.jpg" alt="gradient"><br>对于这个问题，求导的目的，基本上可以说取这个红点的切线，就是这样一条红色的直线，刚好与函数相切于这一点，让我们看看这条红色直线的斜率，就是这条刚好与函数曲线相切的这条直线，这条直线的斜率正好是这个三角形的高度除以这个水平长度，现在，这条线有一个正斜率，也就是说它有正导数，因此，我得到的新的\(θ_1\)，\(θ_1\)更新后等于\(θ_1\)减去一个正数乘以 α。这就是我梯度下降法的更新规则：\(\theta_j:=\theta_j-\alpha\frac{\partial}{\partial\theta_j}J(\theta_0,\theta_1)\)<br>让我们来看看如果 α太小或α太大会出现什么情况：</p>
<ul>
<li>如果α太小了，即我的学习速率太小，结果就是只能这样像小宝宝一样一点点地挪动，去努力接近最低点，这样就需要很多步才能到达最低点，所以如果 α太小的话，可能会很慢因为它会一点点挪动，它会需要很多步才能到达全局最低点。</li>
<li>如果 α太大，那么梯度下降法可能会越过最低点，甚至可能无法收敛，下一次迭代又移动了一大步，越过一次，又越过一次，一次次越过最低点，直到你发现实际上离最低点越来越远，所以，如果 α太大，它会导致无法收敛，甚至发散。</li>
</ul>
<p>现在，还有一个问题，<strong>如果我们预先把\(θ_1\)放在一个局部的最低点，你认为下一步梯度下降法会怎样工作？</strong><br>假设你将\(θ_1\)初始化在局部最低点，在这儿，它已经在一个局部的最优处或局部最低点。<br>结果是局部最优点的导数将等于零，因为它是那条切线的斜率。这意味着你已经在局部最优点，它使得\(θ_1\)不再改变，也就是新的\(θ_1\)等于原来的\(θ_1\)，因此，如果你的参数已经处于局部最低点，那么梯度下降法更新其实什么都没做，它不会改变参数的值。这也解释了为什么即使学习速率α保持不变时，梯度下降也可以收敛到局部最低点。<br>我们来看一个例子，这是代价函数 J(θ)。<br><img src="/picture/machine-learning/gradient_descent3.jpg" alt="gradient"><br>我想找到它的最小值，首先初始化我的梯度下降算法，在那个品红色的点初始化，如果我更新一步梯度下降，也许它会带我到这个点，因为这个点的导数是相当陡的。现在，在这个绿色的点，如果我再更新一步，你会发现我的导数，也即斜率，是没那么陡的。随着我接近最低点，我的导数越来越接近零，所以，梯度下降一步后，新的导数会变小一点点。然后我想再梯度下降一步，在这个绿点，我自然会用一个稍微跟刚才在那个品红点时比，再小一点的一步，到了新的红色点，更接近全局最低点了，因此这点的导数会比在绿点时更小。所以，我再进行一步梯度下降时，我的导数项是更小的，\(θ_1\)更新的幅度就会更小。所以随着梯度下降法的运行，你移动的幅度会自动变得越来越小，直到最终移动幅度非常小，你会发现，已经收敛到局部极小值。<br>回顾一下，在梯度下降法中，当我们接近局部最低点时，梯度下降法会自动采取更小的幅度，这是因为当我们接近局部最低点时，很显然在局部最低时导数等于零，所以当我们接近局部最低时，导数值会自动变得越来越小，所以梯度下降将自动采取较小的幅度，这就是梯度下降的做法。所以实际上没有必要再另外减小 α。这就是梯度下降算法，你可以用它来最小化任何代价函数 J，不只是线性回归中的代价函数J。</p>
<h2 id="梯度下降的线性回归"><a href="#梯度下降的线性回归" class="headerlink" title="梯度下降的线性回归"></a>梯度下降的线性回归</h2><p>梯度下降是很常用的算法，它不仅被用在线性回归上和线性回归模型、平方误差代价函数。接下来我们要将梯度下降和代价函数结合。我们将用到此算法，并将其应用于具体的拟合直线的线性回归算法里。<br>梯度下降算法和线性回归算法比较如图：<br><img src="/picture/machine-learning/gradient_descent_line.jpg" alt="gradient"><br>对我们之前的线性回归问题运用梯度下降法，关键在于求出代价函数的导数，即：<br>$$\frac{\partial}{\partial\theta_j}J(\theta_0,\theta_1)=\frac{\partial}{\partial\theta_j}\left(\frac{1}{2m}\sum_{i=1}^m(h_{\theta}(x^{(i)})-y^{(i)})^2\right)$$ </p>
<ul>
<li>j = 0时,<br>$$\frac{\partial}{\partial\theta_0}J(\theta_0,\theta_1)=\frac{1}{m}\sum_{i=1}^m(h_{\theta}(x^{(i)})-y^{(i)})$$</li>
<li>j = 1时，<br>$$\frac{\partial}{\partial\theta_1}J(\theta_0,\theta_1)=\frac{1}{m}\sum_{i=1}^m((h_{\theta}(x^{(i)})-y^{(i)})*x^{(i)})$$<br>则算法改写成：<br><strong>repeat until convergence</strong>{<br>$$\theta_0:=\theta_0-\alpha\frac{1}{m}\sum_{i=1}^m(h_{\theta}(x^{(i)})-y^{(i)}) \\\ \theta_1:=\theta_1-\alpha\frac{1}{m}\sum_{i=1}^m((h_{\theta}(x^{(i)})-y^{(i)})*x^{(i)})$$<br>}<br>我们刚刚使用的算法，有时也称为批量梯度下降。实际上，在机器学习中，通常不太会给算法起名字，但这个名字”批量梯度下降”，指的是在梯度下降的每一步中，我们都用到了所有的训练样本，在梯度下降中，在计算微分求导项时，我们需要进行求和运算，所以，在每一个单独的梯度下降中，我们最终都要计算这样一个东西，这个项需要对所有m个训练样本求和。因此，批量梯度下降法这个名字说明了我们需要考虑所有这一”批”训练样本，而事实上，有时也有其他类型的梯度下降法，不是这种”批量”型的，不考虑整个的训练集，而是每次只关注训练集中的一些小的子集。<br>有一种计算代价函数J最小值的数值解法，不需要梯度下降这种迭代算法。它可以在不需要多步梯度下降的情况下，也能解出代价函数J的最小值，这是另一种称为正规方程(normal equations)的方法。实际上在数据量较大的情况下，梯度下降法比正规方程要更适用一些。</li>
</ul>
<h1 id="多变量线性回归"><a href="#多变量线性回归" class="headerlink" title="多变量线性回归"></a>多变量线性回归</h1><h2 id="多维特征"><a href="#多维特征" class="headerlink" title="多维特征"></a>多维特征</h2><p>目前为止，我们探讨了单变量/特征的回归模型，现在我们对房价模型增加更多的特征，<br>例如房间数楼层等，构成一个含有多个变量的模型，模型中的特征为\((x_1,x_2,…,x_n)\)<br><img src="/picture/machine-learning/multple_feature.jpg" alt="multiple feature"><br>增添更多特征后，我们引入一系列新的注释：<br>n代表特征的数量<br>\(x^{(i)}\) 代表第i个训练实例，是特征矩阵中的第i行，是一个向量（vector）。例如上图中：$$x^{(2)}=\begin{bmatrix} 1416 \\\ 3 \\\ 2 \\\ 40\end{bmatrix}$$</p>
<p>\(x_j^{(i)}\) 代表特征矩阵中第i行的第j个特征，也就是第i个训练实例的第j个特征。例如上图中， \(x_3^{(2)}=2\)<br>支持多变量的假设h表示为：<br>$$h_\theta(x)=\theta_0+\theta_1x_1+\theta_2x_2+…+\theta_nx_n$$<br>这个公式中有 n+1个参数和n个变量，为了使得公式能够简化一些，引入 \(x_0=1\)， 此时模型中的参数是一个n+1维的向量，任何一个训练实例也都是n+1维的向量，特征矩阵\(X\)的维度是m*(n+1)。因此公式可以简化为：<br>$$h_{\theta}(x)=\theta^TX$$ </p>
<h2 id="多变量梯度下降"><a href="#多变量梯度下降" class="headerlink" title="多变量梯度下降"></a>多变量梯度下降</h2><p>与单变量线性回归类似，在多变量线性回归中，我们也构建一个代价函数，则这个代价<br>函数是所有建模误差的平方和，即：<br>$$J(\theta_{0},\theta_{1}…,\theta_{n})=\frac{1}{2m}\sum_{i=1}^m\left(h_{\theta}(x^{(i)})-y^{(i)}\right)^2$$<br>其中 \(h_\theta(x)=\theta_0x_0+\theta_1x_1+\theta_2x_2+…+\theta_nx_n\)<br>我们的目标和单变量线性回归问题中一样，是要找出使得代价函数最小的一系列参数。<br>多变量线性回归的批量梯度下降算法为：<br><strong>repeat until convergence</strong>{<br>   $$\theta_j:=\theta_j-\alpha\frac{\partial}{\partial\theta_j}J(\theta_0,\theta_1,…,\theta_n)$$<br>}<br>即：<br><strong>repeat until convergence</strong>{<br>   $$\theta_j:=\theta_j-\alpha\frac{\partial}{\partial\theta_j}\left(\frac{1}{2m}\sum_{i=1}^m(h_{\theta}(x^{(i)})-y^{(i)})^2\right)$$<br>}<br>求导后得到：</p>
<p><strong>repeat until convergence</strong>{<br>   $$\theta_j:=\theta_j-\alpha\frac{1}{m}\sum_{i=1}^m((h_{\theta}(x^{(i)})-y^{(i)})*x_j^{(i)}) \\\ (simultaneously \ update \ \theta_j \ for \ j=0,1,2,…,n)$$<br>}<br>我们开始随机选择一系列的参数值，计算所有的预测结果后，再给所有的参数一个新的值，如此循环直到收敛。</p>
<h2 id="梯度下降实践——特征缩放"><a href="#梯度下降实践——特征缩放" class="headerlink" title="梯度下降实践——特征缩放"></a>梯度下降实践——特征缩放</h2><p>在我们面对多维特征问题的时候，我们要保证这些特征都具有相近的尺度，这将帮助梯度下降算法更快地收敛。<br>以房价问题为例，假设我们使用两个特征，房屋的尺寸和房间的数量，尺寸的值为   0-2000平方英尺，而房间数量的值则是0-5，以两个参数分别为横纵坐标，绘制代价函数的等高线图能，看出图像会显得很扁，梯度下降算法需要非常多次的迭代才能收敛。<br><img src="/picture/machine-learning/feature_scale.jpg" alt="feature_scale"><br>解决的方法是尝试将所有特征的尺度都尽量缩放到-1到1之间。如图：<br><img src="/picture/machine-learning/feature_scale2.jpg" alt="feature_scale"><br>最简单的方法是令：<br>$$x_n = \frac{x_n-\mu_n}{S_n} \\\ 其中, \mu_n是平均值，S_n是标准差$$</p>
<h2 id="梯度下降实践——学习率"><a href="#梯度下降实践——学习率" class="headerlink" title="梯度下降实践——学习率"></a>梯度下降实践——学习率</h2><p>梯度下降算法收敛所需要的迭代次数根据模型的不同而不同，我们不能提前预知，我们可以绘制迭代次数和代价函数的图表来观测算法在何时趋于收敛。<br><img src="/picture/machine-learning/learn_rate.jpg" alt="learn_rate"><br>梯度下降算法的每次迭代受到学习率的影响，如果学习率 α过小，则达到收敛所需的迭代次数会非常高；如果学习率 α过大，每次迭代可能不会减小代价函数，可能会越过局部最小值导致无法收敛。通常可以考虑尝试些学习率：\(α=0.01，0.03，0.1，0.3，1，3，10\)</p>
<h2 id="特征与多项式回归"><a href="#特征与多项式回归" class="headerlink" title="特征与多项式回归"></a>特征与多项式回归</h2><p>如房价预测问题，<br>$$h_\theta(x)=\theta_0+\theta_1*frontage+\theta_2*depth\\\ x_1=frontage(临街宽度),x_2=depth(纵向深度) \\\ x=frontage*depth=area(面积) ， 则：h_\theta(x)=\theta_0+\theta_1x$$<br>线性回归并不适用于所有数据，有时我们需要曲线来适应我们的数据，比如一个二次方模型：<br>$$h_\theta(x)=\theta_0+\theta_1x+\theta_2x_2^2$$<br>或者三次方模型：<br>$$h_\theta(x)=\theta_0+\theta_1x+\theta_2x_2^2+\theta_3x_3^3$$<br><img src="/picture/machine-learning/polynomial_regression.jpg" alt="polynomial_regression"><br>通常我们需要先观察数据然后再决定准备尝试怎样的模型。另外，我们可以令：<br>$$x_2=x_2^2 \\\ x_3=x_3^2$$<br>从而将模型转化为线性回归模型。<br>根据函数图形特性，我们还可以使：<br>$$h_\theta(x)=\theta_0+\theta_1(size)+\theta_2(size)^2<br>\\\ 或者 \\\\<br>h_\theta(x)=\theta_0+\theta_1(size)+\theta_2\sqrt{(size)}$$<br>注：如果我们采用多项式回归模型，在运行梯度下降算法前，特征缩放非常有必要。</p>
<h2 id="正规方程"><a href="#正规方程" class="headerlink" title="正规方程"></a>正规方程</h2><p>到目前为止，我们都在使用梯度下降算法，但是对于某些线性回归问题，正规方程方法正规方程是通过求解下面的方程来找出使得代价函数最小的参数的是更好的解决方案。<br>正规方程是通过求解下面的方程来找出使得代价函数最小的参数的：<br>$$\frac{\partial}{\partial\theta_j}J(\theta_j)=0$$<br>假设我们的训练集特征矩阵为:\(X\)（包含 \(x_0=1\) ）,并且我们的训练集结果为向量   y，则利用正规方程解出向量:<br>$$\theta=\left(X^TX\right)^{-1}X^Ty \\\\<br>设矩阵A=X^TX，则：(X^TX)^{-1}=A^{-1} \\\\<br>上标T代表矩阵转置，-1代表矩阵的逆。$$</p>
<h3 id="示例："><a href="#示例：" class="headerlink" title="示例："></a>示例：</h3><p><img src="/picture/machine-learning/normal_example.jpg" alt="normal_example"></p>
<p><table><br>    <tr><br>        <td>X(0)</td> <td>X(1)</td> <td>X(2)</td> <td>X(3)</td> <td>X(4)</td><td>y</td><br>    </tr><br>    <tr><br>        <td>1</td><td>2104</td> <td>5</td> <td>1</td> <td>45</td> <td>460</td><br>    </tr><br>    <tr><br>        <td>1</td><td>1416</td> <td>3</td> <td>2</td> <td>40</td> <td>232</td><br>    </tr><br>    <tr><br>        <td>1</td><td>1534</td> <td>3</td> <td>2</td> <td>30</td> <td>315</td><br>    </tr><br>    <tr><br>        <td>1</td><td>852</td> <td>2</td> <td>1</td> <td>36</td> <td>178</td><br>    </tr><br></table><br>再运用正规方程求解参数：<br><img src="/picture/machine-learning/normal_example2.jpg" alt="normal_example"><br>注：<strong>对于那些不可逆的矩阵</strong>（通常是因为特征之间不独立，如同时包含英尺为单位的尺寸和米为单位的尺寸两个特征，也有可能是特征数量大于训练集的数量），正规方程方法是不能用的。<br> |编号|说明|国外|   </p>
<h3 id="梯度下降和正规方程比较："><a href="#梯度下降和正规方程比较：" class="headerlink" title="梯度下降和正规方程比较："></a>梯度下降和正规方程比较：</h3><p><table><br>    <tr><br>        <td><strong>梯度下降</strong></td> <td><strong>正规方程</strong></td><br>    </tr><br>    <tr><br>        <td>需要选择学习率</td><td>不需要</td><br>    </tr><br>    <tr><br>        <td>需要多次迭代</td><td>一次运算得出</td><br>    </tr><br>    <tr><br>        <td>当特征数量 n大时也能较好适用</td> <td>需要计算 \((X^TX)^{-1}\) 如果特征数量n较大则运算代价大，因为矩阵逆的计算时间复杂度为 \(O(n^3)\)，通常来说当n小于10000时还是可以接受的</td><br>    </tr><br>    <tr><br>        <td>适用于各种类型的模型</td><td>只适用于线性模型，不适合逻辑回归模型等其他模型</td><br>    </tr><br></table><br>总结一下，只要特征变量的数目并不大，标准方程是一个很好的计算参数  θ的替代方法。具体地说，只要特征变量数量小于一万，我通常使用标准方程法，而不使用梯度下降法。随着学习算法越来越复杂，例如，当我们讲到分类算法，像逻辑回归算法，我们会看到，实际上对于那些算法，并不能使用标准方程法。对于那些更复杂的学习算法，我们将不得不仍然使用梯度下降法。因此，梯度下降法是一个非常有用的算法，可以用在有大量特征变量的线性回归问题。</p>
<h3 id="正规方程之不可逆性"><a href="#正规方程之不可逆性" class="headerlink" title="正规方程之不可逆性"></a>正规方程之不可逆性</h3><p>我们要讲的问题如下：<br>$$\theta=\left(X^TX\right)^{-1}X^Ty$$<br>对于矩阵，\(X^TX\) 不可逆的情况怎么解决？<br>如果你懂一点线性代数的知识，你或许会知道，有些矩阵可逆，而有些矩阵不可逆。我们称那些不可逆矩阵为奇异或退化矩阵。问题的重点在于 X’X的不可逆的问题很少发生。</p>
<ul>
<li>特征值存在线性关联。<br>例如，在预测住房价格时，如果x1是以英尺为尺寸规格计算的房子，x2是以平方米为尺寸规格计算的房子，同时，你也知道 1米等于3.28英尺(四舍五入到两位小数)，这样，你的这两个特征值将始终满足约束：\(x_1=x_2*(3.28)\)。</li>
<li>大量的特征<br>第二个原因是，在你想用大量的特征值，尝试实践你的学习算法的时候，可能会导致矩阵 \(X^TX\)的结果是不可逆的。<br>具体地说，在 m小于或等于n的时候，例如，有m等于10个的训练样本,n等于100的特征数量。要找到适合的(n+1)维参数矢量\(θ\)，这将会变成一个101维的矢量，尝试从10个训练样本中找到满足101个参数的值，这工作可能会让你花上一阵.<br>稍后我们将看到，如何使用小数据样本以得到这 100或  101个参数，通常，我们会使用一种叫做正则化的线性代数方法，通过删除某些特征或者是使用某些技术，来解决当m比n小的时候的问题。即使你有一个相对较小的训练集，也可使用很多的特征来找到很多合适的参数。</li>
</ul>
<p><strong>总之当你发现的矩阵 X’X的结果是奇异矩阵，或者找到的其它矩阵是不可逆的，要怎么做？</strong><br>首先，看特征值里是否有一些多余的特征，像这些 x1和x2是线性相关的，互为线性函数。同时，当有一些多余的特征时，可以删除这两个重复特征里的其中一个，无须两个特征同时保留，将解决不可逆性的问题。如果特征数量实在太多，我会删除些用较少的特征来反映尽可能多内容，否则我会考虑使用正规化方法。</p>
<h1 id="拓展-广义线性模型与线性回归"><a href="#拓展-广义线性模型与线性回归" class="headerlink" title="拓展 广义线性模型与线性回归"></a>拓展 广义线性模型与线性回归</h1><p>线性回归是广义线性模型的一种特殊形式。</p>
<h2 id="广义线性模型GLM"><a href="#广义线性模型GLM" class="headerlink" title="广义线性模型GLM"></a>广义线性模型GLM</h2><p>三个假设：</p>
<ul>
<li>\(y|x;\sim ExponentialFamily(\eta) \)，即y的条件概率属于指数分布簇（The exponential Family）</li>
<li>给定x广义线性模型的目标是求解\(T(y)|x\),不过由于很多情况下\(T(y)=y\),所以我们的目标就变成\(y|x\),也即我们希望拟合函数为\(h(x)=E[y|x]\) (备注：这个条件在线性回归中满足)</li>
<li>自然参数\(\eta\)与\(x\)是线性关系：\(\eta=\theta^Tx\) (\(\eta为向量时,\eta_i=\theta_i^Tx\))</li>
</ul>
<h2 id="指数分布簇-The-exponential-Family）"><a href="#指数分布簇-The-exponential-Family）" class="headerlink" title="指数分布簇(The exponential Family）"></a>指数分布簇(The exponential Family）</h2><p>首先定义一下指数分布，它有如下形式:<br>$$p(y;\eta)=b(y)exp(\eta^{T}T(y)-a(\eta)) \\\\<br>其中，\eta是自然参数(natural \ parameter), \\\ <br>T(y)是充分统计量(sufficient \ statistic,一般T(y)=y),\\\\<br>a(\eta)是log \ partition \ function(e^{-a(\eta)}充当正规化常量的角色，保证\sum p(y;\eta)=1)<br>$$<br>也就是说，T,a,b确定了一种分布，\(\eta\)是该分布的参数。选择合适的T,a,b我们可以得到高斯分布Gaussian和伯努利分布Bernoulli等。</p>
<p>即，有了广义线性模型，我们只需要把符合指数分布的一般模型的参数转换为它对应的广义线性模型参数，然后按照广义线性模型的求解步骤，即可轻松求解问题。</p>
<h2 id="Gaussian高斯分布的指数分布簇形式"><a href="#Gaussian高斯分布的指数分布簇形式" class="headerlink" title="Gaussian高斯分布的指数分布簇形式"></a>Gaussian高斯分布的指数分布簇形式</h2><p>在线性回归中，\(\sigma对于模型参数\theta的选择没有影响，为了推导方便我们将其设为1：\)<br>$$p(y;\mu)=\frac{1}{\sqrt{2\pi}}exp\left(-\frac{1}{2}(y-\mu)^2\right) \\\\<br>    = \frac{1}{\sqrt{2\pi}}exp\left(-\frac{1}{2}y^2\right)*exp\left(\mu y-\frac{1}{2}\mu^2\right)<br>$$<br>得到对应参数：<br>$$T(y)=y \\\\<br>\eta=\mu \\\\<br>a(\eta)=\mu^2/2=\eta^2/2 \\\\<br>b(y)=(1/\sqrt{2\pi})exp(-y^2/2)<br>$$</p>
<h2 id="广义线性模型推导线性回归"><a href="#广义线性模型推导线性回归" class="headerlink" title="广义线性模型推导线性回归"></a>广义线性模型推导线性回归</h2><ul>
<li>step1: \(y|x; \sim N(\mu,\theta)\)</li>
<li>step2: 由假设2: \(h(x)=E[y|x]\) 得到：<br>$$h_\theta(x)=E[y|x;\Theta]\\\\<br>=\mu \\\\<br>=\eta  \\\\<br>=\Theta^Tx  \\\\<br>其中, E[y|x;\Theta]=\mu由假设1得到；\\\\<br>\mu=\eta由高斯分布对应的广义线性模型参数得到；\\\\<br>\eta = \Theta^Tx由假设3得到。<br>$$<h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2>可以看出，广义线性模型要求被解释变量属于指数分布簇。为什么呢？<br>逆推：被解释变量属于指数分布簇-&gt;被解释变可以写成指数分布的形式-&gt;其指数分布形式的参数\(\eta\)与原分布参数会发生联系-&gt;联系的方式是\(\eta=f(原分布中的参数,比如\mu等，则f(\mu)即连接函数)\)</li>
</ul>
<h1 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h1><p><a href="http://open.163.com/special/opencourse/machinelearning.html" target="_blank" rel="external">斯坦福大学机器学习视频教程</a><br><a href="https://www.zhihu.com/question/47637500?sort=created" target="_blank" rel="external">知乎：为什么广义线性模型GLM要求被解释变量属于指数分布簇</a><br><a href="https://zhuanlan.zhihu.com/p/22876460" target="_blank" rel="external">知乎：广义线性模型</a>  </p>
]]></content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;模型表示&quot;&gt;&lt;a href=&quot;#模型表示&quot; class=&quot;headerlink&quot; title=&quot;模型表示&quot;&gt;&lt;/a&gt;模型表示&lt;/h1&gt;&lt;h2 id=&quot;房价预测例子&quot;&gt;&lt;a href=&quot;#房价预测例子&quot; class=&quot;headerlink&quot; title=&quot;房价预测例子&quot;&gt;&lt;/a&gt;房价预测例子&lt;/h2&gt;&lt;p&gt;  让我们通过一个例子来开始：这个例子是预测住房价格的，我们要使用一个数据集，数据集包含俄勒冈州波特兰市的住房价格。在这里，我要根据不同房屋尺寸所售出的价格，画出我的数据集。比方说，如果你朋友的房子是1250平方尺大小，你要告诉他们这房子能卖多少钱。那么，你可以做的一件事就是构建一个模型，也许是条直线，从这个数据模型上来看，也许你可以告诉你的朋友，他能以大约 220000(美元)左右的价格卖掉这个房子。这就是监督学习算法的一个例子&lt;br&gt;&lt;img src=&quot;/picture/machine-learning/house_price.jpg&quot; alt=&quot;house_price&quot;&gt;&lt;br&gt;   它被称作监督学习是因为对于每个数据来说，我们给出了“正确的答案”，即告诉我们：根据我们的数据来说，房子实际的价格是多少，而且，更具体来说，这是一个回归问题。回归一词指的是，我们根据之前的数据预测出一个准确的输出值，对于这个例子就是价格，同时，还有另一种最常见的监督学习方式，叫做分类问题，当我们想要预测离散的输出值，例如，我们正在寻找癌症肿瘤，并想要确定肿瘤是良性的还是恶性的，这就是 0/1离散输出的问题。更进一步来说，在监督学习中我们有一个数据集，这个数据集被称训练集。下图是房价预测数据格式：&lt;br&gt;  &lt;img src=&quot;/picture/machine-learning/train_set_representation.jpg&quot; alt=&quot;train_set_representation&quot;&gt;&lt;br&gt;
    
    </summary>
    
      <category term="机器学习" scheme="xtf615.com/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="机器学习" scheme="xtf615.com/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="线性回归" scheme="xtf615.com/tags/%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/"/>
    
      <category term="广义线性模型" scheme="xtf615.com/tags/%E5%B9%BF%E4%B9%89%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8B/"/>
    
      <category term="梯度下降" scheme="xtf615.com/tags/%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D/"/>
    
  </entry>
  
  <entry>
    <title>机器学习概念</title>
    <link href="xtf615.com/2017/01/18/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E6%A6%82%E5%BF%B5/"/>
    <id>xtf615.com/2017/01/18/机器学习概念/</id>
    <published>2017-01-18T01:25:34.000Z</published>
    <updated>2017-01-18T03:56:17.623Z</updated>
    
    <content type="html"><![CDATA[<h1 id="介绍"><a href="#介绍" class="headerlink" title="介绍"></a>介绍</h1><p>机器学习是目前信息技术中最激动人心的方向之一。你或许每天都在不知不觉中使用了机器学习的算法。</p>
<ul>
<li>你打开谷歌、必应搜索到你需要的内容，正是因为他们有良好的学习算法，谷歌和微软实现了学习算法来排行网页。</li>
<li>你用Facebook或苹果的图片分类程序他能认出你朋友的照片，这也是机器学习。</li>
<li>每次您阅读您的电子邮件垃圾邮件筛选器，可以帮你过滤大量的垃圾邮件这也是一种学习算法。</li>
</ul>
<p><strong>那么，为什么机器学习如此受欢迎呢？</strong><br>机器学习不只是用于人工智能领域。我们创造智能的机器，有很多基础的知识。比如，我们可以让机器找到A与B之间的最短路径，但我们仍然不知道怎么让机器做更有趣的事情，如web搜索、照片标记、反垃圾邮件。我们发现，<strong>唯一方法是让机器自己学习怎么来解决问题</strong>。所以，机器学习已经成为计算机的一个能力，现在它涉及到各个行业和基础科学中。</p>
<p><strong>这里有一些机器学习的案例。</strong></p>
<ul>
<li><strong>数据挖掘</strong>。机器学习被用于数据挖掘的原因之一是网络和自动化技术的增长，这意味着，我们有史上最大的数据集比如说，大量的硅谷公司正在收集  web上的单击数据，也称为点击流数据，并尝试使用机器学习算法来分析数据，更好的了解用户，并为用户提供更好的服务。这在硅谷有巨大的市场。</li>
<li><strong>医疗记录</strong>。随着自动化的出现，我们现在有了电子医疗记录。如果我们可以把医疗记录变成医学知识，我们就可以更好地理解疾病。</li>
<li><strong>计算生物学</strong>。还是因为自动化技术，生物学家们收集的大量基因数据序列、DNA序列和等等，机器运行算法让我们更好地了解人类基因组，大家都知道这对人类意味着什么。</li>
<li><strong>工程方面</strong>。在工程的所有领域，我们有越来越大、越来越大的数据集，我们试图使用学习算法，来理解这些数据。另外，在机械应用中，有些人不能直接操作。例如，我已经在无人直升机领域工作了许多年。我们不知道如何写一段程序让直升机自己飞。我们唯一能做的就是让计算机自己学习如何驾驶直升机。</li>
<li><p><strong>手写识别</strong>。现在我们能够非常便宜地把信寄到这个美国甚至全世界的原因之一就是当你<br>写一个像这样的信封，一种学习算法已经学会如何读你信封，它可以自动选择路径，所以我们只需要花几个美分把这封信寄到数千英里外。</p>
</li>
<li><p><strong>自然语言处理或计算机视觉</strong>。这些语言理解或图像理解都是属于AI领域。大部分的自然语言处理和大部分的计算机视觉，都应用了机器学习。学习算法还广泛用于自定制程序。每次你去亚马逊或 Netflix或  iTunes Genius，它都会给出其他电影或产品或音乐的建议，这是一种学习算法。仔细想一想，他们有百万的用户；但他们没有办法为百万用户，编写百万个不同程序。软件能给这些自定制的建议的唯一方法是通过学习你的行为，来为你定制服务。</p>
<a id="more"></a>
</li>
</ul>
<h1 id="机器学习概念"><a href="#机器学习概念" class="headerlink" title="机器学习概念"></a>机器学习概念</h1><ul>
<li><strong>Arthur Samuel</strong>： 他定义机器学习为，在进行特定编程的情况下，给予计算机学习能力的领域。<br>   <code>Samuel的定义可以回溯到50年代，他编写了一个西洋棋程序。这程序神奇之处在于，编程者自己并不是个下棋高手。但因为他太菜了，于是就通过编程，让西洋棋程序自己跟自己下了上万盘棋。通过观察哪种布局（棋盘位置）会赢，哪种布局会输，久而久之，这西洋棋程序明白了什么是好的布局，什么样是坏的布局。然后就牛逼大发了，程序通过学习后，玩西洋棋的水平超过了Samuel。这绝对是令人注目的成果。</code></li>
<li><strong>Tom Mitchell</strong>: 一个程序被认为能从经验E中学习，解决任务T，达到性能度量值P，当且仅当，有了经验E后，经过P评判，程序在处理T时的性能有所提升。<br>   <code>e就是程序上万次的自我练习的经验, 而任务t就是下棋。性能度量值p呢，就是它在与一些新的对手比赛时，赢得比赛的概率。</code></li>
</ul>
<h1 id="监督学习概念"><a href="#监督学习概念" class="headerlink" title="监督学习概念"></a>监督学习概念</h1><ul>
<li><strong>回归问题</strong>（房价预测）：<br><img src="/picture/machine-learning/house_price_prediction.jpg" alt="house_price_prediction"><br>我们应用学习算法，可以在这组数据中画一条直线，或者换句话说，拟合一条直线，根据这条线我们可以推测出，这套房子可能卖$150, 000，当然这不是唯一的算法。可能还有更好的，比如我们不用直线拟合这些数据，用二次方程去拟合可能效果会更好。根据二次方程的曲线，我们可以从这个点推测出，这套房子能卖接近$200, 000。<br>可以看出，<strong>监督学习指的就是我们给学习算法一个数据集，这个数据集由“正确答案”组成。</strong>在房价的例子中，我们给了一系列房子的数据，我们给定数据集中每个样本的正确价格，即它们实际的售价然后运用学习算法，算出更多的正确答案。比如你朋友那个新房子的价格。用术语来讲，这叫做回归问题。我们试着推测出一个连续值的结果，即房子的价格。一般房子的价格会记到美分，所以房价实际上是一系列离散的值，但是我们通常又把房价看成实数，看成是标量，所以又把它看成一个连续的数值。</li>
<li><strong>分类问题</strong>（乳腺癌良性与否）：<br><img src="/picture/machine-learning/breast_cancer.jpg" alt="breast_cancer"><br> 假设说你想通过查看病历来推测乳腺癌良性与否，假如有人检测出乳腺肿瘤，恶性肿瘤有害并且十分危险，而良性的肿瘤危害就没那么大，所以人们显然会很在意这个问题。让我们来看一组数据：这个数据集中，横轴表示肿瘤的大小，纵轴上，我标出 1和  0表示是或者不是恶性肿瘤。我们之前见过的肿瘤，如果是恶性则记为 1，不是恶性，或者说良性记为 0。我有 5个良性肿瘤样本，在1的位置有5个恶性肿瘤样本。现在我们有一个朋友很不幸检查出乳腺肿瘤。假设说她的肿瘤大概这么大，那么机器学习的问题就在于，你能否估算出肿瘤是恶性的或是良性的概率。用术语来讲，这是一个分类问题。分类指的是，我们试着推测出离散的输出值：0或1良性或恶性，而事实上在分类问题中，输出可能不止两个值。比如说可能有三种乳腺癌，所以你希望预测离散输出  0、1、2、3。0代表良性，1表示第一类乳腺癌，2表示第二类癌症，3表示第三类，但这也是分类问题。因为这几个离散的输出分别对应良性，第一类第二类或者第三类癌症，在分类问题中我们可以用另一种方式绘制这些数据点。现在我用不同的符号来表示这些数据。既然我们把肿瘤的尺寸看做区分恶性或良性的特征，那么我可以这么画，我用不同的符号来表示良性和恶性肿瘤。或者说是负样本和正样本现在我们不全部画 X，良性的肿瘤改成用 O表示，恶性的继续用  X表示。来预测肿瘤的恶性与否。在其它一些机器学习问题中，可能会遇到不止一种特征。举个例子，我们不仅知道肿瘤的尺寸，还知道对应患者的年龄。在其他机器学习问题中，我们通常有更多的特征，比如肿块密度，肿瘤细胞尺寸的一致性和形状的一致性等等，还有一些其他的特征。这就是我们即将学到最有趣的学习算法之一。</li>
<li><strong>总结</strong><br>现在来回顾一下，这节课我们介绍了监督学习。其基本思想是，<strong>我们数据集中的每个样本都有相应的“正确答案”。</strong>再根据这些样本作出预测，就像房子和肿瘤的例子中做的那样。我们还介绍了回归问题，即通过回归来推出一个连续的输出，之后我们介绍了分类问题，其目标是推出一组离散的结果。</li>
</ul>
<h1 id="无监督学习概念"><a href="#无监督学习概念" class="headerlink" title="无监督学习概念"></a>无监督学习概念</h1><p>   &emsp;在无监督学习中，我们已知的数据。看上去有点不一样，不同于监督学习的数据的样子，即<strong>无监督学习中没有任何的标签或者是有相同的标签或者就是没标签</strong>。所以我们已知数据集，却不知如何处理，也未告知每个数据点是什么。别的都不知道，就是一个数据集。你能从数据中找到某种结构吗？针对数据集，无监督学习就能判断出数据有两个不同的聚集簇。这是一个，那是另一个，二者不同。是的，无监督学习算法可能会把这些数据分成两个不同的簇。所以叫做聚类算法。事实证明，它能被用在很多地方。</p>
<ul>
<li><strong>谷歌新闻</strong>：<br>聚类应用的一个例子就是在谷歌新闻中。如果你以前从来没见过它，你可以到这个  URL网址 news.google.com去看看。谷歌新闻每天都在，收集非常多，非常多的网络的新闻内容。它再将这些新闻分组，组成有关联的新闻。所以谷歌新闻做的就是搜索非常多的新闻事件，自动地把它们聚类到一起。所以，这些新闻事件全是同一主题的，所以显示到一起。事实证明，聚类算法和无监督学习算法同样还用在很多其它的问题上。</li>
<li><strong>基因学</strong>：<br><img src="/picture/machine-learning/DNA.jpg" alt="DNA"><br>一个 DNA微观数据的例子。基本思想是输入一组不同个体，对其中的每个个体，你要分析出它们是否有一个特定的基因。技术上，你要分析多少特定基因已经表达。所以这些颜色，红，绿，灰等等颜色，这些颜色展示了相应的程度，即不同的个体是否有着一个特定的基因。你能做的就是运行一个聚类算法，把个体聚类到不同的类或不同类型的组（人）……</li>
<li><strong>组织大型计算机集群</strong>:<br>在大数据中心工作，那里有大型的计算机集群，他们想解决什么样的机器易于协同地工作，如果你能够让那些机器协同工作，你就能让你的数据中心工作得更高效。</li>
<li><strong>社交网络</strong>:<br>所以已知你朋友的信息，比如你经常发email的，或是你Facebook的朋友、谷歌+圈子的朋友，我们能否自动地给出朋友的分组呢？即每组里的人们彼此都熟识，认识组里的所有人？</li>
<li><strong>市场分割</strong>：<br>许多公司有大型的数据库，存储消费者信息。所以，你能检索这些顾客数据集，自动地发现市场分类，并自动地把顾客划分到不同的细分市场中，你才能自动并更有效地销售或不同的细分市场一起进行销售。</li>
<li><strong>天文数据分析</strong>：<br>这些聚类算法给出了令人惊讶、有趣、有用的理论，解释了星系是如何诞生的。这些都是聚类的例子，聚类只是无监督学习中的一种。</li>
<li><strong>语音识别</strong>：<br><img src="/picture/machine-learning/cocktail_party.jpg" alt="party"><br>鸡尾酒宴问题。嗯，你参加过鸡尾酒宴吧？你可以想像下，有个宴会房间里满是人，全部坐着，都在聊天，这么多人同时在聊天，声音彼此重叠，因为每个人都在说话，同一时间都在说话，你几乎听不到你面前那人的声音。所以，可能在一个这样的鸡尾酒宴中的两个人，他俩同时都在说话，假设现在是在个有些小的鸡尾酒宴中。我们放两个麦克风在房间中，因为这些麦克风在两个地方，离说话人的距离不同每个麦克风记录下不同的声音，虽然是同样的两个说话人。听起来像是份录音被叠加到一起，或是被归结到一起，产生了我们现在的这些录音。另外，这个算法还会区分出两个音频资源，这两个可以合成或合并成之前的录音，实际上，鸡尾酒算法的第一个输出结果是：1，2，3，4，5，6，7，8，9，10。<br>看看这个无监督学习算法，实现这个得要多么的复杂，是吧？它似乎是这样，为了构建这个应用，完成这个音频处理似乎需要你去写大量的代码或链接到一堆的合成器JAVA库,处理音频的库，看上去绝对是个复杂的程序，去完成这个从音频中分离出音频。事实上，这个算法对应你刚才知道的那个问题的算法可以就用一行代码来完成.就是这里展示的代码：<br><code>[W,s,v] = svd((repmat(sum(x.*x,1),size(x,1),1).*x)*x&#39;);</code><br>研究人员花费了大量时间才最终实现这行代码。我不是说这个是简单的问题，但它证明了，当你使用正确的编程环境，许多学习算法是相当短的程序。所以，这也是为什么在本课中，我们打算使用 Octave编程环境。Octave,是免费的开源软件，使用一个像Octave或Matlab的工具，许多学习算法变得只有几行代码就可实现。</li>
</ul>
<p>所以这个就是无监督学习，因为我们没有提前告知算法一些信息，比如，这是第一类的人，那些是第二类的人，还有第三类，等等。我们只是说，是的，这是有一堆数据。我不知道数据里面有什么。我不知道谁是什么类型。我甚至不知道人们有哪些不同的类型，这些类型又是什么。但你能自动地找到数据中的结构吗？就是说你要自动地聚类那些个体到各个类，我没法提前知道哪些是哪些。因为我们没有给算法正确答案来回应数据集中的数据，所以这就是无监督学习。</p>
<h1 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h1><p><a href="http://open.163.com/special/opencourse/machinelearning.html" target="_blank" rel="external">斯坦福大学机器学习视频教程</a></p>
]]></content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;介绍&quot;&gt;&lt;a href=&quot;#介绍&quot; class=&quot;headerlink&quot; title=&quot;介绍&quot;&gt;&lt;/a&gt;介绍&lt;/h1&gt;&lt;p&gt;机器学习是目前信息技术中最激动人心的方向之一。你或许每天都在不知不觉中使用了机器学习的算法。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;你打开谷歌、必应搜索到你需要的内容，正是因为他们有良好的学习算法，谷歌和微软实现了学习算法来排行网页。&lt;/li&gt;
&lt;li&gt;你用Facebook或苹果的图片分类程序他能认出你朋友的照片，这也是机器学习。&lt;/li&gt;
&lt;li&gt;每次您阅读您的电子邮件垃圾邮件筛选器，可以帮你过滤大量的垃圾邮件这也是一种学习算法。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;那么，为什么机器学习如此受欢迎呢？&lt;/strong&gt;&lt;br&gt;机器学习不只是用于人工智能领域。我们创造智能的机器，有很多基础的知识。比如，我们可以让机器找到A与B之间的最短路径，但我们仍然不知道怎么让机器做更有趣的事情，如web搜索、照片标记、反垃圾邮件。我们发现，&lt;strong&gt;唯一方法是让机器自己学习怎么来解决问题&lt;/strong&gt;。所以，机器学习已经成为计算机的一个能力，现在它涉及到各个行业和基础科学中。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;这里有一些机器学习的案例。&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;数据挖掘&lt;/strong&gt;。机器学习被用于数据挖掘的原因之一是网络和自动化技术的增长，这意味着，我们有史上最大的数据集比如说，大量的硅谷公司正在收集  web上的单击数据，也称为点击流数据，并尝试使用机器学习算法来分析数据，更好的了解用户，并为用户提供更好的服务。这在硅谷有巨大的市场。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;医疗记录&lt;/strong&gt;。随着自动化的出现，我们现在有了电子医疗记录。如果我们可以把医疗记录变成医学知识，我们就可以更好地理解疾病。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;计算生物学&lt;/strong&gt;。还是因为自动化技术，生物学家们收集的大量基因数据序列、DNA序列和等等，机器运行算法让我们更好地了解人类基因组，大家都知道这对人类意味着什么。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;工程方面&lt;/strong&gt;。在工程的所有领域，我们有越来越大、越来越大的数据集，我们试图使用学习算法，来理解这些数据。另外，在机械应用中，有些人不能直接操作。例如，我已经在无人直升机领域工作了许多年。我们不知道如何写一段程序让直升机自己飞。我们唯一能做的就是让计算机自己学习如何驾驶直升机。&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;手写识别&lt;/strong&gt;。现在我们能够非常便宜地把信寄到这个美国甚至全世界的原因之一就是当你&lt;br&gt;写一个像这样的信封，一种学习算法已经学会如何读你信封，它可以自动选择路径，所以我们只需要花几个美分把这封信寄到数千英里外。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;自然语言处理或计算机视觉&lt;/strong&gt;。这些语言理解或图像理解都是属于AI领域。大部分的自然语言处理和大部分的计算机视觉，都应用了机器学习。学习算法还广泛用于自定制程序。每次你去亚马逊或 Netflix或  iTunes Genius，它都会给出其他电影或产品或音乐的建议，这是一种学习算法。仔细想一想，他们有百万的用户；但他们没有办法为百万用户，编写百万个不同程序。软件能给这些自定制的建议的唯一方法是通过学习你的行为，来为你定制服务。&lt;/p&gt;
    
    </summary>
    
      <category term="机器学习" scheme="xtf615.com/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="机器学习" scheme="xtf615.com/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="概念" scheme="xtf615.com/tags/%E6%A6%82%E5%BF%B5/"/>
    
      <category term="人工智能" scheme="xtf615.com/tags/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/"/>
    
  </entry>
  
  <entry>
    <title>windows下idea编程实现远程发布任务到Spark集群</title>
    <link href="xtf615.com/2016/12/30/windows%E4%B8%8Bidea%E7%BC%96%E7%A8%8B%E5%AE%9E%E7%8E%B0%E8%BF%9C%E7%A8%8B%E5%8F%91%E5%B8%83%E4%BB%BB%E5%8A%A1%E5%88%B0Spark%E9%9B%86%E7%BE%A4/"/>
    <id>xtf615.com/2016/12/30/windows下idea编程实现远程发布任务到Spark集群/</id>
    <published>2016-12-30T11:47:13.000Z</published>
    <updated>2017-01-06T01:26:30.181Z</updated>
    
    <content type="html"><![CDATA[<h1 id="说明"><a href="#说明" class="headerlink" title="说明"></a>说明</h1><p>本文的目标是：在windows下，使用idea编写spark任务，并可直接右键运行提交至远程Linux Spark集群上，不需要打包后再拷贝至远程Linux服务器上，再使用命令运行。</p>
<h1 id="准备工作"><a href="#准备工作" class="headerlink" title="准备工作"></a>准备工作</h1><ul>
<li>软件<ul>
<li>win10</li>
<li>jdk1.7(windows版本:1.7.0_79)</li>
<li>scala2.11.8(windows版本：scala-2.11.8.zip)</li>
<li>idea 2016.3.2(windows版本：ideaIU-2016.3.2.exe)</li>
<li>hadoop2.7.3(linux版本：hadoop-2.7.3.tar.gz)</li>
<li>spark2.0.2(linux版本：spark-2.0.2-bin-hadoop2.7.tgz)</li>
<li>idea scala插件（scala-intellij-bin-2016.3.4.zip，<a href="https://plugins.jetbrains.com/idea/plugin/1347-scala）" target="_blank" rel="external">https://plugins.jetbrains.com/idea/plugin/1347-scala）</a></li>
<li>winutil.exe等（<a href="https://github.com/xuetf/spark/blob/master/idea/hadoop-common-2.2.0-bin.rar?raw=true" target="_blank" rel="external">winutil下载地址</a>）</li>
<li>maven3.3.9(windows版本：apache-maven-3.3.9-bin.zip)<a id="more"></a></li>
</ul>
</li>
<li>搭建Spark集群<br><a href="/2016/12/29/Spark%E5%88%86%E5%B8%83%E5%BC%8F%E7%8E%AF%E5%A2%83%E6%90%AD%E5%BB%BA%E6%95%99%E7%A8%8B/">分布式Spark集群搭建</a></li>
<li>配置windows环境变量<ul>
<li>jdk(windows版本) JAVA_HOME</li>
<li>scala(windows版本) SCALA_HOME</li>
<li>hadoop(linux版本) HADOOP_HOME</li>
<li>maven(windows版本) MAVEN_HOME<br><strong>注意：以上环境变量均在windows下配置，括号中强调了软件包的平台版本。</strong></li>
</ul>
</li>
<li><p>配置idea</p>
<ul>
<li><p>maven配置：</p>
<ul>
<li><p><strong>修改setting.xml</strong><br>修改%MAVEN_HOME%下的conf/setting.xml为阿里云镜像</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line">在mirrors节点添加：</div><div class="line">&lt;mirror&gt; </div><div class="line">    &lt;id&gt;nexus-aliyun&lt;/id&gt;</div><div class="line">    &lt;name&gt;Nexus aliyun&lt;/name&gt;</div><div class="line">    &lt;url&gt;http://maven.aliyun.com/nexus/content/groups/public&lt;/url&gt; </div><div class="line">    &lt;mirrorOf&gt;central&lt;/mirrorOf&gt; </div><div class="line">&lt;/mirror&gt;</div></pre></td></tr></table></figure>
</li>
<li><p><strong>修改idea的maven配置</strong><br>主要是为了加快建立maven项目时的速度<br> <img src="https://raw.githubusercontent.com/xuetf/spark/master/idea/maven_setting1.png" alt="maven-idea-setting1"><br> <img src="https://raw.githubusercontent.com/xuetf/spark/master/idea/maven_setting2.png" alt="maven-idea-setting2"> </p>
</li>
</ul>
</li>
<li><p>scala pluin配置<br> <img src="https://raw.githubusercontent.com/xuetf/spark/master/idea/idea_scala_plugin1.png" alt="scala-idea-setting1"><br> <img src="https://raw.githubusercontent.com/xuetf/spark/master/idea/idea_scala_plugin2.png" alt="scala-idea-setting1">     </p>
</li>
</ul>
</li>
</ul>
<h1 id="开发流程"><a href="#开发流程" class="headerlink" title="开发流程"></a>开发流程</h1><ul>
<li><strong>新建MAVEN+SCALA项目</strong><br>  <img src="https://raw.githubusercontent.com/xuetf/spark/master/idea/maven_scala.png" alt="maven-scala1"><br>  <img src="https://raw.githubusercontent.com/xuetf/spark/master/idea/maven_scala2.png" alt="maven-scala2"><br>  <img src="https://raw.githubusercontent.com/xuetf/spark/master/idea/maven_scala3.png" alt="maven-scala3"> </li>
<li><p><strong>配置JDK、SCALA</strong><br>  <img src="https://raw.githubusercontent.com/xuetf/spark/master/idea/maven_scala4.png" alt="maven-scala4"> </p>
</li>
<li><p><strong>添加POM依赖</strong></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div></pre></td><td class="code"><pre><div class="line">&lt;properties&gt;</div><div class="line">  &lt;spark.version&gt;2.0.2&lt;/spark.version&gt;</div><div class="line">  &lt;scala.version&gt;2.11&lt;/scala.version&gt;</div><div class="line">&lt;/properties&gt;</div><div class="line">&lt;dependency&gt;</div><div class="line">  &lt;groupId&gt;org.apache.spark&lt;/groupId&gt;</div><div class="line">  &lt;artifactId&gt;spark-core_$&#123;scala.version&#125;&lt;/artifactId&gt;</div><div class="line">  &lt;version&gt;$&#123;spark.version&#125;&lt;/version&gt;</div><div class="line">&lt;/dependency&gt;</div><div class="line">&lt;dependency&gt;</div><div class="line">  &lt;groupId&gt;org.apache.hadoop&lt;/groupId&gt;</div><div class="line">  &lt;artifactId&gt;hadoop-client&lt;/artifactId&gt;</div><div class="line">  &lt;version&gt;2.6.0&lt;/version&gt;</div><div class="line">&lt;/dependency&gt;</div></pre></td></tr></table></figure>
</li>
<li><p><strong>编写代码</strong></p>
  <figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div></pre></td><td class="code"><pre><div class="line">import org.apache.spark.&#123;SparkConf, SparkContext&#125;</div><div class="line">import scala.math.random</div><div class="line">object SparkPi &#123;</div><div class="line">  def main(args:Array[String]):Unit = &#123;</div><div class="line">    val conf = new SparkConf().setAppName(&quot;Spark Pi&quot;).setMaster(&quot;spark://172.16.21.121:7077&quot;)</div><div class="line">      .setJars(List(&quot;E:\\idea-workspace\\spark-practice\\out\\artifacts\\spark_practice_jar\\spark-practice.jar&quot;));</div><div class="line"></div><div class="line">    val spark = new SparkContext(conf)</div><div class="line">    val slices = if (args.length &gt; 0) args(0).toInt else 2</div><div class="line">    val n = 100000 * slices</div><div class="line">    val count = spark.parallelize(1 to n, slices).map &#123; i =&gt;</div><div class="line">      val x = random * 2 - 1</div><div class="line">      val y = random * 2 - 1</div><div class="line">      if (x * x + y * y &lt; 1) 1 else 0</div><div class="line">    &#125;.reduce(_ + _)</div><div class="line">    println(&quot;Pi is roughly &quot; + 4.0 * count / n)</div><div class="line">    spark.stop()</div><div class="line">  &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>  其中setMaster为：spark主节点的地址。setjars为下面步骤生成的jar包在window路径下的目录</p>
</li>
<li><p>添加输出sparkdemo.jar<br>  <img src="https://raw.githubusercontent.com/xuetf/spark/master/idea/artifacts1.png" alt="artifacts1"><br>  <img src="https://raw.githubusercontent.com/xuetf/spark/master/idea/artifacts2.png" alt="artifacts2"><br>  <img src="https://raw.githubusercontent.com/xuetf/spark/master/idea/artifacts3.png" alt="artifacts3"><br>  <img src="https://raw.githubusercontent.com/xuetf/spark/master/idea/artifacts4.png" alt="artifacts4"> </p>
</li>
<li><p>编译代码<br>  <img src="https://raw.githubusercontent.com/xuetf/spark/master/idea/build.png" alt="build"> </p>
</li>
<li><p><strong>删除输出的sparkdemo.jar中META-INF中多余文件</strong><br>只保留MANIFEST.MF和MAVEN文件夹<br><img src="https://raw.githubusercontent.com/xuetf/spark/master/idea/sparkdemo_mete_inf_delete.png" alt="delete"> </p>
</li>
<li><p><strong>include in build勾掉</strong><br>防止右键运行的时候，重新输出，导致mete-inf又恢复了<br><img src="https://raw.githubusercontent.com/xuetf/spark/master/idea/build2.png" alt="去掉include in build"> </p>
</li>
<li><p>设置VM参数<br><img src="https://raw.githubusercontent.com/xuetf/spark/master/idea/vm.png" alt="VM参数"> </p>
</li>
<li>右键运行<br><img src="https://raw.githubusercontent.com/xuetf/spark/master/idea/run-result.png" alt="run1"> </li>
<li>运行时可查看web控制台<br><img src="https://raw.githubusercontent.com/xuetf/spark/master/idea/spark-running.png" alt="run2"><br><img src="https://raw.githubusercontent.com/xuetf/spark/master/idea/spark-finished.png" alt="run3"> </li>
</ul>
]]></content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;说明&quot;&gt;&lt;a href=&quot;#说明&quot; class=&quot;headerlink&quot; title=&quot;说明&quot;&gt;&lt;/a&gt;说明&lt;/h1&gt;&lt;p&gt;本文的目标是：在windows下，使用idea编写spark任务，并可直接右键运行提交至远程Linux Spark集群上，不需要打包后再拷贝至远程Linux服务器上，再使用命令运行。&lt;/p&gt;
&lt;h1 id=&quot;准备工作&quot;&gt;&lt;a href=&quot;#准备工作&quot; class=&quot;headerlink&quot; title=&quot;准备工作&quot;&gt;&lt;/a&gt;准备工作&lt;/h1&gt;&lt;ul&gt;
&lt;li&gt;软件&lt;ul&gt;
&lt;li&gt;win10&lt;/li&gt;
&lt;li&gt;jdk1.7(windows版本:1.7.0_79)&lt;/li&gt;
&lt;li&gt;scala2.11.8(windows版本：scala-2.11.8.zip)&lt;/li&gt;
&lt;li&gt;idea 2016.3.2(windows版本：ideaIU-2016.3.2.exe)&lt;/li&gt;
&lt;li&gt;hadoop2.7.3(linux版本：hadoop-2.7.3.tar.gz)&lt;/li&gt;
&lt;li&gt;spark2.0.2(linux版本：spark-2.0.2-bin-hadoop2.7.tgz)&lt;/li&gt;
&lt;li&gt;idea scala插件（scala-intellij-bin-2016.3.4.zip，&lt;a href=&quot;https://plugins.jetbrains.com/idea/plugin/1347-scala）&quot;&gt;https://plugins.jetbrains.com/idea/plugin/1347-scala）&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;winutil.exe等（&lt;a href=&quot;https://github.com/xuetf/spark/blob/master/idea/hadoop-common-2.2.0-bin.rar?raw=true&quot;&gt;winutil下载地址&lt;/a&gt;）&lt;/li&gt;
&lt;li&gt;maven3.3.9(windows版本：apache-maven-3.3.9-bin.zip)
    
    </summary>
    
      <category term="spark" scheme="xtf615.com/categories/spark/"/>
    
    
      <category term="spark" scheme="xtf615.com/tags/spark/"/>
    
      <category term="idea" scheme="xtf615.com/tags/idea/"/>
    
      <category term="scala" scheme="xtf615.com/tags/scala/"/>
    
  </entry>
  
  <entry>
    <title>spark分布式环境搭建教程</title>
    <link href="xtf615.com/2016/12/29/Spark%E5%88%86%E5%B8%83%E5%BC%8F%E7%8E%AF%E5%A2%83%E6%90%AD%E5%BB%BA%E6%95%99%E7%A8%8B/"/>
    <id>xtf615.com/2016/12/29/Spark分布式环境搭建教程/</id>
    <published>2016-12-29T13:31:00.000Z</published>
    <updated>2016-12-29T15:34:47.954Z</updated>
    
    <content type="html"><![CDATA[<h1 id="概述"><a href="#概述" class="headerlink" title="概述"></a>概述</h1><p>  本文是对spark2.0.2分布式集群搭建的一个详细说明。旨在通过阅读该文章帮助开发人员快速搭建spark分布式集群。</p>
<h1 id="三种集群资源管理概述"><a href="#三种集群资源管理概述" class="headerlink" title="三种集群资源管理概述"></a>三种集群资源管理概述</h1><ul>
<li><p>Spark Standalone<br>作为Spark的一部分,Standalone是一个简单的集群管理器。它具有master的HA，弹性应对WorkerFailures，对每个应用程序的管理资源的能力，并且可以在现有的Hadoop一起运行和访问HDFS的数据。该发行版包括一些脚本，可以很容易地部署在本地或在AmazonEC2云计算。它可以在Linux，Windows或Mac OSX上运行。</p>
</li>
<li><p>Apache Mesos<br>Apache Mesos ,分布式系统内核，具有HA的masters和slaves，可以管理每个应用程序的资源，并对Docker容器有很好的支持。它可以运行Spark工作， Hadoop的MapReduce的，或任何其他服务的应用程序。它有Java， Python和C ++ 的API。它可以在Linux或Mac OSX上运行。</p>
</li>
<li><p>Hadoop YARN<br>Hadoop YARN，作业调度和集群资源管理的分布式计算框架，具有HA为masters和slaves，在非安全模式下支持Docker容器，在安全模式下支持Linux和Windows Container executors，和可插拔的调度器。它可以运行在Linux和Windows上运行。</p>
</li>
</ul>
<p><strong>本文将使用Hadoop YARN方式进行集群搭建。</strong><br><a id="more"></a></p>
<h1 id="准备"><a href="#准备" class="headerlink" title="准备"></a>准备</h1><ul>
<li><p><strong>装有centOS7的3台服务器</strong></p>
 <figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">master 172.16.21.121</div><div class="line">node1  172.16.21.129</div><div class="line">node2  172.16.21.130</div></pre></td></tr></table></figure>
</li>
<li><p><strong>搭建hadoop集群环境</strong><br><a href="/2016/12/29/hadoop%E5%88%86%E5%B8%83%E5%BC%8F%E7%8E%AF%E5%A2%83%E6%90%AD%E5%BB%BA%E6%95%99%E7%A8%8B/">hadoop分布式环境搭建教程</a></p>
</li>
<li><p><strong>scala: scala-2.12.1.tgz</strong></p>
</li>
<li><strong>spark: sprak-2.0.2-bin-hadoop2.7.tgz</strong></li>
<li><strong>上传sacala和spark到3台服务器</strong><!--more-->
<h1 id="安装Scala"><a href="#安装Scala" class="headerlink" title="安装Scala"></a>安装Scala</h1></li>
<li><strong>解压到/usr/local/scala</strong></li>
<li><p><strong>配置环境变量</strong></p>
  <figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">export SCALA_HOME=/usr/local/scala/scala-2.12.1</div><div class="line">export PATH=$PATH:$SCALA_HOME/bin</div></pre></td></tr></table></figure>
<p>  scala -version查看版本</p>
</li>
</ul>
<h1 id="安装spark"><a href="#安装spark" class="headerlink" title="安装spark"></a>安装spark</h1><ul>
<li><strong>解压</strong><br>  tar -zxvf spark-2.0.2-bin-hadoop2.7.tgz到/usr/local/spark</li>
<li><p><strong>配置环境变量</strong></p>
  <figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">export SPARK_HOME=/usr/local/spark/spark-2.0.2-bin-hadoop2.7</div><div class="line">export PATH=$PATH:$SPARK_HOME/bin</div></pre></td></tr></table></figure>
</li>
<li><p><strong>配置集群</strong></p>
<ul>
<li><p>master上：$SPARK_HOME/conf/slaves 添加:</p>
  <figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">node1 </div><div class="line">node2</div></pre></td></tr></table></figure>
</li>
<li><p>spark-env.sh： 添加SCALA_HOME和JAVA_HOME</p>
  <figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">export SCALA_HOME=/usr/local/scala/scala-2.12.1</div><div class="line">export JAVA_HOME=/usr/local/java/jdk1.8.0_73</div></pre></td></tr></table></figure>
</li>
<li><p>修改spark web 默认端口为8081</p>
  <figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">cd $SPARK_HOME/sbin</div><div class="line">vim start-master.sh</div><div class="line">if [ &quot;$SPARK_MASTER_WEBUI_PORT&quot; = &quot;&quot; ]; then</div><div class="line">  SPARK_MASTER_WEBUI_PORT=8081</div></pre></td></tr></table></figure>
</li>
</ul>
</li>
<li><p><strong>启动</strong></p>
<ul>
<li>启动hadoop集群,master上执行<br>  $HADOOP_HOME/sbin/start-all.sh</li>
<li>启动spark集群，master上执行<br>  $SPARK_HOME/sbin/start-all.sh</li>
<li><p>jps查看<br>  master:<br>  <img src="https://raw.githubusercontent.com/xuetf/spark/master/spark-master-jps.png" alt="spark-master-jps"></p>
<p>  node1:<br>  <img src="https://raw.githubusercontent.com/xuetf/spark/master/spark-node1-jps.png" alt="spark-node1-jps"></p>
<p>  node2:<br>  <img src="https://raw.githubusercontent.com/xuetf/spark/master/spark-node2-jps.png" alt="spark-node2-jps"></p>
</li>
</ul>
</li>
</ul>
<ul>
<li><p><strong>验证</strong></p>
<ul>
<li>访问master的8081<br>  <a href="http://172.16.21.121:8081/" target="_blank" rel="external">http://172.16.21.121:8081/</a><br>   <img src="https://raw.githubusercontent.com/xuetf/spark/master/spark-8081.png" alt="spark-node2-jps"></li>
<li><p>运行SparkPi例子</p>
  <figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">cd $SPARK_HOME</div><div class="line">bin/spark-submit --class org.apache.spark.examples.SparkPi --master     spark://master:7077 examples/jars/spark-examples_2.11-2.0.2.jar 100 2&gt;&amp;1 | grep &quot;Pi is roughly&quot;</div></pre></td></tr></table></figure>
<p>   <img src="https://raw.githubusercontent.com/xuetf/spark/master/sparkpi.png" alt="spark-node2-jps"></p>
</li>
</ul>
</li>
</ul>
<h1 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h1><p><a href="http://www.voidcn.com/blog/dream_broken/article/p-6319289.html" target="_blank" rel="external">http://www.voidcn.com/blog/dream_broken/article/p-6319289.html</a>    </p>
]]></content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;概述&quot;&gt;&lt;a href=&quot;#概述&quot; class=&quot;headerlink&quot; title=&quot;概述&quot;&gt;&lt;/a&gt;概述&lt;/h1&gt;&lt;p&gt;  本文是对spark2.0.2分布式集群搭建的一个详细说明。旨在通过阅读该文章帮助开发人员快速搭建spark分布式集群。&lt;/p&gt;
&lt;h1 id=&quot;三种集群资源管理概述&quot;&gt;&lt;a href=&quot;#三种集群资源管理概述&quot; class=&quot;headerlink&quot; title=&quot;三种集群资源管理概述&quot;&gt;&lt;/a&gt;三种集群资源管理概述&lt;/h1&gt;&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Spark Standalone&lt;br&gt;作为Spark的一部分,Standalone是一个简单的集群管理器。它具有master的HA，弹性应对WorkerFailures，对每个应用程序的管理资源的能力，并且可以在现有的Hadoop一起运行和访问HDFS的数据。该发行版包括一些脚本，可以很容易地部署在本地或在AmazonEC2云计算。它可以在Linux，Windows或Mac OSX上运行。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Apache Mesos&lt;br&gt;Apache Mesos ,分布式系统内核，具有HA的masters和slaves，可以管理每个应用程序的资源，并对Docker容器有很好的支持。它可以运行Spark工作， Hadoop的MapReduce的，或任何其他服务的应用程序。它有Java， Python和C ++ 的API。它可以在Linux或Mac OSX上运行。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Hadoop YARN&lt;br&gt;Hadoop YARN，作业调度和集群资源管理的分布式计算框架，具有HA为masters和slaves，在非安全模式下支持Docker容器，在安全模式下支持Linux和Windows Container executors，和可插拔的调度器。它可以运行在Linux和Windows上运行。&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;本文将使用Hadoop YARN方式进行集群搭建。&lt;/strong&gt;&lt;br&gt;
    
    </summary>
    
      <category term="spark" scheme="xtf615.com/categories/spark/"/>
    
    
      <category term="spark" scheme="xtf615.com/tags/spark/"/>
    
      <category term="大数据" scheme="xtf615.com/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
      <category term="分布式" scheme="xtf615.com/tags/%E5%88%86%E5%B8%83%E5%BC%8F/"/>
    
      <category term="内存" scheme="xtf615.com/tags/%E5%86%85%E5%AD%98/"/>
    
      <category term="环境" scheme="xtf615.com/tags/%E7%8E%AF%E5%A2%83/"/>
    
  </entry>
  
  <entry>
    <title>hadoop分布式环境搭建教程</title>
    <link href="xtf615.com/2016/12/29/hadoop%E5%88%86%E5%B8%83%E5%BC%8F%E7%8E%AF%E5%A2%83%E6%90%AD%E5%BB%BA%E6%95%99%E7%A8%8B/"/>
    <id>xtf615.com/2016/12/29/hadoop分布式环境搭建教程/</id>
    <published>2016-12-29T10:53:29.000Z</published>
    <updated>2016-12-29T15:44:31.304Z</updated>
    
    <content type="html"><![CDATA[<h1 id="概述"><a href="#概述" class="headerlink" title="概述"></a>概述</h1><p>本文是搭建hadoop分布式集群的一个详细说明，旨在通过本文，快速入手hadoop</p>
<h1 id="部署方案"><a href="#部署方案" class="headerlink" title="部署方案"></a>部署方案</h1><p>hadoop部署方案包括：单机模式、伪分布模式、完全分布模式</p>
<p><strong>本文将使用完全分布模式进行集群搭建</strong></p>
<a id="more"></a>
<h1 id="准备工作"><a href="#准备工作" class="headerlink" title="准备工作"></a>准备工作</h1><ul>
<li><strong>64位centos7服务器3台</strong><ul>
<li>master:172.16.21.121</li>
<li>node1:172.16.21.129</li>
<li>node2:172.16.21.130</li>
</ul>
</li>
<li><strong>hadoop-2.7.3.tar.gz</strong></li>
<li><strong>jdk-8u73-linux-x64.tar.gz</strong></li>
<li><strong>关闭防火墙</strong><br><code>service firewalld stop或systemctl stop firewalld.service</code></li>
<li><p><strong>关闭selinux</strong></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">setenforce 0临时关闭，sestatus查看状态:current mode变成permissive</div></pre></td></tr></table></figure>
</li>
<li><p><strong>纠正系统时间</strong></p>
<ul>
<li><p>设置时区</p>
  <figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">timedatectl查看时区</div><div class="line">timedatactl set-timezone Asia/Shanghai</div></pre></td></tr></table></figure>
</li>
<li><p>安装ntp并启动</p>
  <figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">yum -y install ntp</div><div class="line">systemctl enable ntpd</div><div class="line">start ntpd</div></pre></td></tr></table></figure>
</li>
</ul>
</li>
<li><p><strong>安装jdk</strong>    </p>
<pre><code><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div></pre></td><td class="code"><pre><div class="line">解压tar -zxvf jdk-8u73-linux-x64.tar.gz到/usr/local/java</div><div class="line">vim /etc/profile</div><div class="line">添加：</div><div class="line">export JAVA_HOME=/usr/local/java/jdk1.8.0_73</div><div class="line">export JRE_HOME=/$JAVA_HOME/jre</div><div class="line">export CLASSPATH=.:$JAVA_HOME/jre/lib/rt.jar:$JAVA_HOME/lib/dt.jar:$JAVA_HOME/lib/tools.jar</div><div class="line">export PATH=$PATH:$JAVA_HOME/bin:$JRE_HOME/bin</div><div class="line"></div><div class="line">source /etc/profile配置生效</div><div class="line">java -version查看</div></pre></td></tr></table></figure>
</code></pre></li>
<li><p><strong>配置主机域名</strong></p>
<ul>
<li><p>配置hostname</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div></pre></td><td class="code"><pre><div class="line">172.16.21.121(master): </div><div class="line">hostname master</div><div class="line">vim /etc/hostname 输入master</div><div class="line"></div><div class="line">172.16.21.129(node1): </div><div class="line">hostname node1</div><div class="line">vim /etc/hostname 输入node1</div><div class="line"></div><div class="line">172.16.21.130(node2): </div><div class="line">hostname node2</div><div class="line">vim /etc/hostname 输入node2</div></pre></td></tr></table></figure>
</li>
<li><p>配置host(3台服务器同时输入)</p>
 <figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">172.16.21.121 master</div><div class="line">172.16.21.129 node1</div><div class="line">172.16.21.130 node2</div></pre></td></tr></table></figure>
</li>
</ul>
</li>
<li><p><strong>ssh免密码登录</strong></p>
<ul>
<li><p>master上操作：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">ssh-keygen -t rsa 一直回车，信息中会看到.ssh/id_rsa.pub的路径</div><div class="line">复制：cat /root/.ssh/id_rsa.pub &gt;&gt; /root/.ssh/authorized_keys</div></pre></td></tr></table></figure>
</li>
<li><p>node1和node2上操作:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">创建node1和node2上root/.ssh目录:mkdir /root/.ssh</div></pre></td></tr></table></figure>
</li>
<li><p>master上操作：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">复制authorized_keys到node1和node2节点：</div><div class="line">scp /root/.ssh/authorized_keys root@172.16.21.129:/root/.ssh/</div><div class="line">scp /root/.ssh/authorized_keys root@172.16.21.130:/root/.ssh/</div></pre></td></tr></table></figure>
</li>
<li><p>master,node1,node2都操作:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">chmod 700 /root/.ssh</div></pre></td></tr></table></figure>
</li>
<li><p>master上验证: </p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">ssh master</div><div class="line">ssh node1</div><div class="line">ssh node2</div></pre></td></tr></table></figure>
</li>
</ul>
</li>
</ul>
<h1 id="配置Hadoop集群"><a href="#配置Hadoop集群" class="headerlink" title="配置Hadoop集群"></a>配置Hadoop集群</h1><ul>
<li>解压 tar -zxvf hadoop-2.7.3.tar.gz, 到/usr/local/hadoop</li>
<li><p>配置环境变量：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line">vim /etc/profile</div><div class="line">添加：</div><div class="line">export HADOOP_HOME=/usr/local/hadoop/hadoop-2.7.3</div><div class="line">export PATH=$PATH:$HADOOP_HOME/bin:$HADOOP_HOME/sbin</div><div class="line">生效：source /etc/profile</div><div class="line">查看版本: hadoop version</div></pre></td></tr></table></figure>
<ul>
<li><p>修改hadoop配置添加JAVA_HOME </p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">vim /usr/local/hadoop/hadoop-2.7.3/etc/hadoop hadoop-env.sh</div><div class="line">vim /usr/local/hadoop/hadoop-2.7.3/etc/hadoop yarn-env.sh</div><div class="line">export JAVA_HOME=/usr/local/java/jdk1.8.0_73</div></pre></td></tr></table></figure>
</li>
<li><p><strong>创建目录</strong></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">mkdir -p /usr/local/hadoop/hdfs/data</div><div class="line">mkdir -p /usr/local/hadoop/hdfs/name</div><div class="line">mkdir -p /usr/local/tmp</div></pre></td></tr></table></figure>
</li>
<li><p><strong>配置core-site.xml</strong></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div></pre></td><td class="code"><pre><div class="line">&lt;configuration&gt;</div><div class="line">    &lt;property&gt;</div><div class="line">            &lt;name&gt;hadoop.tmp.dir&lt;/name&gt;</div><div class="line">            &lt;value&gt;/usr/local/hadoop/tmp&lt;/value&gt;</div><div class="line">    &lt;/property&gt;</div><div class="line">    &lt;property&gt;</div><div class="line">            &lt;name&gt;fs.defaultFS&lt;/name&gt;</div><div class="line">            &lt;value&gt;hdfs://master:9000&lt;/value&gt;</div><div class="line">    &lt;/property&gt;</div><div class="line">    &lt;property&gt;</div><div class="line">            &lt;name&gt;io.file.buffer.size&lt;/name&gt;</div><div class="line">            &lt;value&gt;4096&lt;/value&gt;</div><div class="line">    &lt;/property&gt;</div><div class="line">&lt;/configuration&gt;</div></pre></td></tr></table></figure>
</li>
</ul>
</li>
</ul>
<ul>
<li><strong>配置hdfs-site.xml</strong> <figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div></pre></td><td class="code"><pre><div class="line">&lt;configuration&gt;</div><div class="line">   &lt;property&gt;</div><div class="line">      &lt;name&gt;dfs.namenode.name.dir&lt;/name&gt;</div><div class="line">      &lt;value&gt;file:/hadoop/hdfs/name&lt;/value&gt;</div><div class="line">   &lt;/property&gt;</div><div class="line">   &lt;property&gt;</div><div class="line">      &lt;name&gt;dfs.datanode.data.dir&lt;/name&gt;</div><div class="line">      &lt;value&gt;file:/hadoop/hdfs/data&lt;/value&gt;</div><div class="line">   &lt;/property&gt;</div><div class="line">   &lt;property&gt;</div><div class="line">      &lt;name&gt;dfs.replication&lt;/name&gt;</div><div class="line">      &lt;value&gt;2&lt;/value&gt;</div><div class="line">   &lt;/property&gt;</div><div class="line">   &lt;property&gt;</div><div class="line">      &lt;name&gt;dfs.namenode.secondary.http-address&lt;/name&gt;</div><div class="line">      &lt;value&gt;master:9001&lt;/value&gt;</div><div class="line">   &lt;/property&gt;</div><div class="line">   &lt;property&gt;</div><div class="line">      &lt;name&gt;dfs.webhdfs.enabled&lt;/name&gt;</div><div class="line">      &lt;value&gt;true&lt;/value&gt;</div><div class="line">   &lt;/property&gt;</div><div class="line">&lt;/configuration&gt;</div></pre></td></tr></table></figure>
</li>
</ul>
<ul>
<li><p><strong>复制mapred-site.xml.template为mapred-site.xml,并修改</strong></p>
<pre><code>cp mapred-site.xml.template mapred-site.xml
</code></pre><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div></pre></td><td class="code"><pre><div class="line">  &lt;configuration&gt;</div><div class="line">    &lt;property&gt;</div><div class="line">            &lt;name&gt;mapreduce.framework.name&lt;/name&gt;</div><div class="line">            &lt;value&gt;yarn&lt;/value&gt;</div><div class="line">            &lt;final&gt;true&lt;/final&gt;</div><div class="line">    &lt;/property&gt;</div><div class="line">    &lt;property&gt;</div><div class="line">            &lt;name&gt;mapreduce.jobtracker.http.address&lt;/name&gt;</div><div class="line">            &lt;value&gt;master:50030&lt;/value&gt;</div><div class="line">    &lt;/property&gt;</div><div class="line"></div><div class="line">    &lt;property&gt;</div><div class="line">            &lt;name&gt;mapreduce.jobhistory.address&lt;/name&gt;</div><div class="line">            &lt;value&gt;master:10020&lt;/value&gt;</div><div class="line">    &lt;/property&gt;</div><div class="line">    &lt;property&gt;</div><div class="line">            &lt;name&gt;mapreduce.jobhistory.webapp.address&lt;/name&gt;</div><div class="line">            &lt;value&gt;master:19888&lt;/value&gt;</div><div class="line">    &lt;/property&gt;</div><div class="line"></div><div class="line">    &lt;property&gt;</div><div class="line">            &lt;name&gt;mapred.job.tracker&lt;/name&gt;</div><div class="line">            &lt;value&gt;http://master:9001&lt;/value&gt;</div><div class="line">    &lt;/property&gt;</div><div class="line">&lt;/configuration&gt;</div></pre></td></tr></table></figure>
</li>
<li><p><strong>修改yarn-site.xml</strong></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div></pre></td><td class="code"><pre><div class="line">&lt;configuration&gt;</div><div class="line">    &lt;property&gt;</div><div class="line">            &lt;name&gt;yarn.resourcemanager.hostname&lt;/name&gt;</div><div class="line">             &lt;value&gt;master&lt;/value&gt;</div><div class="line">    &lt;/property&gt;</div><div class="line">    &lt;property&gt;</div><div class="line">            &lt;name&gt;yarn.nodemanager.aux-services&lt;/name&gt;</div><div class="line">            &lt;value&gt;mapreduce_shuffle&lt;/value&gt;</div><div class="line">    &lt;/property&gt;</div><div class="line">    &lt;property&gt;</div><div class="line">            &lt;name&gt;yarn.resourcemanager.address&lt;/name&gt;</div><div class="line">            &lt;value&gt;master:8032&lt;/value&gt;</div><div class="line">    &lt;/property&gt;</div><div class="line">    &lt;property&gt;</div><div class="line">             &lt;name&gt;yarn.resourcemanager.scheduler.address&lt;/name&gt;</div><div class="line">             &lt;value&gt;master:8030&lt;/value&gt;</div><div class="line">    &lt;/property&gt;</div><div class="line">    &lt;property&gt;</div><div class="line">             &lt;name&gt;yarn.resourcemanager.resource-tracker.address&lt;/name&gt;</div><div class="line">            &lt;value&gt;master:8031&lt;/value&gt;</div><div class="line">    &lt;/property&gt;</div><div class="line">    &lt;property&gt;</div><div class="line">            &lt;name&gt;yarn.resourcemanager.admin.address&lt;/name&gt;</div><div class="line">            &lt;value&gt;master:8033&lt;/value&gt;</div><div class="line">    &lt;/property&gt;</div><div class="line">    &lt;property&gt;</div><div class="line">            &lt;name&gt;yarn.resourcemanager.webapp.address&lt;/name&gt;</div><div class="line">            &lt;value&gt;master:8088&lt;/value&gt;</div><div class="line">    &lt;/property&gt;</div><div class="line">&lt;/configuration&gt;</div></pre></td></tr></table></figure>
</li>
<li><p><strong>将以上步骤操作在node1和node2上重复</strong><br>  可将修改的文件拷贝至node1和node2节点</p>
</li>
<li><p><strong>修改master上的slaves文件</strong><br>  $HADOOP_HOME/etc/hadoop/slaves<br>  删除localhost<br>  添加:</p>
  <figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">node1</div><div class="line">node2</div></pre></td></tr></table></figure>
</li>
<li><p><strong>启动</strong></p>
<ul>
<li><p><strong>只在master上操作</strong></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line">master上格式化：</div><div class="line">cd $HADOOP_HOME/bin/</div><div class="line">./hadoop namenode -format</div><div class="line">master上启动：</div><div class="line">cd $HADOOP_HOME/sbin/</div><div class="line">./start-all.sh</div></pre></td></tr></table></figure>
</li>
<li><p><strong>查看jps：</strong><br>  jps<br>  master: ResourceManager SecondaryNameNode NameNode<br>  <img src="https://raw.githubusercontent.com/xuetf/hadoop/master/master-jps.png" alt="master-jps"></p>
<p>  node1/node2: DataNode NodeManager<br>  <img src="https://raw.githubusercontent.com/xuetf/hadoop/master/node1-jps.png" alt="node1-jps"><br>  <img src="https://raw.githubusercontent.com/xuetf/hadoop/master/node2-jps.png" alt="node2-jps"></p>
</li>
<li><p>访问master的50070：<br>  <a href="http://172.16.21.121:50070" target="_blank" rel="external">http://172.16.21.121:50070</a><br>  <img src="https://raw.githubusercontent.com/xuetf/hadoop/master/50070.png" alt="master-50070"></p>
</li>
<li>访问master的8088：<br>   <a href="http://172.16.21.121:8088" target="_blank" rel="external">http://172.16.21.121:8088</a><br>  <img src="https://raw.githubusercontent.com/xuetf/hadoop/master/8088.png" alt="master-8088"></li>
</ul>
</li>
</ul>
<h1 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h1><p><a href="http://www.voidcn.com/blog/dream_broken/article/p-6319288.html" target="_blank" rel="external">http://www.voidcn.com/blog/dream_broken/article/p-6319288.html</a></p>
]]></content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;概述&quot;&gt;&lt;a href=&quot;#概述&quot; class=&quot;headerlink&quot; title=&quot;概述&quot;&gt;&lt;/a&gt;概述&lt;/h1&gt;&lt;p&gt;本文是搭建hadoop分布式集群的一个详细说明，旨在通过本文，快速入手hadoop&lt;/p&gt;
&lt;h1 id=&quot;部署方案&quot;&gt;&lt;a href=&quot;#部署方案&quot; class=&quot;headerlink&quot; title=&quot;部署方案&quot;&gt;&lt;/a&gt;部署方案&lt;/h1&gt;&lt;p&gt;hadoop部署方案包括：单机模式、伪分布模式、完全分布模式&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;本文将使用完全分布模式进行集群搭建&lt;/strong&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="hadoop" scheme="xtf615.com/categories/hadoop/"/>
    
    
      <category term="大数据" scheme="xtf615.com/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
      <category term="分布式" scheme="xtf615.com/tags/%E5%88%86%E5%B8%83%E5%BC%8F/"/>
    
      <category term="环境" scheme="xtf615.com/tags/%E7%8E%AF%E5%A2%83/"/>
    
      <category term="hadoop" scheme="xtf615.com/tags/hadoop/"/>
    
  </entry>
  
  <entry>
    <title>redis分布式环境搭建教程</title>
    <link href="xtf615.com/2016/12/29/redis%E5%88%86%E5%B8%83%E5%BC%8F%E7%8E%AF%E5%A2%83%E6%90%AD%E5%BB%BA%E6%95%99%E7%A8%8B/"/>
    <id>xtf615.com/2016/12/29/redis分布式环境搭建教程/</id>
    <published>2016-12-29T07:56:43.000Z</published>
    <updated>2016-12-29T15:42:48.219Z</updated>
    
    <content type="html"><![CDATA[<h1 id="redis部署说明"><a href="#redis部署说明" class="headerlink" title="redis部署说明"></a>redis部署说明</h1><ul>
<li><strong>版本</strong><br>使用redis最新版3.2.3进行安装</li>
<li><strong>主从关系</strong><br>使用1个主节点，3个从节点。主节点提供读写操作，从节点只提供读操作。主节点Master安装在dbp模块，提供大量的写操作服务；  3个从节点。</li>
<li><strong>哨兵机制</strong><br>配置3个哨兵，主节点dbp安装1个哨兵，另外3台从服务器选其中两台各安装一个。作为HA高可用方案，防止主节点单点失败，通过重新选举主节点实现故障快速转移。<a id="more"></a>
</li>
</ul>
<h1 id="安装具体步骤"><a href="#安装具体步骤" class="headerlink" title="安装具体步骤"></a>安装具体步骤</h1><ul>
<li>解压</li>
<li>安装gcc</li>
<li>进入redis的bin目录，先执行 make MALLOC=libc； 再执行make install</li>
<li>配置文件：先拷贝redis目录下的配置文件redis.conf和sentinel.conf到/usr/local/etc(或其他任意目录)，再修改<ul>
<li><strong>redis节点配置</strong><br>  <code><strong>bind 主机ip</strong>                        #主机ip<br>  <strong>protected-mode no</strong>                     #保护模式关闭，否则不能通过远程连接，哨兵机制也不起作用，下面使用密码进行安全保证<br>  <strong>port 端口</strong>                          #端口<br>  <strong>daemonize yes</strong>                       #守护进程<br>  <strong>pidfile  /var/run/redis_端口.pid</strong>            #进程号，命名规则redis_端口号.pid<br>  logfile /usr/local/logs/redis/redis_端口.log   #日志文件<br>  <strong>dir  /usr/local/data/redis/端口</strong>          #持久化文件夹，必须是空文件夹<br>  <strong>requirepass 密码</strong>    #认证密码<br>  <strong>masterauth 密码</strong>    #和认证密码一致<br>  <strong>maxmemory 最大内存</strong>  #eg:10g<br>  <strong>maxmemory-policy</strong>        allkeys-lru   #lru算法回收<br>  </code></li>
<li><strong>从节点需要额外配置</strong><br>  <code>slaveof 主机 ip  #例如slaveof  172.16.21.127  6379</code></li>
<li><strong>Sentinel哨兵节点</strong><br><code>port  端口    #命名规则： 本机redis端口前加个2,比如redis:6379: 则sentinel：26379<br>  <strong>sentinel announce-ip</strong>  主机ip<br>  <strong>protected-mode  no</strong>  #需要手动添加这条。<br>  <strong>dir</strong>  /usr/local/data/sentinel_端口    #空文件夹<br>  <strong>logfile</strong>  /usr/local/logs/redis/sentinel_端口.log<br>  <strong>sentinel monitor 主节点名称 主节点ip 主节点端口 仲裁至少需要的哨兵数</strong> #eg：sentinel monitor mymaster  172.16.21.127 6379 2<br>  <strong>sentinel auth-pass 主节点名称 密码</strong>   #认证<br>  </code></li>
</ul>
</li>
<li><strong>进入redis的src目录启动redis和sentinel</strong><br>  <code><strong>reids-server redis配置文件</strong><br>  #eg:redis-server /usr/local/etc/redis_6379.conf<br>  <strong>redis-sentinel sentinel配置文件</strong> &amp;<br>  #eg:redis-sentinel /usr/local/etc/sentinel_26379.conf &amp;<br>  </code></li>
<li><strong>依次启动主节点和从节点后，使用redis-cli连接</strong><br>  <code><strong>reids-cli -h ip地址 -p 端口 -a 密码</strong><br> <strong>sentinel reset mymaster</strong> #重置哨兵状态*<br>  使用命令查看部署情况，info replication可查看集群状态<br>  </code></li>
</ul>
<h1 id="具体配置参见"><a href="#具体配置参见" class="headerlink" title="具体配置参见"></a>具体配置参见</h1><p><a href="https://github.com/xuetf/redis" target="_blank" rel="external">https://github.com/xuetf/redis</a></p>
<h1 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h1><p><a href="http://blog.csdn.net/ownfire/article/details/51546543" target="_blank" rel="external">http://blog.csdn.net/ownfire/article/details/51546543</a><br><a href="http://www.ilanni.com/?p=11838" target="_blank" rel="external">http://www.ilanni.com/?p=11838</a></p>
]]></content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;redis部署说明&quot;&gt;&lt;a href=&quot;#redis部署说明&quot; class=&quot;headerlink&quot; title=&quot;redis部署说明&quot;&gt;&lt;/a&gt;redis部署说明&lt;/h1&gt;&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;版本&lt;/strong&gt;&lt;br&gt;使用redis最新版3.2.3进行安装&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;主从关系&lt;/strong&gt;&lt;br&gt;使用1个主节点，3个从节点。主节点提供读写操作，从节点只提供读操作。主节点Master安装在dbp模块，提供大量的写操作服务；  3个从节点。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;哨兵机制&lt;/strong&gt;&lt;br&gt;配置3个哨兵，主节点dbp安装1个哨兵，另外3台从服务器选其中两台各安装一个。作为HA高可用方案，防止主节点单点失败，通过重新选举主节点实现故障快速转移。
    
    </summary>
    
      <category term="redis" scheme="xtf615.com/categories/redis/"/>
    
    
      <category term="分布式" scheme="xtf615.com/tags/%E5%88%86%E5%B8%83%E5%BC%8F/"/>
    
      <category term="环境" scheme="xtf615.com/tags/%E7%8E%AF%E5%A2%83/"/>
    
      <category term="redis" scheme="xtf615.com/tags/redis/"/>
    
      <category term="缓存" scheme="xtf615.com/tags/%E7%BC%93%E5%AD%98/"/>
    
      <category term="HA方案" scheme="xtf615.com/tags/HA%E6%96%B9%E6%A1%88/"/>
    
  </entry>
  
</feed>
