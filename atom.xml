<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>蘑菇先生学习记</title>
  
  <link href="/atom.xml" rel="self"/>
  
  <link href="xtf615.com/"/>
  <updated>2017-09-30T09:11:21.630Z</updated>
  <id>xtf615.com/</id>
  
  <author>
    <name>xuetf</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>Learning Theory(2)</title>
    <link href="xtf615.com/2017/09/30/Learning-Theory-2/"/>
    <id>xtf615.com/2017/09/30/Learning-Theory-2/</id>
    <published>2017-09-30T08:49:06.000Z</published>
    <updated>2017-09-30T09:11:21.630Z</updated>
    
    <content type="html"><![CDATA[<p>　　本部分将继续讨论学习理论的相关知识。学习理论内容包括：模型/特征选择(model/feature selection)、贝叶斯统计和正则化(Bayesian statistics and Regularization)。本文重点在于贝叶斯统计和正则化。将重点理解这句话，从贝叶斯估计的角度来看,正则化项对应于模型的先验概率。<br><a id="more"></a></p>
<h1 id="模型选择"><a href="#模型选择" class="headerlink" title="模型选择"></a>模型选择</h1><p>　　假设我们尝试在许多不同的模型中为我们的学习问题寻找最合适的模型。例如我们可能想在众多多项式回归模型\(h_\theta(x)=g(\theta_0+\theta_1x+\theta_2 x^2 + … + \theta_k x^k)\)中进行寻找，那么就需要确定k的值是多少(0,1,…,or 10)。我们怎样才能自动选择一个合适的模型，该模型能对偏差和方差进行很好的权衡呢？又或者该如何为SVM模型选择惩罚参数\(C\)和正则化项? 该如何为局部加权回归选择带宽参数?<br>　　更具体地讨论，我们考虑有限模型集合\(\mathcal{M}=\{M_1,…,M_d\}\),我们将从该集合中选择模型。例如对于上面的问题，模型\(M_i\)可以代表i阶多项式模型。当然，该模型集合也可以包含不同种类的模型，例如SVM、神经网络或者逻辑回归等。</p>
<h2 id="交叉验证"><a href="#交叉验证" class="headerlink" title="交叉验证"></a>交叉验证</h2><p>　　选择模型最简单的方式，就是根据ERM经验风险最小化原理，对每个模型M，取训练误差最小的模型。显然，这样最终会容易选择那些过拟合的模型。例如考虑选择多项式模型，显然阶数越高，训练集拟合的就越好，则训练误差也越小，因此这种方法选择的模型总是会出现高方差、高阶数。<br>　　简单的对其进行修改，我们就可以得到保留交叉验证的方法或称作简单交叉验证(hold-out cross validation also called simple cross validation),具体做法如下：<br>　　- 随机划分数据S为\(S_{train}\)(如70%的数据)、\(S_{cv}\)(如30%)。\(S_{cv}\)称作保留交叉验证集。<br>　　- 对每个模型\(M_i\)，在\(S_{train}\)上进行训练,得到假设\(h_i\)<br>　　- 对得到的每个假设\(h_i\),在\(S_{cv}\)上测试，求出误差\(\hat{\epsilon}_{S_{cv}}(h_i)\),即假设\(h_i\)在验证集上的经验误差。<br>　　- 选择误差最小的模型作为最终模型。<br>　　通过在验证集上进行测试，我们得到了一个对模型更好的估计。在实际使用过程中，对于得到的最终模型，我们可以将该模型在全部的数据集上重新训练，以利用更多的数据，达到更好的效果。<br>　　该方法的劣势在于分出过多的数据用来测试，对于标注数据难得的实际问题来说(比如医学实验),这是不能容忍的。因而产生了如下的改进方法，即k折交叉验证。<br>　　k折交叉验证(k-fold cross validation)，做法如下：<br>　　- 将标注数据集S随机平均分成k份,每份有m/k个样本,记为\(S_1,…,S_k\)<br>　　- 对于每一个模型\(M_i\),<br>　　　\(For \ j = 1,…,k\)<br>　　　　分别在\(S_1 \cup S_2…\cup S_{j-1} \cup S_{j+1} \cup S_k\)上训练，即除了\(S_j\)以外的子样本集上进行训练，得到假设\(h_{ij}\)<br>　　　　再使用\(h_{ij}\)对保留的\(S_j\)进行测试，得到误差\(\hat{\epsilon}_{S_j}(h_{ij})\)<br>　　　模型\(M_i\)的泛化误差可以通过计算\(\hat{\epsilon}_{S_j}(h_{ij})\)的平均值来估计。<br>　　- 选择泛化误差估计值最小的模型\(M_i\)作为最终的模型。同样，可以使用全部的数据集对最终模型进行训练。<br>　　k通常取值为10。此时保留的验证集大小每次只有1/k,比之前的30%小的多。但是这个过程的缺点是比较耗时，我们需要对每个模型训练k次。<br>　　当样本很少的时候，我们也可能取极端情况k=m。称为留一交叉验证(leave-one-out cross validation)，即每次训练只保留1个样本作为验证集。<br>　　上面讨论的交叉验证用于选择模型，同样，交叉验证也可以用于评估一个模型的好坏，例如可以使用交叉验证在不同测试集上对模型性能进行评估。</p>
<h1 id="特征选择"><a href="#特征选择" class="headerlink" title="特征选择"></a>特征选择</h1><p>　　特征选择是一类比较特殊的模型选择问题。考虑你面对一个监督学习问题，该问题的特征数量n非常巨大(甚至有 \(n \gg m\)),但是实际上只有部分特征和学习任务是相关的。即使使用一个简单的线性模型，例如感知机，按照之前VC维分析，n个特征有n+1个参数，也需要O(n)级别的样本数才能得到一个较好的模型。如果训练数据不够，那么就会导致过拟合的问题，因此需要考虑特征选择。</p>
<h2 id="前向-后向选择"><a href="#前向-后向选择" class="headerlink" title="前向/后向选择"></a>前向/后向选择</h2><p>　　对于n个特征来说，特征子集的数量有\(2^n\)个，如何进行选择呢?穷举法计算量太大，必然不可行。本部分介绍一种启发式的算法，前向选择法(Forward Search)。步骤如下：<br>　　- 初始化特征子集为\(\mathcal{F}=\varnothing\)<br>　　- 重复如下步骤：<br>　　　1) For i = 1,…,n, If \(i \notin \mathcal{F}\),let \(\mathcal{F} = \mathcal{F} \cup \{i\}\),使用交叉验证来评估特征集\(\mathcal{F}_i\)<br>　　　2) 令\(\mathcal{F}\)为上面步骤中评估性能最好的特征子集，即选择提升最大的特征加入原特征子集。<br>　　- 直到性能不再提升，或已经达到设置的阙值k个特征上限。<br>　　该算法也被称作wrapper model feature selection，因为它将模型的训练和评测包含在算法的内部。<br>　　根据前向选择法，可以容易得到后向选择法(Backward Search)，即初始化特征集包含所有的特征，然后每次删除对模型性能影响最不大的特征。<br>　　上面的方法虽然可以达到较优的特征选择结果，但是由于其反复多次调用模型训练算法，因而计算量非常大，尤其是在训练数据量比较大的时候。为了使得特征选择更简便，提出了一种更简单的特征选择方法——过滤法。</p>
<h2 id="过滤特征选择"><a href="#过滤特征选择" class="headerlink" title="过滤特征选择"></a>过滤特征选择</h2><p>　　过滤特征选择(Filter Feature Selection),采用一种启发式的规则对特征进行评分，评分指标衡量了特征对于样本标记结果y的影响程度，或称作特征的信息量大小。选择评分较优的k个特征，其计算量相对于前后向搜索算法来说已经非常小了。<br>　　一个可能的评分函数是衡量\(x_i\)和y的相关性，从而选择出与类别标记y最相关的特征\(x_i\)。而相关性可以使用互信息(mutual information,MI)来表示，当\(x_i\)是离散变量时，MI的计算公式如下：<br>$$MI(x_i,y)=\sum_{x_i}\sum{y_i} p(x_i,y) log \frac{p(x_i,y)}{p(x_i)p(y)}$$<br>其中\(p(x_i,y)\)指特征和类别的联合概率分布,\(p(x_i,y),p(x_i),p(y)\)都可以通过训练集来估计出经验概率分布。<br>　　为了更直观的理解，互信息也可以表示为KL(Kullback-Leibler)距离的形式：<br>$$MI(x_i,y)=KL(p(x_i,y) || p(x_i)p(y))$$<br>　　KL距离的作用是衡量分布之间的差异。就此例来说，如果\(x_i,y\)相互独立，那么它们的KL距离为0,如果它们之间的关联关系比较强，那么KL距离会变大。<br>　　使用MI进行衡量后，我们得到了各个特征的评分，那么选择多少个特征可以让模型效果最好呢?标准的方法还是采用交叉验证进行选择。例如，选择出k个特征，那么对特征子集{1},{1,2},{1,2,3},…,{1,2,3…,k}分别进行交叉验证，看哪一个子集效果最好。</p>
<h1 id="贝叶斯估计"><a href="#贝叶斯估计" class="headerlink" title="贝叶斯估计"></a>贝叶斯估计</h1><p>　　在开始前，强烈建议先阅读这篇博文<a href="http://blog.csdn.net/daunxx/article/details/51725086" target="_blank" rel="external">贝叶斯线性回归(Bayesian Linear Regression)</a>,这篇文章写得非常好! 本文会截取部分内容，并修改一些符号标记为我们之前比较习惯使用的标记。<br>　　这部分将讨论如何应对过拟合的问题。为了讨论贝叶斯统计，我们首先从极大似然估计、最大后验估计谈起。</p>
<h2 id="关于参数估计"><a href="#关于参数估计" class="headerlink" title="关于参数估计"></a>关于参数估计</h2><p>　　在很多的<strong>机器学习</strong>或数据挖掘的问题中，我们所面对的只有数据，但数据中潜在的<strong>概率密度函数</strong>是不知道的，其概率密度分布需要我们从数据中估计出来。想要确定数据对应的概率密度分布，就需要确定两个东西：<strong>概率密度函数的形式</strong> 和 <strong>概率密度函数的参数</strong>。<br>　　有时可能知道的是概率密度函数的形式(高斯、伯努利等等)，但是不知道具体的参数，例如均值或者方差；还有的时候可能不知道概率密度的类型，但是知道一些估计的参数，比如均值和方差。<br>　　关于上面提到的需要确定的两个东西：概率密度函数的<strong>形式</strong>和<strong>参数</strong>，我们目前所接触的大部分教材知识，基本都是：给了一堆数据，然后假设其概率密度函数的形式为<strong>高斯分布</strong>，或者是混合高斯分布，那么，剩下的事情就是对高斯分布的参数，\(\mu\)和\(\sigma^2\) 进行估计。所以，参数估计便成了极其最重要的问题。<br>　　其实，常用的参数估计方法有：极大似然估计、最大后验估计、贝叶斯估计、最大熵估计、混合模型估计。他们之间是有递进关系的，想要理解后一个参数估计方法，最好对前一个参数估计有足够的理解。</p>
<h2 id="极大似然估计"><a href="#极大似然估计" class="headerlink" title="极大似然估计"></a>极大似然估计</h2><p>　　首先回顾一下极大似然估计。<br>　　这里先以一个分类问题来说明一般参数估计面对的数据形式。考虑一个\(M\)类的问题，特征向量服从\(p(x|\omega_i),i=1,2…,M\)分布。这是现实情况中最常见的一种数据存在形式，数据集合\(X\)是由\(M\)个类别的数据子集\(X_m,m=1,2…,M\)组成的，第\(m\)类别的数据子集\(X_m\) 对应的概率密度函数是\(p(x|\omega_m)\)。\(\omega\)可以理解为不同类别的数据子集可以建立不同的模型，如果所有类别子集采用同一个模型，实际上\(\omega\)可以省略不写。<br>　　前面已经介绍过了，想要确定数据的概率分布，需要知道概率密度函数的 <strong>形式</strong> 和 <strong>参数</strong>，这里首先做一个基本假设：概率分布的形式已知，比如假设每个类别的数据都满足高斯分布，那么，似然函数就可以以参数\(θ_i\)的形式表示，如果是高斯分布，则参数为\(\mu_i\)和\(\sigma_i^2\)，即\(θ_i=(μ_i,σ^2_i)\)。<br>　　为了强调概率分布\(p(x|\omega_i)\)和 \(θ_i\) 有关，将对应的概率密度函数记为\(p(x|\omega_i;θ_i)\)，这种记法属于频率概率学派的记法。这里的极大似然估计对应于一个\(类条件概率密度函数\)。<br>　　从上面的描述中可以知道，利用每一个类\(X_i\)中已知的特征向量集合，可以估计出其对应的参数\(θ_i\)。进一步<strong>假设每一类中的数据不影响其他类别数据的参数估计</strong>，那么上面的\(M\)个类别的参数估计就可以用下面这个统一的模型，独立的解决：<br>　　设\(x^{(1)},x^{(2)},…,x^{(m)}\)是从概率密度函数\(p(x;\theta)\)随机抽取的样本，那么就可以得到<strong>联合概率密度函数</strong>\(p(X;\theta)\),其中\(X=\{x^{(1)},x^{(2)},…,x^{(m)}\}\)是样本集合，假设不同的样本之间具有统计独立性，那么：<br>$$p(X;\theta)\equiv p(x^{(1)},x^{(2)},…,x^{(m)};\theta)=\prod_{k=1}^mp(x^{(k)};\theta)$$<br>　　注意，这里的\(p(x^{(k)};\theta)\)本来的写法是\(p(x^{(k)}|\omega_i;\theta)\),是一个类条件概率密度函数，只不过是因为这里用的是统一的模型，所以才可以将\(w_i\)省略。<br>　　需要重申一下，想要得到上面这个公式，是做了几个基本的假设的，<strong>第一</strong>：假设\(M\)个类别的数据子集的<strong>概率密度函数形式</strong>一样，只是参数的取值不同；<strong>第二</strong>：假设类别\(i\)中的数据和类别\(j\)中的数据是<strong>相互独立抽样</strong>的，即类别\(j\)的参数仅仅根据类别\(j\)的数据就可以估计出来，类别\(i\)的数据并不能影响类别\(j\)的参数估计，反之亦然；<strong>第三</strong>：每个类别内的样本之间具有<strong>统计独立性</strong>，即每个类别内的样本之间是独立同分布 (\(iid\)) 的。<br>　　此时，就可以使用极大似然估计(Maximum Likelihood，ML)来估计参数\(\theta\)了。如下：<br>$$\hat{\theta}_{ML}=\mathop{argmax}_\limits{\theta} \prod_{k=1}^m p(x^{(k)};\theta)$$<br>　　可以使用\(p(y^{(i)}|x^{(i)};\theta)\)替换\(p(x^{(k)};\theta)\),即对于给定样本\(x\)和参数\(\theta\),其类别为\(y\)的概率。得到：<br>$$\hat{\theta}_{ML}=\mathop{argmax}_\limits{\theta} \prod_{i=1}^m p(y^{(i)}|x^{(i)};\theta)　　　　　(1)$$<br>　　根据频率学派的观点，参数\(\theta\)不是随机的，而是固定的，\(\theta\)是一个固定的未知值或向量，我们可以通过极大似然估计来估计\(\theta\)的值。<br>　　极大似然估计有两个非常重要的性质：<strong>渐进无偏</strong> 和 <strong>渐进一致性</strong>，有了这两个性质，使得极大似然估计的成为了非常简单而且实用的参数估计方法。这里假设\(θ_0\)是密度函数\(p(x;θ)\)中未知参数的准确值。<br>　　极大似然估计是渐进无偏的，即：<br>$$\lim_{N \to \infty}E[\hat{\theta}_{ML}]=\theta_0$$<br>　　也就是说，这里认为估计值\(\hat{\theta}_{ML}\)本身是一个随机变量（因为不同的样本集合X会得到不同的\(\hat{\theta}_{ML}\)），那么其均值就是未知参数的真实值，这就是渐进无偏。<br>　　极大似然估计是渐进一致的，即：<br>$$\lim_{N \to \infty}prob{  \lVert \hat{\theta}_{ML}-  \theta_0 \rVert \leqslant \epsilon} = 1$$<br>　　这个公式还可以表示为：<br>$$\lim_{N \to \infty} E \lVert \hat{\theta}_{ML}-  \theta_0 \rVert^2 = 0$$<br>　　对于一个估计器而言，一致性是非常重要的，因为存在满足无偏性，但是不满足一致性的情况，比如，\(\hat{\theta}_{ML}\)在\(θ_0\)周围震荡。如果不满足一致性，那么就会出现很大的方差。<br>　　注意：以上两个性质，都是在渐进的前提下\(N \to \infty\)才能讨论的，即只有N足够大时，上面两个性质才能成立。</p>
<h2 id="最大后验估计"><a href="#最大后验估计" class="headerlink" title="最大后验估计"></a>最大后验估计</h2><p>　　在最大似然估计（ML）中，将\(θ\)看做是固定的未知参数，说的通俗一点，最大似然估计是\(θ\)的函数，其求解过程就是找到使得最大似然函数最大的那个参数\(θ\)。<br>　　从最大后验估计开始，将参数\(θ\)看成一个随机变量，并在已知样本集\(X=\{x^{(1)},x^{(2)},…,x^{(m)}\}\)的条件下，估计参数\(θ\)。<br>　　这里一定要注意，在最大似然估计中，参数\(θ\)是一个定值，只是这个值未知，最大似然函数是\(θ\)的函数，这里\(θ\)是没有概率意义的,代表的是频率学派。但还还有另外一个学派，即贝叶斯学派，和频率学派的观点不同，他们认为\(\theta\)是一个随机变量，服从某个先验分布，即\(\theta \sim p(\theta)\)。因此在最大后验估计中，\(θ\)是有概率意义的，\(θ\)有自己的分布，而这个分布函数，需要通过已有的样本集合\(X\)得到，即最大后验估计需要计算的是\(p(θ|X)\)<br>　　根据贝叶斯理论：<br>$$p(\theta|X)=\frac{p(\theta)p(X|\theta)}{p(X)}$$<br>　　这就是参数\(θ\)关于已有数据集合\(X\)的后验概率，要使得这个后验概率最大，和极大似然估计一样，这里需要对后验概率函数求导。由于分母中的\(p(X)\)相对于\(θ\)是独立的，所以可以直接忽略掉\(p(X)\)。<br>$$\hat{\theta}_{MAP}=arg\max_{\theta}p(\theta|X)=arg\max_{\theta}p(\theta)p(X|\theta)$$<br>　　注意：<strong>这里\(p(X|θ)\)和极大似然估计中的似然函数\(p(X;θ)\)是一样的，只是记法不一样</strong>。\(MAP\)和\(ML\)的区别是：MAP是在ML的基础上加上了p(θ)。<br>　　这里需要说明，虽然从公式上来看 \(MAP = ML \times p(θ)\)，但是这两种算法有本质的区别，\(ML\)将\(θ\)视为一个确定未知的值，而\(MAP\)则将\(θ\)视为一个随机变量。<br>　　在\(MAP\)中，\(p(θ)\)称为\(θ\)的先验。<strong>\(MAP\)和\(ML\)的关系是</strong>，假设\(θ\)服从均匀分布，即对于所有\(θ\)取值，p(θ)都是同一个常量，则\(MAP\)和\(ML\)会得到相同的结果。当然了，如果\(p(θ)\)的方差非常的小，也就是说，\(p(θ)\)是近似均匀分布的话，\(MAP\)和\(ML\)的结果自然也会非常的相似。</p>
<h2 id="贝叶斯估计-1"><a href="#贝叶斯估计-1" class="headerlink" title="贝叶斯估计"></a>贝叶斯估计</h2><p>　　<strong>以下所有的概率分布表述方式均为贝叶斯学派的表述方式</strong>。另外，在标题上加上全。</p>
<h3 id="贝叶斯估计核心问题"><a href="#贝叶斯估计核心问题" class="headerlink" title="贝叶斯估计核心问题"></a>贝叶斯估计核心问题</h3><p>　　为了防止标号混淆，这里定义已有的样本集合为\(S\)，而不是之前的\(X\)。样本集合\(S\)中的样本都是从一个 <strong>固定但是未知</strong> 的<strong>概率密度函数\(p(x)\)</strong>中独立抽取出来的，我们的目标是根据这些样本估计\(x\)的概率分布，记为\(p(x|S)\)，并且<strong>使得\(p(x|S)\)尽量的接近\(p(x)\)</strong>，这就是贝叶斯估计的核心问题。</p>
<h3 id="贝叶斯估计第一个重要元素"><a href="#贝叶斯估计第一个重要元素" class="headerlink" title="贝叶斯估计第一个重要元素"></a>贝叶斯估计第一个重要元素</h3><p>　　虽然\(p(x)\)是未知的，但是前面提到过，一个密度分布的两个要素为：<strong>形式</strong>和<strong>参数</strong>，我们可以假设\(p(x)\)的形式已知，但是参数θ的取值未知。<strong>这里就有了贝叶斯估计的第一个重要元素\(p(x|θ)\)</strong>，这是一个条件概率密度函数，准确的说，是一个类条件概率密度函数（具体原因参见本文前面关于极大似然估计的说明）。强调一下：\(p(x|θ)\)的形式是已知的，只是参数\(θ\)的取值未知。由于这里的\(x\)可以看成一个测试样本，所以这个<strong>条件密度函数</strong>，从本质上讲，<strong>是\(θ\)在点\(x\)处的似然估计</strong>。</p>
<h3 id="贝叶斯估计第二个重要元素"><a href="#贝叶斯估计第二个重要元素" class="headerlink" title="贝叶斯估计第二个重要元素"></a>贝叶斯估计第二个重要元素</h3><p>　　由于参数\(θ\)的取值未知，且，我们将\(θ\)看成一个随机变量，那么，在观察到具体的训练样本之前，关于\(θ\)的全部知识，可以用一个<strong>先验概率密度函数\(p(θ)\)</strong>来表示，对于训练样本的观察，使得我们能够把这个先验概率密度转化成为后验概率密度函数\(p(θ|S)\)，根据后验概率密度相关的论述知道，我们希望\(p(θ|S)\)在\(θ\)的真实值附近有非常显著的尖峰。这里的这个<strong>后验概率密度</strong>，<strong>就是贝叶斯估计的第二个重要元素</strong>。</p>
<h3 id="联系"><a href="#联系" class="headerlink" title="联系"></a>联系</h3><p>　　现在，将<strong>贝叶斯估计核心问题\(p(x|S)\)</strong>，和贝叶斯估计的<strong>两个重要元素</strong>：\(p(x|θ)、p(θ|S)\)联系起来：<br>$$p(x|S)=\int p(x,\theta|S) d\theta=\int p(x|\theta,S)p(\theta|S)d\theta　　　(1)$$</p>
<hr>
<p>　　以上等式都是基于训练集\(S\)的，我们可以暂且忽略S来理解。<br>　　<strong>第一个等式理解</strong>：联合概率密度和边缘概率密度关系。更一般的即：针对连续型随机变量\((X,Y)\),设它的联合概率密度为\(f(x,y)\)，则：<br>$$f_X(x)=\int_{ - \infty }^{ + \infty } f(x,y)dy$$<br>　　上式称为随机变量\((X,Y)\)关于X的边缘概率密度。同理可以求Y的边缘概率密度。我们将\(\theta\)替换y就可以得到，\(p(x)=\int p(x,\theta) d\theta\)。<br>　　<strong>第二个等式理解</strong>：条件概率和联合概率关系，即：<br>$$p(x|y)=\frac{p(x,y)}{p(y)}$$<br>　　同样，可以将\(\theta\)替换y，并整理就可以得到，\(p(x,\theta)=p(x|\theta)p(\theta)\)。<br>　　对每个式子加上\(|S\)即可得到上述公式，注意\(p(x|\theta,S)\)就是指在\(\theta和S\)条件下,相当于\(p(x|\theta)\)和\(p(x|S)\),也即\(p(x|(\theta,S))\)。</p>
<hr>
<p>　　上面式子中，\(x\)是测试样本，\(S\)是训练集。\(x\)和\(S\)的选取是独立进行的，因此，\(p(x|θ,S)\)可以写成\(p(x|θ)\)。所以，贝叶斯估计的核心问题就是下面这个公式：<br>$$p(x|S)=\int p(x|\theta)p(\theta|S)d\theta　　　　　(2)$$<br>　　下面这句话一定要理解：这里<strong>\(p(x|θ)\)是\(θ\)关于测试样本\(x\)这一个点的似然估计</strong>，而<strong>\(p(θ|S)\)则是\(θ\)在已有样本集合上的后验概率</strong>。<br>　　上面这个式子就是贝叶斯估计最核心的公式，它把<strong>类条件概率密度\(p(x|S)\)</strong> （这里一定要理解为什么是类条件概率密度，其实这个的准确写法可以是\(p(x|S_i)\),或者 \(p(x|w_i,S_i)\)，具体原因参见本文前面关于极大似然估计的部分）和未知参数向量\(θ\)的<strong>后验概率密度</strong>\(p(θ|S)\)联系在了一起。如果后验概率密度\(p(θ|S)\)在某一个值\(\hat{θ}\)附近形成显著的尖峰，那么就有\(p(x|S)≈p(x|\hat{θ})\)，就是说，可以用估计值\(\hat{θ}\)近似代替真实值所得的结果。<br>　　因此，贝叶斯估计实际上解决的根本问题不是去估计参数(\(P(\theta|S)\)),而是求新的测试样本可能出现的概率(\(P(x|S)\))，也就是样本的类条件概率密度函数的值，或者说是要学习到类条件概率密度函数。只不过我们需要利用到后验概率密度\(P(\theta|S)\)来求类条件概率\(P(x|S)\)，并且会在后验概率密度\(p(θ|S)\)形成尖峰处(n不断增大，\(\sigma\)越来越小，即估计的不确定性不断降低)，近似取得该类条件概率\(P(x|S)\)的估计值。<br>　　这也是为什么本文一开始并没有直接讲贝叶斯估计，而是先说明极大似然估计和最大后验估计的原因。其中，\(p(x|θ)\)和极大似然估计中的似然函数\(p(x;θ)\)其实是一样的，而后验概率\(p(θ|S)\)为：<br>$$p(\theta|S)=\frac{p(S|\theta)p(\theta)}{p(S)}=\frac{p(S|\theta)p(\theta)}{\int p(S|\theta)p(\theta)d\theta}　　　(3)\\\\<br>p(S|\theta)=\prod_{k=1}^m p(x^{(k)}|\theta) \\\\<br>其中，p(S|θ)和极大似然估计中的p(S;θ)一样，\\\\<br>同理，p(x^{(k)}|\theta)和极大似然估计中的p(x^{(k)};\theta)是一样的。\\\\<br>不同点在于，关于\theta是未知的固定值还是随机变量的区别。<br>$$<br>　　贝叶斯公式的分子，相当于条件概率里的\(P(S,\theta)\)联合概率，而分母是先通过条件概率\(P(S|\theta)p(\theta)\)得到联合概率\(P(S,\theta)\),然后通过联合概率求得边缘概率\(p(S)\)。<br>　　具体的，给定一个训练集\(S={\{(x^{(i)},y^{(i)})\}}_{i=1}^m\)，参数\(\theta\)的后验概率分布如下：<br>$$p(\theta|S)=\frac{p(S|\theta)p(\theta)}{p(S)}<br>=\frac{\left(\prod_{i=1}^m p(y^{(i)}|x^{(i)},\theta)\right)p(\theta)}{\int_{\theta}\left(\prod_{i=1}^m p(y^{(i)}|x^{(i)},\theta)p(\theta)\right)d\theta}　　 \\\\<br>其中，p(y^{(i)}|x^{(i)},\theta)=p(y^{(i)}|(x^{(i)},\theta)),即在x^{(i)},\theta已知情况下,样本类别为y^{(i)}的概率。<br>$$　<br>　　注意到前面似然估计中我们用的是，\(p(y^{(i)}|x^{(i)};\theta)\)的x和与\(\theta\)以分号隔开，表示\(\theta\)是一个具体的未知值。上述贝叶斯估计用的是\(p(y^{(i)}|x^{(i)},\theta)\)的x和\(\theta\)之间以逗号隔开，表示\(\theta\)是随机变量。<br>　　公式2中的\(p(y^{(i)}|x^{(i)},\theta)\)形式来源于所选模型，比如使用贝叶斯Logistic回归模型(bayesian logistic model,BLR),那么有：<br>$$p(y^{(i)}|x^{(i)},\theta)=h_\theta(x^{(i)})^{y^{(i)}}(1-h_\theta(x^{(i)}))^{1-y^{(i)}}\\\\<br>其中,h_\theta(x^{(i)})=\frac{1}{1+exp(-\theta^Tx^{(i)})}$$<br>　　在对新样本进行预测时，根据上述得到的公式(2)，可以使用如下公式计算后验概率：<br>$$p(y|x,S)=\int_{\theta} p(y|x,\theta)p(\theta|S)d\theta$$<br>　　上式中的\(p(\theta|S)\)后验概率使用贝叶斯公式(3)计算。<br>　　如果预测的是在给定x情况下y的值，则使用下列公式：<br>$$E[y|x,S]=\int_y yp(y|x,S)dy$$</p>
<h2 id="最大后验估计和贝叶斯估计的联系"><a href="#最大后验估计和贝叶斯估计的联系" class="headerlink" title="最大后验估计和贝叶斯估计的联系"></a>最大后验估计和贝叶斯估计的联系</h2><p>　　上述公式2称作全贝叶斯预测(fully Bayesian prediction)。可以看到，贝叶斯估计需要在在\(\theta\)上计算后验概率\(p(\theta|S)\)。不幸的是计算该后验分布非常困难，因为根据(3)，需要在\(\theta\)上求积分，即对所有的参数求积分，而\(\theta\)经常是高维度的。<br>　　因此实践中，经常对后验分布进行近似求解。一个通常的近似方法是对公式2中的后验分布中的\(\theta\)使用点估计来代替。即：<br>$$\hat{\theta}_{MAP}=arg\max_{\theta}p(\theta|S)=arg\max_{\theta}p(S|\theta)p(\theta)=arg\max_{\theta}(\prod_{i=1}^m p(y^{(i)}|x^{(i)},\theta))p(\theta)$$<br>　　\(p(y^{(i)}|x^{(i)},\theta)\)是\(\theta\)在样本点\((x^{(i)},y^{(i)})\)的点估计，实际上该公式就是最大后验估计中的公式。即<strong>最大后验估计是贝叶斯估计的一种近似。</strong><br>　　 从以上可以看出，一方面，极大似然估计和最大后验概率都是参数的点估计。在频率学派中，参数固定了，预测值也就固定了。<strong>最大后验概率是贝叶斯学派的一种近似手段</strong>，因为完全贝叶斯估计不一定可行。</p>
<h1 id="贝叶斯估计和正则化"><a href="#贝叶斯估计和正则化" class="headerlink" title="贝叶斯估计和正则化"></a>贝叶斯估计和正则化</h1><p>　　正则化，对于监督学习而言，我们有如下的目标函数：<br>$$\theta^{*}=arg \min_{\theta} \sum_{i} L(y^{(i)},f(x^{(i)};\theta))+\lambda \Omega(\theta)$$<br>　　其中，第一项\(L(y^{(i)},f(x^{(i)};\theta))\)衡量我们的模型对第\(i\)个样本的预测值\(f(x^{(i)};\theta)\)和真实标签\(y^{(i)}\)之间的误差。我们要最小化这一项，根据ERM，即训练误差最小化。但我们不仅要保证训练误差最小，我们更希望模型的泛化误差小，所以加上第二项，也就是对参数\(\theta\)的正则化函数\(\Omega(\theta)\)取约束我们的模型，使其尽量简单，即惩罚复杂的模型，约束要优化的参数。这个正则化函数就是我们常说的\(L_0,L_1,L_2\)范数。这是防止过拟合的重要手段。<br>　　上述方法我们经常使用极大似然估计来最优化。注意此时估计时，是<strong>需要加上正则化项</strong>才能防止过拟合的。<br>　　那么前面我们提到的贝叶斯估计，又是如何做到防止过拟合呢? 我们知道全贝叶斯估计很难计算，因此使用最大后验估计来近似。这里我们直接讨论最大后验估计。<strong>我们要从最大后验估计推导出上述目标函数的形式，即加了正则化项的极大似然估计。</strong><br>　　$$\hat{\theta}_{MAP}=arg\max_{\theta}p(\theta|S)=arg\max_{\theta}p(S|\theta)p(\theta)=arg\max_{\theta}(\prod_{i=1}^m p(y^{(i)}|x^{(i)},\theta))p(\theta)$$<br>　　　当\(\theta\)的先验概率分布满足均值为0的正态分布的时候，对于\(\theta\)的每一个分量\(\theta_j\)，都有：<br>$$p(\theta_j)=\mathcal{N}(\theta_j|0,\sigma^2)=\frac{1}{\sqrt{2\pi\sigma^2}}e^{-\frac{\theta_j^2}{2\sigma^2}}$$<br>　　此时，根据\(\theta_i\)之间独立同分布，有，\(p(\theta) = \prod_{j=1}^n p(\theta_i)\)</p>
<p>　　对最大后验估计两边取对数有：<br>$$log(\hat{\theta}_{MAP})=arg\max_{\theta} \sum_{i=1}^m \left(log \ p(y^{(i)}|x^{(i)},\theta)\right)+log \ p(\theta) \\\\<br>=arg\min_{\theta} -\sum_{i=1}^m \left(log \ p(y^{(i)}|x^{(i)},\theta)\right)-log \ p(\theta)$$<br>　<br>　　代入公式展开有：<br>$$\theta^{*}=log(\hat{\theta}_{MAP})=arg\min_{\theta} -\sum_{i=1}^m \left(log \ p(y^{(i)}|x^{(i)},\theta)\right)-log \ p(\theta)\\\\<br>=arg\min_{\theta} -\sum_{i=1}^m log \ p(y^{(i)}|x^{(i)},\theta)-\sum_{j=1}^n log \ p(\theta) \\\\<br>= arg\min_{\theta} -\sum_{i=1}^m log \ p(y^{(i)}|x^{(i)},\theta)+\frac{1}{2\sigma^2}\sum_{j=1}^n \theta_j^2+nlog(\sqrt{2\pi\sigma^2}) \\\\<br>= arg\min_{\theta} -\sum_{i=1}^m log \ p(y^{(i)}|x^{(i)},\theta)+\lambda\sum_{j=1}^n \theta_j^2 \\\\<br>(\lambda=\frac{1}{\sigma^2},常数可以去掉)<br>$$<br>　　对比下式：<br>$$\theta^{*}=arg \min_{\theta} \sum_{i} L(y^{(i)},f(x^{(i)};\theta))+\lambda \Omega(\theta)$$<br>　　可以看到，似然函数部分对应于损失函数(训练误差)，而先验概率部分对应于正则化项。此处是\(L_2\)正则化，等价于参数\(\theta\)的先验概率分布满足正态分布。<br>　　最终的公式就是岭回归计算公式。与上面最大似然估计推导出的最小二乘相比，最大后验估计就是在最大似然估计公式乘以高斯先验，这就理解了L2正则就是加入高斯先验知识。<br>　　拓展，当先验概率分布满足均值为0的拉普拉斯分布的时候，即:<br>$$p(\theta_j)=\mathcal{Lplace}(\theta_j|0,b)=\frac{1}{2b}e^{-\frac{|\theta_j|}{b}}$$<br>　　此时根据最大后验估计可以推导出：<br>$$\theta^{*}= arg\min_{\theta} -\sum_{i=1}^m log \ p(y^{(i)}|x^{(i)},\theta)+\lambda\sum_{j=1}^n |\theta_j| \\\\<br>(\lambda=\frac{1}{b})<br>$$<br>　　最终的公式就是Lasso计算公式。与上面最大似然估计推导出的最小二乘相比，最大后验估计就是在最大似然估计公式乘以拉普拉斯先验，这里就理解了L1正则就是加入拉普拉斯先验知识。<br>　　我们之前学习的线性回归的代价函数使用的最小二乘法，实际上是在最大似然法基础上加上残差的正态分部假设得出的。同样如果假设残差是拉普拉斯分布，得出的就是最小一乘。<br>　　总结一句，<strong>从贝叶斯估计的角度来看,正则化项对应于模型的先验概率。</strong></p>
<h1 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h1><p><a href="http://open.163.com/special/opencourse/machinelearning.html" target="_blank" rel="external">斯坦福大学机器学习视频教程</a><br><a href="http://blog.csdn.net/daunxx/article/details/51725086" target="_blank" rel="external">贝叶斯线性回归</a><br><a href="http://www.jianshu.com/p/a47c46153326" target="_blank" rel="external">回归系列之L1和L2正则化</a><br><a href="http://blog.csdn.net/zhuxiaodong030/article/details/54408786" target="_blank" rel="external">从贝叶斯角度深入理解正则化</a></p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;　　本部分将继续讨论学习理论的相关知识。学习理论内容包括：模型/特征选择(model/feature selection)、贝叶斯统计和正则化(Bayesian statistics and Regularization)。本文重点在于贝叶斯统计和正则化。将重点理解这句话，从贝叶斯估计的角度来看,正则化项对应于模型的先验概率。&lt;br&gt;
    
    </summary>
    
      <category term="机器学习" scheme="xtf615.com/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="机器学习" scheme="xtf615.com/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="正则化" scheme="xtf615.com/tags/%E6%AD%A3%E5%88%99%E5%8C%96/"/>
    
      <category term="特征选择" scheme="xtf615.com/tags/%E7%89%B9%E5%BE%81%E9%80%89%E6%8B%A9/"/>
    
      <category term="贝叶斯估计" scheme="xtf615.com/tags/%E8%B4%9D%E5%8F%B6%E6%96%AF%E4%BC%B0%E8%AE%A1/"/>
    
      <category term="概率论" scheme="xtf615.com/tags/%E6%A6%82%E7%8E%87%E8%AE%BA/"/>
    
      <category term="极大似然估计" scheme="xtf615.com/tags/%E6%9E%81%E5%A4%A7%E4%BC%BC%E7%84%B6%E4%BC%B0%E8%AE%A1/"/>
    
      <category term="最大后验估计" scheme="xtf615.com/tags/%E6%9C%80%E5%A4%A7%E5%90%8E%E9%AA%8C%E4%BC%B0%E8%AE%A1/"/>
    
  </entry>
  
  <entry>
    <title>Week 2 For Leetcode</title>
    <link href="xtf615.com/2017/09/24/Week-2-For-Leetcode/"/>
    <id>xtf615.com/2017/09/24/Week-2-For-Leetcode/</id>
    <published>2017-09-24T01:55:32.000Z</published>
    <updated>2017-09-24T13:27:05.993Z</updated>
    
    <content type="html"><![CDATA[<p>　本文是Leetcode刷题计划第二周的总结。<br><a id="more"></a></p>
<h1 id="Maximum-Subarray-53"><a href="#Maximum-Subarray-53" class="headerlink" title="Maximum Subarray(#53)"></a>Maximum Subarray(#53)</h1><p>　　问：找到一个最大和的连续子数组。<br>　　分析：这道题有O(n)的解法，也有O(nlogn)的解法。先看O(n)动态规划的方法。我们想找到一个子数组A[i..j]使得sum(A[i..j])最大化。分解原问题的规模，如果只有1个数的话，那么直接返回。如果2个数的话，我们已经知道了1个数情况下的最大和，那么2个数最大和肯定是max(A[0], A[1], A[0]+A[1]), A[0]对应的是1个数的最大和，上式可改成max(A[0], max(A[1], A[0]+A[1])), 也就是说如果A[0]&gt;0的话，显然最大和就是A[0]+A[1]。<br>　　拓展来说，已知A[1..j-1]数组的最大子数组，那么求A[1..j]的最大子数组，实际上只有两种情况。</p>
<ul>
<li>A[1..j]的最大子数组就是A[1..j-1]的最大子数组，即最大子数组不包含A[j]该元素。</li>
<li>A[1..j]的最大子数组包含A[j]这个元素，即最大子数组此时以A[j]作为末端元素，我们需要找到起始端点A[i],其中0&lt;=i&lt;=j。<br>　　凭直觉而言，对于上述第二种情况，我们可以从A[j]开始往左遍历一直到起始元素，不断求和和比较，找到以A[j]结尾最大的子数组。但是这样会花费额外的时间。实际上我们发现这里面存在大量重复的操作，例如以A[j+1]结尾的子数组势必会和以A[j]结尾的子数组，存在重复操作，都要往左一直遍历到起点。因此我们实际上可以定义一个变量MaxEndHere来记录以A[j]结尾的最大子数组。则MaxEndHere_{j+1} = Max (MaxEndHere_{j}+A[j+1], A[j+1]);<br>　　这样再定义一个maxSoFar, 则maxSoFar = Max(maxSoFar, maxEndHere);maxSoFar对应第一种情况，maxEndHere对应第二种情况。<figure class="highlight c++"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">int</span> <span class="title">maxSubArray</span><span class="params">(<span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt;&amp; nums)</span> </span>&#123;</div><div class="line">    <span class="keyword">if</span>(nums.empty()) <span class="keyword">return</span> <span class="number">0</span>;</div><div class="line">    </div><div class="line">    <span class="keyword">int</span> maxSoFar = nums[<span class="number">0</span>];<span class="comment">//记录[1..i]当前最大子数组</span></div><div class="line">    <span class="keyword">int</span> maxEndHere = nums[<span class="number">0</span>];<span class="comment">//记录以i为结尾端点最大的子数组</span></div><div class="line">    <span class="keyword">for</span>(<span class="keyword">int</span> i = <span class="number">1</span>; i &lt; nums.size();++i)&#123;</div><div class="line">        maxEndHere = max(maxEndHere + nums[i], nums[i]);<span class="comment">//实际上maxEndHere&gt;0即可</span></div><div class="line">        maxSoFar = max(maxSoFar, maxEndHere);<span class="comment">//[1..i]最大子数组，要么是[1..i-1]的最大子数组maxSofar, 要么是包含i这个端点，往左数到开始位置最大连续和maxEndHere</span></div><div class="line">    &#125;</div><div class="line">    <span class="keyword">return</span> maxSoFar;</div><div class="line">            </div><div class="line">&#125;</div></pre></td></tr></table></figure>
</li>
</ul>
<p>　　第二种方法是分治法，最大子数组只可能是如下几个情况：</p>
<ul>
<li>A[i..j]在A的左半边数组出现,其中 low &lt;= i &lt;= j &lt;= mid</li>
<li>A[i..j]在A的右半边数组出现,其中, mid &lt; i &lt;= j &lt;= high</li>
<li>A[i..j]跨越左右半边，即 low &lt;= i &lt;= mid &lt; j &lt;= high (j严格大于mid)<br>　　前两种情况对应原问题的子问题，使用递归求解。第三种情况是合并问题，实际上就是从mid开始，以mid为结尾端点往左求最大子数组，以mid为开始端点往右求最大子数组，方法同第一种方法里求maxEndHere最原始的方法。<figure class="highlight c++"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">int</span> <span class="title">maxSubArray</span><span class="params">(<span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt;&amp; nums)</span> </span>&#123;</div><div class="line">    <span class="keyword">return</span> dq_maxSubArray(nums, <span class="number">0</span>, nums.size() - <span class="number">1</span>);</div><div class="line">&#125;</div><div class="line"></div><div class="line"><span class="function"><span class="keyword">int</span> <span class="title">dq_maxSubArray</span><span class="params">(<span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt;&amp; nums, <span class="keyword">int</span> low, <span class="keyword">int</span> high)</span></span>&#123;</div><div class="line">    <span class="keyword">if</span>(low == high) <span class="keyword">return</span> nums[low];</div><div class="line">    <span class="keyword">int</span> mid = low + (high-low)/<span class="number">2</span>;</div><div class="line">    <span class="keyword">int</span> left_max = dq_maxSubArray(nums, low, mid);</div><div class="line">    <span class="keyword">int</span> right_max = dq_maxSubArray(nums, mid+<span class="number">1</span>, high);</div><div class="line">    <span class="keyword">int</span> cross_max = findCrossMaxSubArray(nums, low, mid, high);</div><div class="line">    <span class="keyword">return</span> max(max(left_max,right_max), cross_max);</div><div class="line">&#125;</div><div class="line"></div><div class="line"><span class="function"><span class="keyword">int</span> <span class="title">findCrossMaxSubArray</span><span class="params">(<span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt;&amp; nums, <span class="keyword">int</span> low, <span class="keyword">int</span> mid, <span class="keyword">int</span> high)</span></span>&#123;<span class="comment">// low &lt;= i &lt;= mid &lt; j &lt;= high</span></div><div class="line">    <span class="keyword">int</span> left_sum = INT_MIN, right_sum = INT_MIN, temp_sum = <span class="number">0</span>;</div><div class="line">    <span class="comment">//left</span></div><div class="line">    <span class="keyword">int</span> j = mid;</div><div class="line">    <span class="keyword">while</span>(j &gt;= low)&#123;</div><div class="line">        temp_sum += nums[j];</div><div class="line">        left_sum = max(left_sum, temp_sum);</div><div class="line">        --j;</div><div class="line">    &#125;</div><div class="line">    temp_sum = <span class="number">0</span>;</div><div class="line">    j = mid+<span class="number">1</span>;</div><div class="line">    <span class="keyword">while</span>(j &lt;= high)&#123;</div><div class="line">        temp_sum += nums[j];</div><div class="line">        right_sum = max(right_sum, temp_sum);</div><div class="line">        ++j;</div><div class="line">    &#125;</div><div class="line">    <span class="keyword">return</span> left_sum+right_sum;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
</li>
</ul>
<h1 id="Best-Time-To-Buy-and-Sell-Stock-121"><a href="#Best-Time-To-Buy-and-Sell-Stock-121" class="headerlink" title="Best Time To Buy and Sell Stock(#121)"></a>Best Time To Buy and Sell Stock(#121)</h1><p>　　问题：给一个股价序列，要求选择买入价和卖出价，使得盈利最大。<br>　　分析：可以借助上面求连续和最大的子数组的方法。对于给定的股价数组\([a_0,a_1,a_2,…,a_n]\),构造一个数组\(a_1-a_0, a_2-a_1, a_3-a_2,…,a_n-a_{n-1}\), 原数组任意两个数相减，可以转化成新数组求该范围内的连续和。<br>　　例如:对于原数组\([a_0,a_1,a_2,a_3]\),构造新数组\([a_1-a_0,a_2-a_1,a_3-a_2]\)，则比如\(a_2-a_0=(a_2-a_1) + (a_1-a_0)\)等等。<br><figure class="highlight c++"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">int</span> <span class="title">maxProfit</span><span class="params">(<span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt;&amp; prices)</span> </span>&#123;</div><div class="line">    <span class="keyword">if</span>(prices.empty()) <span class="keyword">return</span> <span class="number">0</span>;</div><div class="line">    <span class="keyword">int</span> pre = prices[<span class="number">0</span>];<span class="comment">//前一个数</span></div><div class="line">    <span class="keyword">for</span>(<span class="keyword">int</span> i = <span class="number">1</span>; i &lt; prices.size(); ++i)&#123;</div><div class="line">        <span class="keyword">int</span> t = prices[i] - pre;<span class="comment">//先求差</span></div><div class="line">        pre = prices[i];<span class="comment">//保存当前</span></div><div class="line">        prices[i] = t;</div><div class="line">    &#125;</div><div class="line">    <span class="keyword">int</span> m =  maxSubArray(prices);</div><div class="line">    <span class="keyword">return</span> m &gt; <span class="number">0</span>? m:<span class="number">0</span>;</div><div class="line">&#125;</div><div class="line"></div><div class="line"><span class="function"><span class="keyword">int</span> <span class="title">maxSubArray</span><span class="params">(<span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt;&amp; nums)</span> </span>&#123;<span class="comment">//[1..nums.size() )</span></div><div class="line">    <span class="keyword">int</span> maxSoFar = nums[<span class="number">1</span>], maxEndHere = nums[<span class="number">1</span>];</div><div class="line">    <span class="keyword">for</span>(<span class="keyword">int</span> i = <span class="number">2</span>; i &lt; nums.size(); ++i)&#123;</div><div class="line">        maxEndHere = max(maxEndHere + nums[i], nums[i]);</div><div class="line">        maxSoFar = max(maxSoFar, maxEndHere);</div><div class="line">    &#125;</div><div class="line">    <span class="keyword">return</span> maxSoFar;</div><div class="line">&#125;</div></pre></td></tr></table></figure></p>
<p>　　还有另一种dp的方法，考虑数组A[1…j]最大差值和A[1..j-1]的关系，显然A[1..j]最大差值可以分成两种情况：</p>
<ul>
<li>不在A[j]处卖出，则A[1..j]的最大差值就是A[1..j-1]数组的最大差值。</li>
<li>在A[j]处卖出，这样就需要找到A[1..j]中的最小值。此时最大差值为A[j]-min.<br>　　代码如下：<figure class="highlight c++"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">int</span> <span class="title">maxProfit</span><span class="params">(<span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt; &amp;prices)</span></span>&#123;</div><div class="line">    <span class="keyword">int</span> max_profit = <span class="number">0</span>, min_price = INT_MAX;</div><div class="line">    <span class="keyword">for</span>(<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; prices.size(); ++i)&#123;</div><div class="line">        min_price = min(min_price, prices[i]);</div><div class="line">        max_profit = max(max_profit, prices[i] - min_price);</div><div class="line">    &#125;</div><div class="line">    <span class="keyword">return</span> max_profit;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
</li>
</ul>
<h1 id="Minimum-Absolute-Difference-in-BST-530"><a href="#Minimum-Absolute-Difference-in-BST-530" class="headerlink" title="Minimum Absolute Difference in BST(#530)"></a>Minimum Absolute Difference in BST(#530)</h1><p>　　问题:给定一个二叉搜索树，找到任意两个差值的绝对值最小的节点，并返回最小值。<br>　　分析：假定给的是一个数组，并且已经从小到大排序。显然这个数组差值最小的两个数肯定在所有相邻数对中产生，即\(min\{|A[i+1]-A[i]|\}\)。同样，由于给定的是排序二叉树，如果能将该二叉树排序，则就可以得出解。但是实际上没必要排完序后，再遍历。可以采用中序遍历的方法，使用pre变量保存前一个值，定义全局min_abs保存最小差值。每次遍历当前点时，都比较min_abs和root-&gt;val - pre的大小即可。<br><figure class="highlight c++"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">int</span> min_dif = INT_MAX;</div><div class="line"><span class="keyword">int</span> prev = <span class="number">-1</span>; <span class="comment">//记录前一个值，按从小到大排列时的前一个值</span></div><div class="line"><span class="function"><span class="keyword">int</span> <span class="title">getMinimumDifference</span><span class="params">(TreeNode* root)</span> </span>&#123;</div><div class="line">    <span class="keyword">if</span>(root == <span class="literal">NULL</span>) <span class="keyword">return</span> min_dif;</div><div class="line">    getMinimumDifference(root-&gt;left);<span class="comment">//遍历左边</span></div><div class="line">    <span class="keyword">if</span>(prev != <span class="number">-1</span>)&#123;<span class="comment">//第一次到最左下角的点时候不要比较</span></div><div class="line">        min_dif = min(min_dif, root-&gt;val - prev);</div><div class="line">    &#125;</div><div class="line">    prev = root-&gt;val;<span class="comment">//赋值成当前点</span></div><div class="line">    getMinimumDifference(root-&gt;right);</div><div class="line">    <span class="keyword">return</span> min_dif;</div><div class="line">&#125;</div></pre></td></tr></table></figure></p>
<h1 id="First-Unique-Character-in-a-String-387"><a href="#First-Unique-Character-in-a-String-387" class="headerlink" title="First Unique Character in a String(#387)"></a>First Unique Character in a String(#387)</h1><p>　　问:找到字符串第一个unique的字符。<br>　　分析：使用hash表先统计每个字符的个数。然后遍历一遍原字符串，看哪个字符优先只出现一次，则直接返回结果。<br><figure class="highlight c++"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">int</span> <span class="title">firstUniqChar</span><span class="params">(<span class="built_in">string</span> s)</span> </span>&#123;</div><div class="line">    <span class="built_in">unordered_map</span>&lt;<span class="keyword">char</span>,<span class="keyword">int</span>&gt; <span class="built_in">map</span>;</div><div class="line">    <span class="keyword">for</span>(<span class="keyword">auto</span> &amp;c : s) &#123;</div><div class="line">        m[c]++;</div><div class="line">    &#125;</div><div class="line">    <span class="keyword">for</span>(<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; s.size(); i++) &#123;</div><div class="line">        <span class="keyword">if</span>(m[s[i]] == <span class="number">1</span>) <span class="keyword">return</span> i;</div><div class="line">    &#125;</div><div class="line">    <span class="keyword">return</span> <span class="number">-1</span>;</div><div class="line">&#125;</div></pre></td></tr></table></figure></p>
<h1 id="Delete-Node-in-a-Linked-List-237"><a href="#Delete-Node-in-a-Linked-List-237" class="headerlink" title="Delete Node in a Linked List(#237)"></a>Delete Node in a Linked List(#237)</h1><p>　　问：给定一个链表中的一个节点，删除该节点。<br>　　答：正常想法是直接把后面的往前挪一个位置。但是我们无法拿到该节点的前一个节点，因此无法链接到后面。此处采用指针的方式，将后一个节点的地址里的内容直接赋值给当前节点指向的地址单元。“*指针”在赋值符号右侧的时候，会取得指针指向的地址的内容，“*指针”在赋值符号左侧的时候，会指向该指针指向的地址单元。<br><figure class="highlight c++"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">void</span> <span class="title">deleteNode</span><span class="params">(ListNode* node)</span> </span>&#123;</div><div class="line">    ListNode* t = node-&gt;next;</div><div class="line">    *node = *(node-&gt;next);<span class="comment">//node指向node-&gt;next指向的内容</span></div><div class="line">    <span class="keyword">delete</span> t; <span class="comment">//t和node内容完全一样</span></div><div class="line">&#125;</div></pre></td></tr></table></figure></p>
<h1 id="Same-Tree-100"><a href="#Same-Tree-100" class="headerlink" title="Same Tree(#100)"></a>Same Tree(#100)</h1><p>　　比较两棵二叉树是否结构内容一致。<br><figure class="highlight c++"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">bool</span> <span class="title">isSameTree</span><span class="params">(TreeNode* p, TreeNode* q)</span> </span>&#123;</div><div class="line">    <span class="keyword">if</span>(p == <span class="literal">NULL</span> &amp;&amp; q == <span class="literal">NULL</span>) <span class="keyword">return</span> <span class="literal">true</span>;</div><div class="line">    <span class="keyword">if</span>(p == <span class="literal">NULL</span> &amp;&amp; q != <span class="literal">NULL</span>) <span class="keyword">return</span> <span class="literal">false</span>;</div><div class="line">    <span class="keyword">if</span>(p != <span class="literal">NULL</span> &amp;&amp; q == <span class="literal">NULL</span>) <span class="keyword">return</span> <span class="literal">false</span>;</div><div class="line">    <span class="keyword">if</span>(p-&gt;val != q-&gt;val) <span class="keyword">return</span> <span class="literal">false</span>;</div><div class="line">    <span class="keyword">return</span> isSameTree(p-&gt;left,q-&gt;left) &amp;&amp; isSameTree(p-&gt;right, q-&gt;right);</div><div class="line">&#125;</div></pre></td></tr></table></figure></p>
<h1 id="Minimum-Index-Sum-of-Two-Lists-599"><a href="#Minimum-Index-Sum-of-Two-Lists-599" class="headerlink" title="Minimum Index Sum of Two Lists(#599)"></a>Minimum Index Sum of Two Lists(#599)</h1><p>　　问题：给定两个存放字符串的数组，找到某个字符串使得该字符串同时在两个数组出现，并且下标之和最小。<br>　　分析：可以使用hash，先存储list1，hash[list1[i]]=i。然后遍历list2,如果在hash中找到hash[list2[j]]说明该字符串同时存在两个数组中，定义min为当前最小下标，定义sum=hash[list2[j]]+j。如果sum&lt;min,则清空结果数组，放入list2[j],如果sum=min，则不清空数组，放入list2[j]。</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div></pre></td><td class="code"><pre><div class="line"><span class="built_in">vector</span>&lt;<span class="built_in">string</span>&gt; findRestaurant(<span class="built_in">vector</span>&lt;<span class="built_in">string</span>&gt;&amp; list1, <span class="built_in">vector</span>&lt;<span class="built_in">string</span>&gt;&amp; list2) &#123;</div><div class="line">    <span class="built_in">unordered_map</span>&lt;<span class="built_in">string</span>,<span class="keyword">int</span>&gt; <span class="built_in">map</span>;</div><div class="line">    <span class="built_in">vector</span>&lt;<span class="built_in">string</span>&gt; result;</div><div class="line">    <span class="keyword">for</span>(<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; list1.size(); ++i)&#123;</div><div class="line">        <span class="built_in">map</span>[list1[i]] = i;</div><div class="line">    &#125;</div><div class="line">    <span class="keyword">int</span> min = INT_MAX;</div><div class="line">    <span class="keyword">for</span>(<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; list2.size(); ++i)&#123;</div><div class="line">        <span class="keyword">if</span>(<span class="built_in">map</span>.find(list2[i]) != <span class="built_in">map</span>.end())&#123;<span class="comment">//存在相同的才进行考虑</span></div><div class="line">            <span class="keyword">int</span> sum = <span class="built_in">map</span>[list2[i]]+i;</div><div class="line">            <span class="keyword">if</span>(sum &lt; min)&#123;</div><div class="line">                min = sum;</div><div class="line">                result.clear();</div><div class="line">                result.push_back(list2[i]);</div><div class="line">            &#125;<span class="keyword">else</span> <span class="keyword">if</span>(sum == min) &#123;</div><div class="line">                result.push_back(list2[i]);</div><div class="line">            &#125;   </div><div class="line">        &#125;</div><div class="line">    &#125;</div><div class="line">    <span class="keyword">return</span> result;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<h1 id="Image-Smoother-661"><a href="#Image-Smoother-661" class="headerlink" title="Image Smoother(#661)"></a>Image Smoother(#661)</h1><p>　　问题：将图像中某个像素点值设置成该像素点周围9个(加上自身)像素点值得平均值。<br>　　分析：分拆问题，首先需要一个smooth方法来设置某个像素点[i,j]为周围所有9个点像素点平局值。smooth中获取周围9个点，可以构造位移数组[-1,0,1]，[i,j]的行下标i和列下标j的移动范围在位移数组中，两个循环遍历即可，注意要检测边界。最后对原图像二维数组每个位置调用smooth即可。<br><figure class="highlight c++"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">int</span> dir[<span class="number">3</span>] = &#123;<span class="number">0</span>, <span class="number">-1</span>, <span class="number">1</span>&#125;;</div><div class="line">    <span class="comment">/**</span></div><div class="line">    * 将单元格的值替换成相邻9个(包括自己)的单元格的平均值</div><div class="line">    */</div><div class="line">    <span class="built_in">vector</span>&lt;<span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt;&gt; imageSmoother(<span class="built_in">vector</span>&lt;<span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt;&gt;&amp; M) &#123;</div><div class="line">        <span class="built_in">vector</span>&lt;<span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt;&gt; result;</div><div class="line">        <span class="keyword">if</span>(M.empty()) <span class="keyword">return</span> result;</div><div class="line">        <span class="keyword">int</span> row = M.size(), column = M[<span class="number">0</span>].size();</div><div class="line">        result.resize(row, <span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt; (column));<span class="comment">//记住该方法</span></div><div class="line">        <span class="keyword">for</span>(<span class="keyword">int</span> r = <span class="number">0</span>; r &lt; row; ++r)&#123;</div><div class="line">            <span class="keyword">for</span>(<span class="keyword">int</span> c = <span class="number">0</span>; c &lt; column; ++c)&#123;</div><div class="line">                smooth(M, row, column, r, c, result);</div><div class="line">            &#125;</div><div class="line">        &#125;</div><div class="line">        <span class="keyword">return</span> result;</div><div class="line">    &#125;</div><div class="line">    </div><div class="line">    <span class="function"><span class="keyword">void</span> <span class="title">smooth</span><span class="params">(<span class="built_in">vector</span>&lt;<span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt;&gt;&amp; M, <span class="keyword">int</span> row, <span class="keyword">int</span> col, <span class="keyword">int</span> r, <span class="keyword">int</span> c, <span class="built_in">vector</span>&lt;<span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt;&gt;&amp; result)</span></span>&#123;</div><div class="line">        <span class="keyword">int</span> count = <span class="number">0</span>;</div><div class="line">        <span class="keyword">int</span> sum = <span class="number">0</span>;</div><div class="line">        <span class="keyword">for</span>(<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; <span class="number">3</span>; ++i)&#123;</div><div class="line">            <span class="keyword">for</span>(<span class="keyword">int</span> j = <span class="number">0</span>; j &lt; <span class="number">3</span>; ++j)&#123;</div><div class="line">                <span class="keyword">if</span> (is_valid(row, col, r, c, dir[i], dir[j]))&#123;</div><div class="line">                    ++count;</div><div class="line">                    sum += M[r+dir[i]][c+dir[j]];</div><div class="line">                &#125;</div><div class="line">            &#125;</div><div class="line">        &#125;</div><div class="line">        result[r][c] = sum / count;</div><div class="line">    &#125;</div><div class="line">    </div><div class="line">    </div><div class="line">    <span class="function"><span class="keyword">bool</span> <span class="title">is_valid</span><span class="params">(<span class="keyword">int</span> row, <span class="keyword">int</span> col, <span class="keyword">int</span> r, <span class="keyword">int</span> c, <span class="keyword">int</span> r_inc, <span class="keyword">int</span> c_inc)</span></span>&#123;</div><div class="line">        <span class="keyword">return</span> r + r_inc &gt;= <span class="number">0</span> &amp;&amp; r + r_inc &lt; row &amp;&amp; c + c_inc &gt;= <span class="number">0</span> &amp;&amp; c + c_inc &lt; col;</div><div class="line">    &#125;</div></pre></td></tr></table></figure></p>
<h1 id="Ransom-Note-383"><a href="#Ransom-Note-383" class="headerlink" title="Ransom Note(#383)"></a>Ransom Note(#383)</h1><p>　　问题：给定一个note和一个字符串集合magazine，note只能由magazine中的字符构成，并且magazine中字符是有限的。例如canConstruct(“aa”, “ab”)-&gt;false,canConstruct(“aa”, “aab”) -&gt; true。<br>　　分析：先统计magazine每个字符数量，再遍历note，不断扣除，如果不存在或者扣除到0，则返回false。<br><figure class="highlight c++"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">bool</span> <span class="title">canConstruct</span><span class="params">(<span class="built_in">string</span> ransomNote, <span class="built_in">string</span> magazine)</span> </span>&#123;</div><div class="line">    <span class="built_in">unordered_map</span>&lt;<span class="keyword">char</span>,<span class="keyword">int</span>&gt; <span class="built_in">map</span>;</div><div class="line">    <span class="keyword">for</span>(<span class="keyword">char</span> c : magazine)&#123;</div><div class="line">        <span class="keyword">if</span>(<span class="built_in">map</span>.find(c) != <span class="built_in">map</span>.end())&#123;</div><div class="line">            <span class="built_in">map</span>[c] += <span class="number">1</span>;</div><div class="line">        &#125;</div><div class="line">        <span class="keyword">else</span>&#123;</div><div class="line">            <span class="built_in">map</span>[c] = <span class="number">1</span>;</div><div class="line">        &#125;</div><div class="line">    &#125;</div><div class="line">    <span class="keyword">for</span>(<span class="keyword">char</span> c : ransomNote)&#123;</div><div class="line">        <span class="keyword">if</span>(<span class="built_in">map</span>.find(c) == <span class="built_in">map</span>.end() || <span class="built_in">map</span>[c] &lt;= <span class="number">0</span>) <span class="keyword">return</span> <span class="literal">false</span>;</div><div class="line">        <span class="keyword">else</span>&#123;</div><div class="line">            <span class="built_in">map</span>[c] -= <span class="number">1</span>;</div><div class="line">        &#125;</div><div class="line">    &#125;</div><div class="line">    <span class="keyword">return</span> <span class="literal">true</span>;</div><div class="line">&#125;</div></pre></td></tr></table></figure></p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;　本文是Leetcode刷题计划第二周的总结。&lt;br&gt;
    
    </summary>
    
      <category term="Leetcode" scheme="xtf615.com/categories/Leetcode/"/>
    
    
      <category term="基础算法" scheme="xtf615.com/tags/%E5%9F%BA%E7%A1%80%E7%AE%97%E6%B3%95/"/>
    
      <category term="Leetcode" scheme="xtf615.com/tags/Leetcode/"/>
    
  </entry>
  
  <entry>
    <title>Leetcode (Week 1)</title>
    <link href="xtf615.com/2017/09/17/Week-1-For-Leetcode/"/>
    <id>xtf615.com/2017/09/17/Week-1-For-Leetcode/</id>
    <published>2017-09-17T07:28:05.000Z</published>
    <updated>2017-09-17T12:25:10.728Z</updated>
    
    <content type="html"><![CDATA[<p>　　本文是Leetcode刷题计划第一周的总结。另外，还包括了一些课堂上学习到的算法的练习，下面将围绕每一道题进行分析。<br><a id="more"></a></p>
<h1 id="InPlace-Merge"><a href="#InPlace-Merge" class="headerlink" title="InPlace Merge"></a>InPlace Merge</h1><p>　　第一道题是归并排序。归并排序基本思路是使用分治法进行求解。这里的关键操作在于Merge两个有序的数组。通常的做法是，付出O(n)空间复杂度的代价，新建一个数组，然后使用两个指针分别遍历原数组的左右两边，依次选择较小的值放到新数组，最后复制新数组的值到原数组。(也可以一开始复制原数组左右两边到两个小数组，然后遍历依次放到原数组中)<br>　　这里面我们付出了O(n)的空间复杂度和O(n)的时间复杂度的代价。能否使得空间复杂度降低为O(1)？</p>
<h2 id="内存反转"><a href="#内存反转" class="headerlink" title="内存反转　　"></a>内存反转　　</h2><p>　　这里使用的是循环移位的策略，也称作内存反转。如下所示：<br>　　<strong>Q</strong>:给定序列,\(a_1,a_2,…,a_m, b_1,b_2,…,b_n\),把它变成\(b_1,b_2,…,b_n, a_1,a_2,…,a_m\)?<br>　　<strong>A</strong>:先对\(a_1,a_2,…,a_m\)进行反转；再对\(b_1,b_2,…,b_n\)进行反转，最后再对整体进行反转，就可以得到\(b_1,b_2,…,b_n, a_1,a_2,…,a_m\)。代码如下：<br><figure class="highlight c++"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">void</span> <span class="title">exchange</span><span class="params">(<span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt; &amp;A, <span class="keyword">int</span> s, <span class="keyword">int</span> m, <span class="keyword">int</span> e)</span></span>&#123;</div><div class="line">    reverse(A,s, m);</div><div class="line">    reverse(A,m+<span class="number">1</span>,e);</div><div class="line">    reverse(A, s, e);</div><div class="line">&#125;</div><div class="line"></div><div class="line"><span class="function"><span class="keyword">void</span> <span class="title">swap</span><span class="params">(<span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt; &amp;A, <span class="keyword">int</span> x, <span class="keyword">int</span> y)</span></span>&#123;</div><div class="line">    <span class="keyword">int</span> t = A[x];</div><div class="line">    A[x] = A[y];</div><div class="line">    A[y] = t;</div><div class="line">&#125;</div><div class="line"><span class="function"><span class="keyword">void</span> <span class="title">reverse</span><span class="params">(<span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt; &amp;A, <span class="keyword">int</span> s, <span class="keyword">int</span> e)</span></span>&#123;</div><div class="line">    <span class="keyword">while</span>(s &lt; e)&#123;</div><div class="line">        swap(A, s++, e--);</div><div class="line">    &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure></p>
<h2 id="原地归并"><a href="#原地归并" class="headerlink" title="原地归并"></a>原地归并</h2><p>　　假设数据如下图所示：<br><img src="/picture/machine-learning/leetcode1.jpg" alt="leetcode"><br>　　开始时\(i,j\)分别指向这个数组的两个有序子序列的第一个值，然后指针向后移动,<strong>直到</strong>找到比20大的值，即移动到30，此时我们知道\(i\)指针之前的值一定是两个子序列最小的块。<br>　　接着，先用一个临时指针记录\(j\)的位置。然后把第二个序列的指针\(j\)向后移动，<strong>直到</strong>找到比30大的值，即移动到55，即如下图所示：<br><img src="/picture/machine-learning/leetcode2.png" alt="leetcode"><br>　　这样，<strong>我们把[i,index)和[index,j)的内存块进行交换</strong>，再移动i指针，移动步长step=j-index，两个子数组的分界点mid也要相应的向后移动step=j-index。如下图：<br><img src="/picture/machine-learning/leetcode3.jpg" alt="leetcode"><br>　　这样可以看出\(i\)之前的都已经排好序，而以\(i\)开始的子序列和以\(j\)开始的子序列又是开始的问题模型，同样的操作进行下去最终排序完成。<br><figure class="highlight c++"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">void</span> <span class="title">inplaceMerge</span><span class="params">(<span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt; &amp;A, <span class="keyword">int</span> start, <span class="keyword">int</span> mid, <span class="keyword">int</span> end)</span></span>&#123;</div><div class="line">    <span class="keyword">int</span> i = start;</div><div class="line">    <span class="keyword">int</span> j = mid+<span class="number">1</span>;</div><div class="line">    <span class="keyword">while</span>(i &lt;= mid &amp;&amp; j &lt;= end)&#123;<span class="comment">//两个条件都需要测试</span></div><div class="line">        <span class="keyword">int</span> step = <span class="number">0</span>;</div><div class="line">        <span class="keyword">while</span>(i &lt;= mid &amp;&amp; A[i] &lt;= A[j])&#123;++i;&#125;<span class="comment">//找到第一个比A[j]大的数</span></div><div class="line">        </div><div class="line">        <span class="keyword">while</span>(j &lt;= end &amp;&amp; A[j] &lt; A[i])&#123;++j;++step;&#125;<span class="comment">//找到比A[i]大的所有A[j]</span></div><div class="line">        </div><div class="line">        exchange(A, i, mid, j<span class="number">-1</span>);<span class="comment">//反转</span></div><div class="line">        i += step; <span class="comment">//移动指针</span></div><div class="line">        mid += step;<span class="comment">//分割点也要后移</span></div><div class="line">        </div><div class="line">    &#125;</div><div class="line">&#125;</div><div class="line"><span class="function"><span class="keyword">void</span> <span class="title">sort</span><span class="params">(<span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt; &amp;A)</span> </span>&#123;</div><div class="line">    mergeSort(A, <span class="number">0</span>,  A.size()<span class="number">-1</span>);</div><div class="line">&#125;</div><div class="line">    </div><div class="line"><span class="function"><span class="keyword">void</span> <span class="title">mergeSort</span><span class="params">(<span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt; &amp;A, <span class="keyword">int</span> start, <span class="keyword">int</span> end)</span></span>&#123;</div><div class="line">    <span class="keyword">if</span>(start &lt; end)&#123;</div><div class="line">        <span class="keyword">int</span> mid = (start + end)/<span class="number">2</span>;</div><div class="line">        mergeSort(A, start, mid);</div><div class="line">        mergeSort(A, mid+<span class="number">1</span>, end);</div><div class="line">        inplaceMerge(A, start, mid, end);</div><div class="line">    &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure></p>
<h1 id="Two-Sum-1"><a href="#Two-Sum-1" class="headerlink" title="Two Sum (#1)"></a>Two Sum (#1)</h1><p>　　问题：给定一个无序数组，在O(n)时间内寻找两个数的和为target，返回这两个数的下标。题目已经保证了结果是唯一的。<br>　　分析：如果是O(n^2)复杂度，那么只要两个循环遍历就能找到。另外，O(n)时间复杂度意味着不能排序，返回下标也意味着最好不要打乱原数组的顺序。我们联想到哈希表获取元素的时间复杂度为O(1)，如果将数据处理成target-某个数=另一个数，那么使用哈希表查找就能查找到另一个数(key:原始数据,value:下标)。但是由于题目没有保证元素都不相同，因此不能先循环，处理每个数存入哈希表(相同的数会覆盖)。这里使用的是边遍历边存储到哈希表，如果在哈希表中找到另一个数，则直接返回结果，否则将该数存入哈希表中。代码如下：<br><figure class="highlight c++"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">class</span> Solution &#123;</div><div class="line"><span class="keyword">public</span>:</div><div class="line">    <span class="comment">/**</span></div><div class="line">    *每个数处理成target-该数 7 2 -2 -6</div><div class="line">    *对map进行查找，如果存在等于target-该数的数值，则找到。否则存储map该&#123;原始数据:位置&#125; 题目中说不存在相同的数</div><div class="line">    *</div><div class="line">    */</div><div class="line">    <span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt; twoSum(<span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt;&amp; nums, <span class="keyword">int</span> target) &#123;</div><div class="line">        <span class="built_in">unordered_map</span>&lt;<span class="keyword">int</span>,<span class="keyword">int</span>&gt; <span class="built_in">map</span>;</div><div class="line">        <span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt; result;</div><div class="line">        <span class="keyword">for</span>(<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; nums.size(); ++i)&#123;</div><div class="line">            <span class="keyword">int</span> findNum = target - nums[i];</div><div class="line">            <span class="keyword">if</span>(<span class="built_in">map</span>.find(findNum) != <span class="built_in">map</span>.end())&#123;<span class="comment">//find查找的是key是否存在</span></div><div class="line">                result.push_back(i);</div><div class="line">                result.push_back(<span class="built_in">map</span>[findNum]);</div><div class="line">            &#125;</div><div class="line">            <span class="built_in">map</span>[nums[i]] = i;</div><div class="line">        &#125;</div><div class="line">        <span class="keyword">return</span> result;</div><div class="line">        </div><div class="line">    &#125;</div><div class="line">&#125;;</div></pre></td></tr></table></figure></p>
<p>　　变型：如果加上一个条件原数组是有序的，那么可以使用另外一种方法找到所有和为target的数据对。<br>　　分析：可以使用两个指针\(i,j\),分别指向有序数组的低端和高端，如果两个指针所在位置的数据之和为target，则加入结果集。如果和小于target，说明\(i\)指向的数太小了，必须稍微大一点才可以；或者说\(j\)指向的数不够大，任凭j怎么往左移都没用，因为\(j\)指向的数最大。因此向右移动\(i\)指针。如果和大于target，说明\(j\)指向的太大了，因此往左移动\(j\)指针。<br>　　为了去掉重复的结果，需要在和等于target的条件里，移动两个指针，直到遇到不一样的数。<br><figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">public</span> <span class="keyword">static</span> List&lt;<span class="keyword">int</span>[]&gt; twoSum(Integer[] nums, <span class="keyword">int</span> target) &#123;</div><div class="line">    List&lt;<span class="keyword">int</span>[]&gt; result = <span class="keyword">new</span> ArrayList&lt;<span class="keyword">int</span>[]&gt;();</div><div class="line">    <span class="keyword">int</span> i = <span class="number">0</span>, j = nums.length - <span class="number">1</span>;</div><div class="line">    <span class="keyword">while</span>(i &lt; j)&#123;</div><div class="line">        <span class="keyword">if</span>(nums[i] + nums[j] == target)&#123;</div><div class="line">            result.add(<span class="keyword">new</span> <span class="keyword">int</span>[]&#123;nums[i], nums[j]&#125;);</div><div class="line">            <span class="keyword">while</span>(i &lt; j &amp;&amp; nums[i+<span class="number">1</span>] == nums[i])&#123;++i;&#125;<span class="comment">//filter duplicate</span></div><div class="line">            <span class="keyword">while</span>(i &lt; j &amp;&amp; nums[j-<span class="number">1</span>] == nums[j])&#123;--j;&#125;<span class="comment">//filter duplicate</span></div><div class="line">            ++i;</div><div class="line">            --j;</div><div class="line">        &#125;<span class="keyword">else</span> <span class="keyword">if</span>(nums[i] + nums[j] &lt; target)&#123;</div><div class="line">            ++i;</div><div class="line">        &#125;<span class="keyword">else</span> &#123;</div><div class="line">            --j;</div><div class="line">        &#125;</div><div class="line">    &#125;</div><div class="line">    <span class="keyword">return</span> result;</div><div class="line">&#125;</div></pre></td></tr></table></figure></p>
<h1 id="Three-Sum-15"><a href="#Three-Sum-15" class="headerlink" title="Three Sum (#15)"></a>Three Sum (#15)</h1><p>　　问题：给定一个数组，在O(n^2)复杂度内找出任何3个和为0的数，返回数据对即可。<br>　　分析：该题可以进行排序求解，返回数据而不是下标。可以利用Two Sum中变型策略。首先固定一个数，然后使用Two Sum求解。即，使用for循环遍历数组，固定当前数，然后找到所有和当前数搭配的和0的其余两个数，即twoTarget = -nums[i]。具体代码如下：<br><figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">public</span> <span class="keyword">static</span> List&lt;<span class="keyword">int</span>[]&gt; threeSum(Integer[] nums) &#123;</div><div class="line">    Arrays.sort(nums);</div><div class="line">    List&lt;<span class="keyword">int</span>[]&gt; result = <span class="keyword">new</span> ArrayList&lt;<span class="keyword">int</span>[]&gt;();</div><div class="line">    <span class="keyword">for</span>(<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; nums.length-<span class="number">1</span>; ++i)&#123;</div><div class="line">        <span class="keyword">if</span>(i != <span class="number">0</span> &amp;&amp; nums[i] == nums[i-<span class="number">1</span>]) <span class="comment">//相同元素不需要再查找了</span></div><div class="line">            <span class="keyword">continue</span>;</div><div class="line">        <span class="keyword">int</span> s = i+<span class="number">1</span>, e = nums.length - <span class="number">1</span>;</div><div class="line">        <span class="keyword">int</span> target = -nums[i];</div><div class="line">        <span class="keyword">while</span>(s &lt; e)&#123;</div><div class="line">            <span class="keyword">int</span> twoSum = nums[s] + nums[e];</div><div class="line">            <span class="keyword">if</span>(twoSum == target) &#123;</div><div class="line">                result.add(<span class="keyword">new</span> <span class="keyword">int</span>[]&#123;nums[i],nums[s],nums[e]&#125;);</div><div class="line">                <span class="keyword">while</span> (s &lt; e &amp;&amp; nums[s+<span class="number">1</span>] == nums[s]) ++s;<span class="comment">//filter duplicate</span></div><div class="line">                <span class="keyword">while</span> (s &lt; e &amp;&amp; nums[e-<span class="number">1</span>] == nums[e]) --e;<span class="comment">//filter duplicate</span></div><div class="line">                --s;<span class="comment">//move</span></div><div class="line">                --e;<span class="comment">//move</span></div><div class="line">            &#125;<span class="keyword">else</span> <span class="keyword">if</span>(twoSum &lt; target)&#123;</div><div class="line">                ++s;</div><div class="line">            &#125;<span class="keyword">else</span>&#123;</div><div class="line">                --e;</div><div class="line">            &#125;</div><div class="line">        &#125;</div><div class="line">    &#125;</div><div class="line">    <span class="keyword">return</span> result;</div><div class="line">&#125;</div></pre></td></tr></table></figure></p>
<p>　　注意上述去重操作。
　　</p>
<h1 id="Swap-Elements-By-Xor"><a href="#Swap-Elements-By-Xor" class="headerlink" title="Swap Elements By Xor"></a>Swap Elements By Xor</h1><p>　　可以使用异或操作交换数据。原理是使用异或的性质：</p>
<ul>
<li>任意两个相同的数异或都为0</li>
<li>任意数(1或者0)和0异或都为本身。1 xor 0 = 1, 0 xor 0 = 1;因此式子中有0的地方可以直接去掉0。</li>
<li>异或满足结合律和交换律。<figure class="highlight c++"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">void</span> <span class="title">swap</span><span class="params">(<span class="keyword">int</span> &amp;x, <span class="keyword">int</span> &amp;y)</span></span>&#123;</div><div class="line">    <span class="keyword">if</span>(x == y) <span class="keyword">return</span>;</div><div class="line">    x ^= y; <span class="comment">// x = x ^ y</span></div><div class="line">    y ^= x; <span class="comment">// y = x ^ y ^ y = x ^ (y ^ y) = x ^ (0) = x </span></div><div class="line">    x ^= y; <span class="comment">// x = x ^ y ^ x =  y</span></div><div class="line">&#125;</div></pre></td></tr></table></figure>
</li>
</ul>
<p>　　运行原理如上述代码注释部分。<br>　　这里有一个陷进，如果两个相同的数传入，实际上交换完没变。但是因为相同的数异或为0，使得x和y都等于0了。因此最好要一开始检查一下是否相同。</p>
<h1 id="Number-of-1-Bits-191"><a href="#Number-of-1-Bits-191" class="headerlink" title="Number of 1 Bits(#191)"></a>Number of 1 Bits(#191)</h1><p>　　统计位当中1的个数。正常想法就是看看最后1位是不是1，是就累加。然后不断往右移位，直到数变成0为止。如下：<br><figure class="highlight c++"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">int</span> <span class="title">hammingWeight</span><span class="params">(<span class="keyword">uint32_t</span> n)</span></span>&#123;</div><div class="line">        <span class="keyword">int</span> c = <span class="number">0</span>;</div><div class="line">        <span class="keyword">while</span>(n)&#123;</div><div class="line">            <span class="keyword">int</span> t = n &amp; <span class="number">1</span>;</div><div class="line">            <span class="keyword">if</span>(t)&#123;c++;&#125;</div><div class="line">            n &gt;&gt;= <span class="number">1</span>;</div><div class="line">        &#125;</div><div class="line">        <span class="keyword">return</span> c;</div><div class="line">    &#125;</div></pre></td></tr></table></figure></p>
<p>　　另一种方法，利用\(n \&amp;= (n-1) \),<strong>该式子会将n最右边的1变成0</strong>。如果本身为0,与之后肯定为0不变；如果最右边是1，减一后为0，再&amp;之后就变成0了。<br><figure class="highlight c++"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">int</span> <span class="title">hammingWeight</span><span class="params">(<span class="keyword">uint32_t</span> n)</span> </span>&#123;</div><div class="line">    <span class="keyword">int</span> count = <span class="number">0</span>;</div><div class="line">    <span class="keyword">while</span>(n)&#123;<span class="comment">//非零</span></div><div class="line">        ++count;</div><div class="line">        n &amp;= (n<span class="number">-1</span>); <span class="comment">//关键，每次都会把最靠右边的1变成0。</span></div><div class="line">    &#125;</div><div class="line">    <span class="keyword">return</span> count;</div><div class="line">&#125;</div></pre></td></tr></table></figure></p>
<h1 id="Count-Bits-338"><a href="#Count-Bits-338" class="headerlink" title="Count Bits(#338)"></a>Count Bits(#338)</h1><p>　　问题：统计range(n)范围内所有数当中位为1的个数。要求时间复杂度为O(n)。<br>　　分析：正常会对每个数求1的个数，复杂度为O(n * sizeof(integer))。这里由于要求出前面数的1的个数，因此可以利用前面的数，动态规划思想。将大的数化成前面小的数。一种方法是，单独抽出最后一位，则原始的数可以往右移1位变成前面的数。<br><figure class="highlight c++"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line"><span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt; countBits(<span class="keyword">int</span> num) &#123;</div><div class="line">        <span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt; result(num+<span class="number">1</span>, <span class="number">0</span>);</div><div class="line">        <span class="keyword">for</span>(<span class="keyword">int</span> i = <span class="number">1</span>; i &lt;= num; ++i)&#123;</div><div class="line">            result[i] = result[i&gt;&gt;<span class="number">1</span>] + (i &amp; <span class="number">1</span>);</div><div class="line">        &#125;</div><div class="line">        <span class="keyword">return</span> result;</div><div class="line">    &#125;</div></pre></td></tr></table></figure></p>
<p>　　另一种，化大数为前面的数，是利用前面提到的将某个数位当中最右边1化为0.则：<br><figure class="highlight c++"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">result[i] = result[i &amp; (i<span class="number">-1</span>)] + <span class="number">1</span>;</div></pre></td></tr></table></figure></p>
<h1 id="Reverse-Bits-190"><a href="#Reverse-Bits-190" class="headerlink" title="Reverse Bits (#190)"></a>Reverse Bits (#190)</h1><p>　　问题：反转数的二进制位。<br>　　分析：可以新建一个数\(m=0\),m先左移一位腾出一个位置，然后从原始数的最后一位开始，不断用m求或运算。直到原始数位全部遍历完毕。复杂度为O(sizeof(int)).<br><figure class="highlight c++"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">uint32_t</span> reverseBits(<span class="keyword">uint32_t</span> n) &#123;</div><div class="line">    <span class="keyword">uint32_t</span> m = <span class="number">0</span>;</div><div class="line">    <span class="keyword">for</span>(<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; <span class="number">32</span>; ++i)&#123;</div><div class="line">        m &lt;&lt;= <span class="number">1</span>; <span class="comment">//先挪出一个位置</span></div><div class="line">        m |= (n &amp; <span class="number">1</span>);</div><div class="line">        n &gt;&gt;= <span class="number">1</span>;  </div><div class="line">    &#125;</div><div class="line">    <span class="keyword">return</span> m;</div><div class="line">&#125;</div></pre></td></tr></table></figure></p>
<p>　　另外，还有个O(log sizeof(int))复杂度的算法如下：<br><figure class="highlight c++"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">uint32_t</span> reverseBits(<span class="keyword">uint32_t</span> n) &#123;</div><div class="line">    n = (n &gt;&gt; <span class="number">16</span>) | (n &lt;&lt; <span class="number">16</span>);</div><div class="line">    n = ((n &amp; <span class="number">0xff00ff00</span>) &gt;&gt; <span class="number">8</span>) | ((n &amp; <span class="number">0x00ff00ff</span>) &lt;&lt; <span class="number">8</span>);</div><div class="line">    n = ((n &amp; <span class="number">0xf0f0f0f0</span>) &gt;&gt; <span class="number">4</span>) | ((n &amp; <span class="number">0x0f0f0f0f</span>) &lt;&lt; <span class="number">4</span>);</div><div class="line">    n = ((n &amp; <span class="number">0xcccccccc</span>) &gt;&gt; <span class="number">2</span>) | ((n &amp; <span class="number">0x33333333</span>) &lt;&lt; <span class="number">2</span>);</div><div class="line">    n = ((n &amp; <span class="number">0xaaaaaaaa</span>) &gt;&gt; <span class="number">1</span>) | ((n &amp; <span class="number">0x55555555</span>) &lt;&lt; <span class="number">1</span>);</div><div class="line">    <span class="keyword">return</span> n;</div><div class="line">&#125;</div></pre></td></tr></table></figure></p>
<h1 id="Reverse-Integer-7"><a href="#Reverse-Integer-7" class="headerlink" title="Reverse Integer(#7)"></a>Reverse Integer(#7)</h1><p>　　问题：反转整数<br>　　分析：新建一个数result初始化为0，每次取出最后一位，使用result<em>10+最后一位。即可得到逆序。例如：123： ( (0\</em>10+3)* 10)+2)*10 + 1 = 321. 注意溢出的处理，策略是：将求得的结果tmp=result*10+最后一位,根据：(tmp-最后一位）/10，逆序求出result，看看跟原来的result是否相等，不相等说明溢出了。<br><figure class="highlight c++"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div></pre></td><td class="code"><pre><div class="line"> <span class="function"><span class="keyword">int</span> <span class="title">reverse</span><span class="params">(<span class="keyword">int</span> x)</span> </span>&#123;</div><div class="line">    <span class="keyword">int</span> result = <span class="number">0</span>;</div><div class="line">    <span class="keyword">while</span>(x)&#123;</div><div class="line">        <span class="keyword">int</span> last = x % <span class="number">10</span>;</div><div class="line">        <span class="keyword">int</span> tmp = result * <span class="number">10</span> + last;</div><div class="line">        <span class="keyword">if</span>((tmp-last)/<span class="number">10</span> != result) <span class="keyword">return</span> <span class="number">0</span>; <span class="comment">//溢出</span></div><div class="line">        result = tmp;</div><div class="line">        x /= <span class="number">10</span>; </div><div class="line">    &#125;</div><div class="line">    <span class="keyword">return</span> result;</div><div class="line">    <span class="comment">//for 8 bit binary number abcdefgh, the process is as follow:</span></div><div class="line">    <span class="comment">//abcdefgh -&gt; efghabcd -&gt; ghefcdab -&gt; hgfedcba</span></div><div class="line">&#125;</div></pre></td></tr></table></figure></p>
<h1 id="Judge-Route-Circle-657"><a href="#Judge-Route-Circle-657" class="headerlink" title="Judge Route Circle(# 657)"></a>Judge Route Circle(# 657)</h1><p>　　问题：机器人上下左右移动，给出一个移动序列，判断是否回到出发点。<br>　　答：两个数分别统计上下和左右情况，上下抵消，左右抵消。最后都为0则回到原点。<br><figure class="highlight c++"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">bool</span> <span class="title">judgeCircle</span><span class="params">(<span class="built_in">string</span> moves)</span> </span>&#123;</div><div class="line">    <span class="keyword">int</span> x = <span class="number">0</span>, y = <span class="number">0</span>;</div><div class="line">    <span class="keyword">for</span>(<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; moves.size(); ++i)&#123;</div><div class="line">        <span class="keyword">if</span>(moves[i] == <span class="string">'U'</span>)&#123;</div><div class="line">            x += <span class="number">1</span>;</div><div class="line">        &#125;<span class="keyword">else</span> <span class="keyword">if</span>(moves[i] == <span class="string">'D'</span>)&#123;</div><div class="line">            x -= <span class="number">1</span>;</div><div class="line">        &#125;<span class="keyword">else</span> <span class="keyword">if</span>(moves[i] == <span class="string">'R'</span>)&#123;</div><div class="line">            y += <span class="number">1</span>;</div><div class="line">        &#125;<span class="keyword">else</span> <span class="keyword">if</span>(moves[i] == <span class="string">'L'</span>)&#123;</div><div class="line">            y -= <span class="number">1</span>;</div><div class="line">        &#125;</div><div class="line">    &#125;</div><div class="line">    <span class="keyword">return</span> x == <span class="number">0</span> &amp;&amp; y == <span class="number">0</span>;</div><div class="line">&#125;</div></pre></td></tr></table></figure></p>
<h1 id="Merge-Two-Sorted-Array-88"><a href="#Merge-Two-Sorted-Array-88" class="headerlink" title="Merge Two Sorted Array(#88)"></a>Merge Two Sorted Array(#88)</h1><p>　　问题：给定两个有序的数组nums1,nums2,要求将nums2 Merge到nums1，使得nums1保持有序。只能利用nums1空间，题目已经保证nums1能保证容纳的下nums2的元素。<br>　　分析：常规想法是从第一个数开始merge到nums1中，这样就需要移数操作。如下：<br><figure class="highlight c++"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">void</span> <span class="title">merge</span><span class="params">(<span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt;&amp; nums1, <span class="keyword">int</span> m, <span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt;&amp; nums2, <span class="keyword">int</span> n)</span> </span>&#123;</div><div class="line">        <span class="keyword">int</span> count_i = <span class="number">0</span>, count_j = <span class="number">0</span>, size = m + n;</div><div class="line">        <span class="keyword">int</span> end = m; </div><div class="line">        <span class="keyword">int</span> p = <span class="number">0</span>;</div><div class="line">        <span class="keyword">while</span>(p &lt; size)&#123; <span class="comment">//每个元素都遍历,p指向当前nums1需要同nums2比较的位置</span></div><div class="line">            <span class="keyword">if</span>(count_j &gt;= n) <span class="keyword">break</span>; <span class="comment">//nums2已经merge完毕则结束</span></div><div class="line">            <span class="keyword">if</span>(count_i &lt; m &amp;&amp; nums1[p] &lt;= nums2[count_j])&#123;<span class="comment">//nums1还没merge完毕，</span></div><div class="line">                count_i++;</div><div class="line">                p++;</div><div class="line">            &#125;</div><div class="line">            <span class="keyword">else</span>&#123;</div><div class="line">               <span class="keyword">if</span>(count_i &lt; m)&#123;<span class="comment">//nums[1]还没全部遍历完毕才移动</span></div><div class="line">                   move(nums1, p, end<span class="number">-1</span>);<span class="comment">//移动p之后的元素</span></div><div class="line">               &#125;</div><div class="line">               nums1[p] = nums2[count_j]; <span class="comment">//放到p这个位置</span></div><div class="line">               count_j++;</div><div class="line">               p++; </div><div class="line">               end++;<span class="comment">//尾部编号+1</span></div><div class="line">            &#125;</div><div class="line">        &#125;</div><div class="line">    &#125;</div><div class="line">    <span class="function"><span class="keyword">void</span> <span class="title">move</span><span class="params">(<span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt;&amp; nums, <span class="keyword">int</span> start, <span class="keyword">int</span> end)</span> </span>&#123;</div><div class="line">        <span class="keyword">for</span>(<span class="keyword">int</span> i = end; i &gt;= start; --i)&#123;</div><div class="line">            nums[i+<span class="number">1</span>] = nums[i];</div><div class="line">        &#125;</div><div class="line">    &#125;</div></pre></td></tr></table></figure></p>
<p>　　上述复杂度为O(n^2)。另一种想法是从尾部开始merge。两个指针分别指向两个数组的最后一个元素，将二者中大的数挪到nums1最后一个位置p=m+n-1,一直到p=0或者nums2已经全部放到nums1中了，则停止循环。<br><figure class="highlight c++"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">void</span> <span class="title">merge</span><span class="params">(<span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt;&amp; A, <span class="keyword">int</span> m, <span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt;&amp; B, <span class="keyword">int</span> n)</span> </span>&#123;</div><div class="line">    <span class="keyword">int</span> i = m - <span class="number">1</span>, j = n - <span class="number">1</span>;</div><div class="line">    <span class="keyword">int</span> p = m + n - <span class="number">1</span>;<span class="comment">//最后一个位置</span></div><div class="line">    <span class="keyword">while</span>(p &gt;= <span class="number">0</span>)&#123;</div><div class="line">        <span class="keyword">if</span>(j &lt; <span class="number">0</span>) <span class="keyword">break</span>;</div><div class="line">        <span class="keyword">if</span>(i &gt;= <span class="number">0</span> &amp;&amp; A[i] &gt;= B[j])&#123;</div><div class="line">            A[p] = A[i--];</div><div class="line">        &#125;</div><div class="line">        <span class="keyword">else</span>&#123;</div><div class="line">            A[p] = B[j--];</div><div class="line">        &#125;</div><div class="line">        --p;      </div><div class="line">    &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure></p>
<p>　　还可以利用上面提到到的Inplace Merge方法，将nums2的元素直接全部放到nums1尾部，然后调用InplaceMerge方法。</p>
<h1 id="Merge-Two-Sorted-List-21"><a href="#Merge-Two-Sorted-List-21" class="headerlink" title="Merge Two Sorted List(#21)"></a>Merge Two Sorted List(#21)</h1><p>　　问：将两个有序链表merge到一起。<br>　　答：一种思路是将list2 Merge到list1，不断重新构造list1即可。如下：<br><figure class="highlight c++"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div></pre></td><td class="code"><pre><div class="line"><span class="function">ListNode* <span class="title">mergeTwoLists</span><span class="params">(ListNode* l1, ListNode* l2)</span> </span>&#123;</div><div class="line">    <span class="keyword">if</span>(l1 == <span class="literal">NULL</span>) <span class="keyword">return</span> l2;</div><div class="line">    <span class="keyword">if</span>(l2 == <span class="literal">NULL</span>) <span class="keyword">return</span> l1;</div><div class="line">    </div><div class="line">    <span class="keyword">if</span>(l1-&gt;val &gt; l2-&gt;val)&#123;<span class="comment">//交换，保证l1的第一个数比l2的第一个数小</span></div><div class="line">        ListNode *t = l1;</div><div class="line">        l1 = l2;</div><div class="line">        l2 = t;</div><div class="line">    &#125;</div><div class="line">    </div><div class="line">    ListNode *p = l1, *pre = l1, *q = l2;</div><div class="line">      </div><div class="line">    <span class="keyword">while</span>(p != <span class="literal">NULL</span> || q != <span class="literal">NULL</span>)&#123;</div><div class="line">        <span class="keyword">if</span>(p == <span class="literal">NULL</span>)&#123;<span class="comment">//l1已经到头了</span></div><div class="line">            pre-&gt;next = q;</div><div class="line">            <span class="keyword">break</span>;</div><div class="line">        &#125;</div><div class="line">        <span class="keyword">if</span>(q == <span class="literal">NULL</span>) <span class="keyword">break</span>;<span class="comment">//l2已经全部merge完毕了</span></div><div class="line">        </div><div class="line">        <span class="keyword">if</span>(p-&gt;val &lt;= q-&gt;val)&#123;</div><div class="line">            pre = p;</div><div class="line">            p = p-&gt;next;</div><div class="line">        &#125;<span class="keyword">else</span>&#123;</div><div class="line">            ListNode *tmp = q-&gt;next; </div><div class="line">            q-&gt;next = p;</div><div class="line">            pre-&gt;next = q;</div><div class="line">            pre = pre-&gt;next;<span class="comment">//移动</span></div><div class="line">            q = tmp;<span class="comment">//移动</span></div><div class="line">        &#125;</div><div class="line">    &#125;</div><div class="line">    <span class="keyword">return</span> l1;</div><div class="line">&#125;</div></pre></td></tr></table></figure></p>
<p>　　还有种方法是，新建一条新的链表，这个空间复杂度实际上是O(1),除了浪费一个头节点以外，其余的节点都是list1和list2上已经存在的节点。<br><figure class="highlight c++"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div></pre></td><td class="code"><pre><div class="line"><span class="function">ListNode* <span class="title">mergeTwoLists</span><span class="params">(ListNode* l1, ListNode* l2)</span> </span>&#123;</div><div class="line">    ListNode dump = ListNode(INT_MAX);</div><div class="line">    ListNode *tail = &amp;dump;</div><div class="line">    <span class="keyword">while</span>(l1 != <span class="literal">NULL</span> &amp;&amp; l2 != <span class="literal">NULL</span>)&#123;</div><div class="line">        <span class="keyword">if</span>(l1-&gt;val &lt;= l2-&gt;val) &#123;</div><div class="line">            tail-&gt;next = l1;</div><div class="line">            tail = tail-&gt;next;</div><div class="line">            l1 = l1-&gt;next;</div><div class="line">        &#125;<span class="keyword">else</span>&#123;</div><div class="line">            tail-&gt;next = l2;</div><div class="line">            tail = tail-&gt;next;</div><div class="line">            l2 = l2-&gt;next;</div><div class="line">        &#125;   </div><div class="line">    &#125;</div><div class="line">    tail-&gt;next = (l1 == <span class="literal">NULL</span>)? l2:l1;</div><div class="line">    <span class="keyword">return</span> dump.next;</div><div class="line">&#125;</div></pre></td></tr></table></figure></p>
<h1 id="Merge-Two-Binary-Trees-617"><a href="#Merge-Two-Binary-Trees-617" class="headerlink" title="Merge Two Binary Trees(#617)"></a>Merge Two Binary Trees(#617)</h1><p>　　问题：将两个二叉树Merge起来，如果位置相同则数据相加，否则添加该节点到新树上。如下图：<br><img src="/picture/machine-learning/leetcode4.png" alt="leetcode"><br>　　分析：显然这题需要使用递归操作。我们目标是将Tree2 Merge到Tree1上。首先，任意一棵树为NULL，则返回令一颗树即可，都不为空，则val相加，然后递归调用merge两棵树的左子树，merge两棵树的右子树。merge左子树时，如果Tree1的左子树为空，则需要将Tree2的左子树接到Tree1的左子树上。merge右子树时，如果Tree1的右子树为空，则需要将Tree2的右子树接到Tree1的右子树上。<br><figure class="highlight c++"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">struct</span> TreeNode &#123;</div><div class="line">      <span class="keyword">int</span> val;</div><div class="line">      TreeNode *left;</div><div class="line">      TreeNode *right;</div><div class="line">      TreeNode(<span class="keyword">int</span> x) : val(x), left(<span class="literal">NULL</span>), right(<span class="literal">NULL</span>) &#123;&#125;</div><div class="line">&#125;;</div><div class="line"><span class="function">TreeNode* <span class="title">mergeTrees</span><span class="params">(TreeNode* t1, TreeNode* t2)</span> </span>&#123;</div><div class="line">    <span class="keyword">if</span>(t1 == <span class="literal">NULL</span> &amp;&amp; t2 == <span class="literal">NULL</span>) <span class="keyword">return</span> <span class="literal">NULL</span>;</div><div class="line">    <span class="keyword">if</span>(t1 == <span class="literal">NULL</span> &amp;&amp; t2 != <span class="literal">NULL</span>) <span class="keyword">return</span> t2;</div><div class="line">    <span class="keyword">if</span>(t1 != <span class="literal">NULL</span> &amp;&amp; t2 == <span class="literal">NULL</span>) <span class="keyword">return</span> t1;</div><div class="line">    </div><div class="line">    t1-&gt;val += t2-&gt;val;</div><div class="line">    </div><div class="line">    mergeTrees(t1-&gt;left, t2-&gt;left);<span class="comment">//左子树merge</span></div><div class="line">    <span class="keyword">if</span>(t1-&gt;left == <span class="literal">NULL</span>)&#123;</div><div class="line">        t1-&gt;left = t2-&gt;left;</div><div class="line">    &#125;</div><div class="line">    </div><div class="line">    mergeTrees(t1-&gt;right, t2-&gt;right);<span class="comment">//右子树merge</span></div><div class="line">    <span class="keyword">if</span>(t1-&gt;right == <span class="literal">NULL</span>)&#123;</div><div class="line">        t1-&gt;right = t2-&gt;right;</div><div class="line">    &#125;</div><div class="line">    </div><div class="line">    <span class="keyword">return</span> t1;</div><div class="line">&#125;</div></pre></td></tr></table></figure></p>
<h1 id="Hamming-Distance-461"><a href="#Hamming-Distance-461" class="headerlink" title="Hamming Distance(#461)"></a>Hamming Distance(#461)</h1><p>　　问题：求两个序列的汉明码距离。<br>　　分析：汉明码距离就是二进制相应位置上不同数的总数。比如 0001和0100，第2位和第4位不同，则距离为2。求解很简单，两个数求异或。然后求异或结果的1的个数即可。<br><figure class="highlight c++"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div></pre></td><td class="code"><pre><div class="line"><span class="comment">/* 先异或操作，然后统计1的个数。*/</span></div><div class="line">   <span class="function"><span class="keyword">int</span> <span class="title">hammingDistance</span><span class="params">(<span class="keyword">int</span> x, <span class="keyword">int</span> y)</span> </span>&#123;</div><div class="line">       <span class="keyword">int</span> z = x ^ y; <span class="comment">// xor</span></div><div class="line">       <span class="keyword">return</span> count_not_zero_bits(z);</div><div class="line">   &#125;   </div><div class="line">   <span class="comment">/* 统计非零位的个数，也就是统计任意一个数的二进制表示中1的个数*/</span></div><div class="line">   <span class="function"><span class="keyword">int</span> <span class="title">count_not_zero_bits</span><span class="params">(<span class="keyword">int</span> x)</span></span>&#123;</div><div class="line">       <span class="keyword">int</span> count = <span class="number">0</span>;</div><div class="line">       <span class="keyword">while</span>(x)&#123;<span class="comment">//x非零</span></div><div class="line">           ++count;</div><div class="line">           x &amp;= (x<span class="number">-1</span>); <span class="comment">//关键，每次都会把最靠右边的1变成0。如果最右边为0,与之后肯定为0不变；如果最右边是1，减一为0，与之后就变成0了。 </span></div><div class="line">       &#125;</div><div class="line">       <span class="keyword">return</span> count;</div><div class="line">   &#125;</div></pre></td></tr></table></figure></p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;　　本文是Leetcode刷题计划第一周的总结。另外，还包括了一些课堂上学习到的算法的练习，下面将围绕每一道题进行分析。&lt;br&gt;
    
    </summary>
    
      <category term="Leetcode" scheme="xtf615.com/categories/Leetcode/"/>
    
    
      <category term="基础算法" scheme="xtf615.com/tags/%E5%9F%BA%E7%A1%80%E7%AE%97%E6%B3%95/"/>
    
      <category term="Leetcode" scheme="xtf615.com/tags/Leetcode/"/>
    
  </entry>
  
  <entry>
    <title>算法设计与分析(Lecture 1)</title>
    <link href="xtf615.com/2017/09/16/Lecture-1-for-Algorithm-Design-and-Analysis/"/>
    <id>xtf615.com/2017/09/16/Lecture-1-for-Algorithm-Design-and-Analysis/</id>
    <published>2017-09-16T01:08:05.000Z</published>
    <updated>2017-09-17T07:07:58.840Z</updated>
    
    <content type="html"><![CDATA[<p>　　本文是针对卜东波老师算法设计与分析第一课做的课堂笔记。第一课主要介绍算法的概念以及一些代表性的问题。<br><a id="more"></a></p>
<h1 id="算法起源"><a href="#算法起源" class="headerlink" title="算法起源"></a>算法起源</h1><p>　　算法(Algorithm)一词的起源来自Muhammad ibn Musa al-Khwarizmi(C. 780-650),如下图：<br><img src="/picture/machine-learning/algorithm1.png" alt="algorithm"><br><img src="/picture/machine-learning/algorithm1.png" alt="algorithm"><br>　　算法的艺术可以类比成米开朗琪罗雕刻的艺术：<br><img src="/picture/machine-learning/algorithm3.png" alt="algorithm"><br><img src="/picture/machine-learning/algorithm4.png" alt="algorithm"><br>　　上述是指，理解算法必须看清算法问题本身的结构。</p>
<h1 id="解算法问题的基本方法"><a href="#解算法问题的基本方法" class="headerlink" title="解算法问题的基本方法"></a>解算法问题的基本方法</h1><h2 id="算法问题的产生过程"><a href="#算法问题的产生过程" class="headerlink" title="算法问题的产生过程"></a>算法问题的产生过程</h2><p>　　【Practical Problems】—-Topic Choosing—-&gt; 【A Practical Problem】—-Formulation—–&gt;【Math/Algorithm Problem】—-Key Observation—-&gt;【Solution】<br>　　任何算法产生的步骤大致如上。不同学科的研究人员关注的点也不一样，例如研究数学的可能认为Formulation最重要；研究计算机的可能认为Key Observation最重要；但实际上Topic Choosing才是最重要的，因为Topic Choosing反映了这个实际算法问题的本质。作为计算机专业的，我们重点来研究Key Observation，通过观察来发现算法的本质，可通过观察算法的结构(Structure)以及解的形式(Solution Forms)来发现。</p>
<h2 id="算法求解方法"><a href="#算法求解方法" class="headerlink" title="算法求解方法"></a>算法求解方法</h2><p><img src="/picture/machine-learning/algorithm0.jpg" alt="algorithm"><br>　　<strong>首先进行算法结构的观察</strong>。如上图所示，对于一个实际的问题，首先进行形式化。我们所研究的问题大致可以分成可进行组合优化求解的问题或者通过统计(statistic)学习方法求解的问题。组合优化(Combinatorial Optimization),是指寻找离散事件的最优编排、分组、次序或筛选，是运筹学的一个分支。参考<a href="https://wenku.baidu.com/view/4779896faf1ffc4ffe47ac43.html" target="_blank" rel="external">组合最优化</a>。而统计学习则是机器学习领域主要的方法。<br>　　算法设计与分析课主要研究一类可进行组合优化的问题。组合优化问题当中，有一类可进行自我分解(SELF-REDUCTION)从而降低问题规模的问题，这类问题通常可以通过数学归纳法(<strong>INDUCTION</strong>)来证明求解的可行性，包括DC(Divide And Conquer)分治算法、DP(Dynamic Programming)动态规划问题，最优子结构是DP的前提。另一类问题是不能够通过分解来降低问题规模，此时可以使用反复迭代来逐步提升(<strong>IMPROVEMENT</strong>)性能，通常从一个初始解开始，然后一步一步提升性能，这类问题包括LP/SDP/NF/LS/MC/SA等。<br>　　<strong>其次进行解形式的观察。</strong>对于解形式的观察，我们可以通过SMART <strong>ENUMERATION</strong>即有策略性的枚举来发现规律或求解问题的方法。例如最优化问题(optimal)可以化成近似(approximation)问题，通过枚举近似解来观察；对于确定性问题(deterministic)可以通过枚举化成随机化(randomization)问题;特殊(special cases)情况通过枚举实际(Practical)情况来观察。这类观察解形式来发现求解问题的方法的典型代表包括Greedy贪心算法等。<br>　　总之，要时刻关注问题的求解策略，选择最合适的方法，INDUCTION、IMPROVEMENT、ENUMERATION，所有算法问题的求解不外乎这3种方法。</p>
<h1 id="案例分析"><a href="#案例分析" class="headerlink" title="案例分析"></a>案例分析</h1><p>　　结合上面三种方法来进行案例分析。</p>
<h2 id="求解最大公约数"><a href="#求解最大公约数" class="headerlink" title="求解最大公约数"></a>求解最大公约数</h2><p>　　第一个案例是求解最大公约数。这个问题就是通过分解问题为子问题进行求解。<br><img src="/picture/machine-learning/algorithm5.png" alt="algorithm"><br><img src="/picture/machine-learning/algorithm6.png" alt="algorithm"></p>
<h2 id="TSP问题"><a href="#TSP问题" class="headerlink" title="TSP问题"></a>TSP问题</h2><p><img src="/picture/machine-learning/algorithm7.png" alt="algorithm"><br>　　上图是旅行商问题的形式化定义。</p>
<h3 id="Method1-Divide-And-Conquer"><a href="#Method1-Divide-And-Conquer" class="headerlink" title="Method1:Divide And Conquer"></a>Method1:Divide And Conquer</h3><p>　　首先考虑一个相关的问题。考虑最后一步回到起始点的所有可能。如下图，总共有3种情况。<br><img src="/picture/machine-learning/algorithm8.png" alt="algorithm"><br>　　再考虑一个更小的问题。<br><img src="/picture/machine-learning/algorithm9.png" alt="algorithm"><br>　　如何求解D({2,3,4},4)呢？<br>　　D({2,3,4},4)意味着要先经过2和3再到达4，因此这里面总共有2种情况。1）起始点1先经过2，再经过3，最后到达4。2）起始点1先经过3，再到达2，最后到达4。因此：<br>$$D(\{2,3,4\},4)=min{ D(\{2,3\},3)+d_{34}, D(\{2,3\},2)+d_{24}}$$<br>　　因此算法如下：<br><img src="/picture/machine-learning/algorithm10.png" alt="algorithm"></p>
<h3 id="Method2-Improvement-Strategy"><a href="#Method2-Improvement-Strategy" class="headerlink" title="Method2:Improvement Strategy"></a>Method2:Improvement Strategy</h3><p>　　首先是解空间的一些基本概念。<br><img src="/picture/machine-learning/algorithm11.png" alt="algorithm"><br>　　其中节点代表的是一个完整的解，边代表的是节点之间存在关联。提升策略意味着会从一个初始解开始，通过略微修改解，来逐步提升性能。<br><img src="/picture/machine-learning/algorithm12.png" alt="algorithm"><br>　　提升策略的基本套路如上图所示。<br>　　这里的关键是如何定义s的领域(neighbourhood)s’。我们的目标是尽量少修改s来变成s’。一种定义方法是如下：<br><img src="/picture/machine-learning/algorithm13.png" alt="algorithm"><br>　　下面是一个例子：<br><img src="/picture/machine-learning/algorithm14.png" alt="algorithm"></p>
<h3 id="Method3-BackTracking-an-intelligent-enumeration-strategy"><a href="#Method3-BackTracking-an-intelligent-enumeration-strategy" class="headerlink" title="Method3:BackTracking:an intelligent enumeration strategy"></a>Method3:BackTracking:an intelligent enumeration strategy</h3><p>　　我们注意到，任意一个解都可以被表示成一个由边构成的序列。<br><img src="/picture/machine-learning/algorithm15.png" alt="algorithm"><br>　　回溯树的结构如下：<br><img src="/picture/machine-learning/algorithm16.png" alt="algorithm"><br>　　注意这里面使用到了预测方法，基于如下两个准则：1）如果去掉连接(i,j)点的边会导致节点i(or j)连接的边少于两条，那么这条边必须包含。(有进必有出)。2）如果增加lianjie(i,j)点的边会导致节点i(or j)连接的边多于两条(只能进一次,出一次)，那么这条边一定不能包含。<br><img src="/picture/machine-learning/algorithm18.png" alt="algorithm"><br>　　算法如下：<br><img src="/picture/machine-learning/algorithm17.png" alt="algorithm"><br>　　回溯法的问题是会遍历所有可能的解，这样会导致解空间太大。一种解决方法是使用剪枝策略，对每一个节点，我们计算该节点代价的最小值。如果某个节点代价大于目前最优解，那么该节点及其下面的所有点就不需要进行遍历。<br><img src="/picture/machine-learning/algorithm19.png" alt="algorithm"><br>　　对每个节点的下界计算方法如下：<br><img src="/picture/machine-learning/algorithm21.jpg" alt="algorithm"><br>　　修改完的代码如下：<br><img src="/picture/machine-learning/algorithm22.png" alt="algorithm"><br>　　１个实例如下：<br><img src="/picture/machine-learning/algorithm23.png" alt="algorithm"><br>　　对于P2节点，由于e1边没有选，对于a节点而言，下界变成了(2+4)/2;对于b节点而言，因为e1边不能选，下界变成了(3+4)/2。相对于P0节点而言，代价总共增加了1。又比如对于P4节点，由于e2不能选，对于c节点而言，下界变成了(4+5)/2，相对PO总共增加了0.5。</p>
<p>　　</p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;　　本文是针对卜东波老师算法设计与分析第一课做的课堂笔记。第一课主要介绍算法的概念以及一些代表性的问题。&lt;br&gt;
    
    </summary>
    
      <category term="算法设计与分析" scheme="xtf615.com/categories/%E7%AE%97%E6%B3%95%E8%AE%BE%E8%AE%A1%E4%B8%8E%E5%88%86%E6%9E%90/"/>
    
    
      <category term="算法设计与分析" scheme="xtf615.com/tags/%E7%AE%97%E6%B3%95%E8%AE%BE%E8%AE%A1%E4%B8%8E%E5%88%86%E6%9E%90/"/>
    
      <category term="课堂笔记" scheme="xtf615.com/tags/%E8%AF%BE%E5%A0%82%E7%AC%94%E8%AE%B0/"/>
    
  </entry>
  
  <entry>
    <title>使用Apriori进行关联分析</title>
    <link href="xtf615.com/2017/09/12/%E4%BD%BF%E7%94%A8Apriori%E8%BF%9B%E8%A1%8C%E5%85%B3%E8%81%94%E5%88%86%E6%9E%90/"/>
    <id>xtf615.com/2017/09/12/使用Apriori进行关联分析/</id>
    <published>2017-09-12T12:13:47.000Z</published>
    <updated>2017-09-17T07:07:38.884Z</updated>
    
    <content type="html"><![CDATA[<p>　　未完待续。。。</p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;　　未完待续。。。&lt;/p&gt;

    
    </summary>
    
      <category term="关联分析" scheme="xtf615.com/categories/%E5%85%B3%E8%81%94%E5%88%86%E6%9E%90/"/>
    
    
      <category term="Apriori" scheme="xtf615.com/tags/Apriori/"/>
    
      <category term="关联分析" scheme="xtf615.com/tags/%E5%85%B3%E8%81%94%E5%88%86%E6%9E%90/"/>
    
  </entry>
  
  <entry>
    <title>Python核心编程读书笔记</title>
    <link href="xtf615.com/2017/09/07/Python%E6%A0%B8%E5%BF%83%E7%BC%96%E7%A8%8B%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0/"/>
    <id>xtf615.com/2017/09/07/Python核心编程读书笔记/</id>
    <published>2017-09-07T12:05:12.000Z</published>
    <updated>2017-09-17T12:26:59.705Z</updated>
    
    <content type="html"><![CDATA[<p>　　本文是Python核心编程的读书笔记。<br><a id="more"></a></p>
<h1 id="Chapter-1-Welcome-to-Python"><a href="#Chapter-1-Welcome-to-Python" class="headerlink" title="Chapter 1 Welcome to Python"></a>Chapter 1 Welcome to Python</h1><h2 id="什么是Python"><a href="#什么是Python" class="headerlink" title="什么是Python"></a>什么是Python</h2><p>　　<strong>Python is an elegant and robust programming language that delivers both the power and general applicability of traditional compiled languages with the ease of use of simpler scripting and interpreted languages.</strong></p>
<h2 id="起源"><a href="#起源" class="headerlink" title="起源"></a>起源</h2><p>　　贵泽.范.罗萨姆(Guido van Rossum)于1989年底创造了Python.1991年，Python发布了第一个公开发行版。</p>
<h2 id="特点"><a href="#特点" class="headerlink" title="特点"></a>特点</h2><ul>
<li>高级语言(High Level)</li>
<li>面向对象(Object Oriented)</li>
<li>可升级(Scalable)：提供基本开发模块，可在此之上开发你的软件，当需要扩展和增长时，Python可插入性和模块化机构使得项目易于管理。</li>
<li>可扩展(Extensible)：可用其他语言编写Python扩展。如CPython和Jython。</li>
<li>可移植性(Portable)</li>
<li>易学(Easy to Learn)</li>
<li>易读(Easy to Read)</li>
<li>易维护(Easy to Maintain)</li>
<li>健壮性(Robust): 错误处理机制</li>
<li>高效快速的原型开发工具(Effective as a Rapid Prototyping Tool)：拥有面向其它系统接口，标准库和第三方模块完备。</li>
<li>内存管理器(A Memory Manager)</li>
<li>解释性和(字节)编译性:解释型语言，但是Python实际上是字节编译的(生成带有.pyc或.pyo的扩展名文件)，可以生成一种近似机器语言的中间形式。兼顾了编译型语言的性能和解释型语言的优点。</li>
</ul>
<h2 id="Python和其他语言比较"><a href="#Python和其他语言比较" class="headerlink" title="Python和其他语言比较"></a>Python和其他语言比较</h2><h3 id="Python-VS-Perl"><a href="#Python-VS-Perl" class="headerlink" title="Python VS Perl"></a>Python VS Perl</h3><p>　　同：都是脚本语言.<br>　　异：Perl最大的优势在于它的字符串模式匹配能力，提供了强大的正则表达式匹配引擎，使得Perl实际上成为了一种用于过滤、识别和抽取字符串文本的语言。一直是开发web服务器端通用网关接口CGI网络程序最流行的语言，也是目前Linux中大多数脚本使用的语言。然而Perl语言晦涩、对符号语法过度使用，解读很困难。Pyhon适合开发大程序，吸收了Perl语言特点，也拥有强大的正则化引擎，同时易学易用。</p>
<h3 id="Python-VS-Java"><a href="#Python-VS-Java" class="headerlink" title="Python VS Java"></a>Python VS Java</h3><p>　　同：面向对象。Jython开发联结了二者。Jython是用Java开发的Python解释器，即可以将Python代码解释成Java虚拟机懂的字节码，这意味着可以在只有Java虚拟机的环境中运行Python程序。在Jython脚本环境中，还可以处理Java对象，可以访问Java的标准库。<br>　　异：Java繁琐，Python简洁。</p>
<h3 id="Python-VS-Ruby"><a href="#Python-VS-Ruby" class="headerlink" title="Python VS Ruby"></a>Python VS Ruby</h3><p>　　同：脚本语言。<br>　　异：Python是多编程范式混合，Ruby完全面向对象。Python有一个字节码解释器，Ruby没有。Python更加易读，Ruby可看作是面向对象的Perl。相对于Ruby on Rails,Python有自己的Web应用框架，比如Django和Turbogears.</p>
<p>　　可参考，<a href="https://wiki.python.org/moin/LanguageComparisons" target="_blank" rel="external">Python与其他语言比较</a></p>
<h2 id="其它实现"><a href="#其它实现" class="headerlink" title="其它实现"></a>其它实现</h2><p>　　标准版的Python是用C来编译的，又叫CPython。除此之外，还有一些其它的Python实现。</p>
<h3 id="Java"><a href="#Java" class="headerlink" title="Java"></a>Java</h3><p>　　前面也有提到过。使用Java语言写成的Python解释器Jython。Jython优势：</p>
<ul>
<li>只要有Java虚拟机，就能运行Jython。</li>
<li>拥有访问Java包宇类库的能力。</li>
<li>为Java开发环境提供了脚本引擎。</li>
<li>能够很容易的测试Java类库。</li>
<li>提供访问Java原生异常处理的能力。</li>
<li>继承了JavaBeans特性和内省能力。</li>
<li>鼓励Python到Java的开发。</li>
<li>GUI开发人员可以访问Java的AWT/SWing库</li>
<li>利用了Java原生垃圾收集器。<h3 id="NET"><a href="#NET" class="headerlink" title=".NET"></a>.NET</h3>　　IronPython的Python实现，可以在一个.NET应用程序中整合IronPython解释器来访问.NET对象。<h3 id="Stackless"><a href="#Stackless" class="headerlink" title="Stackless"></a>Stackless</h3>　　CPython的一个局限就是每个Python函数调用都会产生一个C函数调用。这意味着同时产生的函数调用时有限制的，因此Python难以实现用户级的线程库和复杂递归应用，一旦超越这个限制，程序就会崩溃。可以通过使用一个stackless的Python实现来突破这个限制，使得一个C栈帧可以拥有任意数量的Python栈帧，这样就能够拥有无穷的函数调用，并能支持巨大数量的线程。　</li>
</ul>
<h1 id="Chapter-2-Getting-Started"><a href="#Chapter-2-Getting-Started" class="headerlink" title="Chapter 2 Getting Started"></a>Chapter 2 Getting Started</h1><h2 id="print输出"><a href="#print输出" class="headerlink" title="print输出"></a>print输出</h2><p>　　通常当你想看变量内容时，会在代码中使用print语句输出。不过在交互式解释器中，可以使用print语句显示变量的字符串表示，或者仅使用变量名查看该变量的原始值。<br><img src="/picture/machine-learning/python-note1.png" alt="python note"><br>　　注意，在仅用变量名时，输出的字符串被用单引号括起来了。这是为了让非字符串对象也能以字符串的方式显示在屏幕上-即它显示的是该对象的字符串表示，而不仅仅是字符串本身。究其本质，print语句调用str()函数显示对象，而交互式解释器调用repr()函数显示对象。<br>　　下划线(_)在解释器中有特别的含义，表示最后一个表达式的值。<br>　　Python的print语句，与字符串格式运算符(%)结合使用，可实现字符串替换功能。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">&gt;&gt;&gt;<span class="keyword">print</span> <span class="string">"%s is number %d!"</span> % (<span class="string">"Python"</span>, <span class="number">1</span>)</div><div class="line">Python <span class="keyword">is</span> number <span class="number">1</span>!</div></pre></td></tr></table></figure></p>
<p>　　%s替换字符串，%d替换整数，%f替换浮点数。<br>　　Print语句支持将输出重定向到文件。使用符号&gt;&gt;重定向输出。如下是将输出重定向到标准错误输出：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> sys</div><div class="line"><span class="keyword">print</span> &gt;&gt; sys.stderr, <span class="string">'Fatal error: invalid input!'</span></div></pre></td></tr></table></figure></p>
<p>　　下面将输出重定向到日志文件的例子：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">logfile = open(<span class="string">'/tmp/mulog.txt'</span>,<span class="string">'a'</span>)</div><div class="line"><span class="keyword">print</span> &gt;&gt; logfile, <span class="string">'Fatal error:invalid input!'</span></div><div class="line">logfile.close()</div></pre></td></tr></table></figure></p>
<h2 id="raw-input输入"><a href="#raw-input输入" class="headerlink" title="raw_input输入"></a>raw_input输入</h2><p>　　raw_input()读取标准输入，并将读到的数据赋值给指定的变量，但是该变量是字符串，可以使用int()等函数进行类型转换。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">&gt;&gt;&gt;user = raw)input(<span class="string">'Enter login name:'</span>)</div><div class="line">Enter login name: root</div><div class="line"><span class="meta">&gt;&gt;&gt; </span>Print <span class="string">'Your login is:'</span>,user</div><div class="line">Your login <span class="keyword">is</span>: root</div></pre></td></tr></table></figure></p>
<h2 id="help帮助"><a href="#help帮助" class="headerlink" title="help帮助"></a>help帮助</h2><p>　　在学习Python过程中，如果需要得到一个生疏函数的帮助，只需要对它调用help(函数名)就可以得到帮助信息。</p>
<h2 id="注释"><a href="#注释" class="headerlink" title="注释"></a>注释</h2><p>　　使用# 注释单行。<br>　　有一种叫做文档字符串的特别注释。可以在模块、类或者函数的起始添加一个字符串，起到在线文档的功能。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">foo</span><span class="params">()</span>:</span></div><div class="line">  <span class="string">"This is a doc string"</span></div><div class="line">  <span class="keyword">return</span> <span class="keyword">True</span></div></pre></td></tr></table></figure>
<p> 　　<br>　　与普通注释不同，文档字符串可以在运行时访问，也可以用来自动生成文档。</p>
<h2 id="数字"><a href="#数字" class="headerlink" title="数字"></a>数字</h2><p>　　Python五种基本数字类型：</p>
<ul>
<li>int(有符号整数)</li>
<li>long(长整数)：超过C语言中的long，类似Java的BigInteger</li>
<li>bool(布尔值)</li>
<li>float(浮点值)</li>
<li>complex(复数)</li>
</ul>
<h2 id="字符串"><a href="#字符串" class="headerlink" title="字符串"></a>字符串</h2><p>　　支持成对单引号或双引号；也支持三引号(三个连续的单引号或双引号)，可以用来包含特殊字符。<br>　　使用索引运算符([])和切片运算符([:])可以得到子字符串。第一个字符索引为0，最后一个字符索引为-1。<br>　　*号可以用于字符串重复。例如：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line"><span class="meta">&gt;&gt;&gt; </span><span class="string">"python"</span> * <span class="number">2</span></div><div class="line"><span class="string">'pythonpython'</span></div></pre></td></tr></table></figure></p>
<h2 id="列表和元组"><a href="#列表和元组" class="headerlink" title="列表和元组"></a>列表和元组</h2><p>　　可以将列表和元组当作普通的数组，可以保存任意数量任意类型的Python对象，唯一不同的是列表和元组可以同时存储不同类型的对象。<br>　　列表和元组几处重要的区别：列表元素用中括号([])包裹，元素的个数及元素的值可以改变。元组元素用小括号(())包裹，不可以更改(尽管内容可以更改)。元组可以看成是只读的列表，通过切片运算([:])可以得到子集。</p>
<h2 id="字典"><a href="#字典" class="headerlink" title="字典"></a>字典</h2><p>　　字典用于映射数据，键值对构成。几乎所有类型的Python对象都可以用作键，不过一般以字符串或数字字符串最为常用，值可以用任意Python对象，字典元素用大括号({})包裹。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div></pre></td><td class="code"><pre><div class="line">&gt;&gt;&gt;aDict=&#123;<span class="string">'host'</span>:<span class="string">'earth'</span>&#125; <span class="comment"># create dict</span></div><div class="line">&gt;&gt;&gt;aDict[<span class="string">'port'</span>]=<span class="number">80</span> <span class="comment"># add to dict </span></div><div class="line">&gt;&gt;&gt;aDict</div><div class="line">&#123;<span class="string">'host'</span>:<span class="string">'earth'</span>,<span class="string">'port'</span>:<span class="number">80</span>&#125;</div><div class="line">&gt;&gt;&gt;aDict.keys()</div><div class="line">[<span class="string">'host'</span>,<span class="string">'[port'</span>]</div><div class="line">&gt;&gt;&gt;<span class="keyword">for</span> key <span class="keyword">in</span> aDict:</div><div class="line">       <span class="keyword">print</span> key,aDict[key]</div><div class="line">host earth</div><div class="line">port <span class="number">80</span></div></pre></td></tr></table></figure></p>
<h2 id="for循环和range-、enumerate"><a href="#for循环和range-、enumerate" class="headerlink" title="for循环和range()、enumerate()"></a>for循环和range()、enumerate()</h2><p>　　for循环类似foreach功能，不能循环索引。要循环索引可以使用range(len(list))。<br>如果既要循环索引，又要循环元素，恶意使用enumerate()。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line">&gt;&gt;&gt;foo = [<span class="string">'a'</span>,<span class="string">'b'</span>,<span class="string">'c'</span>]</div><div class="line">&gt;&gt;&gt;<span class="keyword">for</span> i,ch <span class="keyword">in</span> enumerate(foo):</div><div class="line">    <span class="keyword">print</span> ch, <span class="string">'(%d)'</span>  % i</div><div class="line">a (<span class="number">0</span>)</div><div class="line">b (<span class="number">1</span>)</div><div class="line">c (<span class="number">2</span>)</div></pre></td></tr></table></figure></p>
<h2 id="文件和内建函数open"><a href="#文件和内建函数open" class="headerlink" title="文件和内建函数open()"></a>文件和内建函数open()</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">handle = open(file_name, access_mode=<span class="string">'r'</span>) <span class="comment">#access mode:'r','w','a','b'</span></div><div class="line"><span class="keyword">for</span> eachLine <span class="keyword">in</span> handle:</div><div class="line">    <span class="keyword">print</span> eachLine</div><div class="line">handle.close()</div></pre></td></tr></table></figure>
<h2 id="错误和异常"><a href="#错误和异常" class="headerlink" title="错误和异常"></a>错误和异常</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">try</span>:</div><div class="line">	handle = open(file_name, access_mode=<span class="string">'r'</span>) <span class="comment">#access mode:'r','w','a','b'</span></div><div class="line">	<span class="keyword">for</span> eachLine <span class="keyword">in</span> handle:</div><div class="line">	    <span class="keyword">print</span> eachLine, handle.close() <span class="comment"># error</span></div><div class="line"><span class="keyword">except</span> IOError, e:</div><div class="line">	<span class="keyword">print</span> <span class="string">'file open error:'</span>,e</div></pre></td></tr></table></figure>
<p>　　也可以通过raise语句故意引发一个异常。</p>
<h2 id="类"><a href="#类" class="headerlink" title="类"></a>类</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">ClassName</span><span class="params">(base_class[es])</span>:</span></div><div class="line">    <span class="string">"optional documentation string"</span></div><div class="line">    static_member_declarations</div><div class="line">    method_declarations</div></pre></td></tr></table></figure>
<p>　　使用class关键字定义类，可以提供一个可选的父类，如果没有合适的父类，那就使用object作为父类。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div></pre></td><td class="code"><pre><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">FooClass</span><span class="params">(object)</span>:</span></div><div class="line">    <span class="string">"""my very first class: FooClass"""</span></div><div class="line">    version = <span class="number">0.1</span> <span class="comment"># class (data) attribute</span></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, nm=<span class="string">'John Doe'</span>)</span>:</span></div><div class="line">        <span class="string">"""constructor"""</span></div><div class="line">        self.name = nm <span class="comment"># class instance (data) attribute</span></div><div class="line">        <span class="keyword">print</span> <span class="string">'Created a class instance for'</span>, nm</div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">showname</span><span class="params">(self)</span>:</span></div><div class="line">        <span class="string">"""display instance attribute and class name"""</span></div><div class="line">        <span class="keyword">print</span> <span class="string">'Your name is'</span>, self.name</div><div class="line">        <span class="keyword">print</span> <span class="string">'My name is'</span>, self.__class__.__name__</div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">showver</span><span class="params">(self)</span>:</span></div><div class="line">        <span class="string">"""display class(static) attribute"""</span></div><div class="line">        <span class="keyword">print</span> self.version <span class="comment"># references FooClass.version</span></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">addMe2Me</span><span class="params">(self, x)</span>:</span> <span class="comment"># does not use 'self'</span></div><div class="line">        <span class="string">"""apply + operation to argument"""</span></div><div class="line">        <span class="keyword">return</span> x + x</div><div class="line"><span class="comment"># 创建类实例</span></div><div class="line">&gt;&gt;&gt;foo1 = FooClass()</div><div class="line">Created a <span class="class"><span class="keyword">class</span> <span class="title">instance</span> <span class="title">for</span> <span class="title">John</span> <span class="title">Doe</span></span></div></pre></td></tr></table></figure></p>
<p>　　__init__可以当成构造函数，不过不像其他语言的构造函数，它并不会创建实例，它仅仅是你的对象创建后执行的第一个方法，做一些必要的初始化工作。__init__有个默认参数self。<br>　　self.__class__.name显示类名。</p>
<h2 id="模块"><a href="#模块" class="headerlink" title="模块"></a>模块</h2><p>　　一个Python源文件就是一个模块，模块不带.py后缀。可以使用import语句导入都其他模块中使用。</p>
<h2 id="实用的函数"><a href="#实用的函数" class="headerlink" title="实用的函数"></a>实用的函数</h2><ul>
<li>dir([obj]):显示对象的属性，如果没有提供参数，则显示全局变量的名字。</li>
<li>help([obj]):以一种整齐美观的形式显示对象的文档字符串，如果没有提供任何参<br>数，则会进入交互式帮助。</li>
<li>int(obj): 对象转整数</li>
<li>len(obj): 对象长度</li>
<li>open(fn,mode):打开文件</li>
<li>range([start,]stop[,step]):返回一个整数列表，起始值为start，结束值为stop-1,步阶是step。start默认为0，step默认为1.</li>
<li>raw_input(str):等待用户输入一个字符串，str用于提示信息。</li>
<li>str(obj): 对象转字符串</li>
<li>type(obj): 返回对象的类型，返回值是一个type对象。</li>
</ul>
<h1 id="Chapter-3-Python基础"><a href="#Chapter-3-Python基础" class="headerlink" title="Chapter 3 Python基础"></a>Chapter 3 Python基础</h1><h2 id="语句和语法"><a href="#语句和语法" class="headerlink" title="语句和语法"></a>语句和语法</h2><p>　　#是注释，\是换行，;可用于同一行书写多个表达式</p>
<h2 id="赋值"><a href="#赋值" class="headerlink" title="赋值"></a>赋值</h2><p>　　多元赋值:<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">&gt;&gt;&gt;x, y, z= <span class="number">1</span>, <span class="number">2</span>, <span class="string">'a'</span><span class="comment">#实际上是元组赋值，即(x, y, z)=(1, 2, 'a')</span></div></pre></td></tr></table></figure></p>
<h2 id="下划线标识符"><a href="#下划线标识符" class="headerlink" title="下划线标识符"></a>下划线标识符</h2><ul>
<li>_xxx:可以看作是私有的，不能通过”from module import *”导入。</li>
<li>__xxx__:系统定义名字</li>
<li>__xxx：类中的私有变量名</li>
</ul>
<h2 id="基本风格指南"><a href="#基本风格指南" class="headerlink" title="基本风格指南"></a>基本风格指南</h2><p>　　python提供了一个机制，可以通过__doc__特别变量，动态获得文档字串。在模块，类声明，或函数声明中<strong>第一个没有赋值的字符串</strong>可以通过属性obj.__doc__来进行访问，其中obj是一个模块，类或函数的名字。<br>　　使用4个空格缩进。</p>
<h2 id="内存管理"><a href="#内存管理" class="headerlink" title="内存管理"></a>内存管理</h2><p>　　引用计数。记录所有使用中的对象各有多个引用。引用计数为0时，它被垃圾回收。</p>
<h3 id="引用计数增加"><a href="#引用计数增加" class="headerlink" title="引用计数增加"></a>引用计数增加</h3><p>　　当对象被创建并将其引用赋值给变量时，该对象的引用计数设置为1.<br>　　当同一个对象的引用又被赋值给其他变量时，或作为参数传递给函数，方法或类实例时，或者被赋值为一个窗口对象(list,tuple等)的成员时，该对象新的引用被创建，则该对象的引用计数加1。</p>
<h3 id="引用计数减少"><a href="#引用计数减少" class="headerlink" title="引用计数减少"></a>引用计数减少</h3><p>　　当对象的引用被销毁时，引用计数会减小。最明显的例子就是当引用离开其作用范围时，这种情况最经常出现在函数运行结束时，所有局部变量都自动销毁，对象的引用计数也随之减少。<br>　　当变量被赋值给另外一个对象时，原对象的引用计数也会自动减少1。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">foo = <span class="string">'xyz'</span></div><div class="line">bar = foo</div><div class="line">foo = <span class="number">123</span></div></pre></td></tr></table></figure></p>
<p>　　如上，当字符串对象“xyz”被创建并赋值给foo时，它的引用计数为，当增加一个别名bar时，引用计数变为2.不过当foo被重新赋值给整数对象123时，‘xyz’对象的引用计数自动减少1，又重新变成了1，此时只有bar是指向“xyz”。<br>　　其他造成对象引用计算减少的方式包括del语句删除一个变量。或者当一个对象被移出一个窗口对象时，或者该窗口对象本身的引用计数变成了0时。　　<br>　　总结一下一个对象的引用计数在以下情况会减少：</p>
<ul>
<li>一个本地引用离开了作用范围。比如foobar()函数结束时。</li>
<li>对象的别名被显示销毁。del y</li>
<li>对象的一个别名被赋值给其它的对象。 x=123</li>
<li>对象被从一个窗口对象中移除：myList.remove(x)</li>
<li>窗口对象本身被销毁：del myList<br>　　del看一个例子：<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">x=<span class="number">3.14</span> <span class="comment">#创建3.14对象并赋值给x</span></div><div class="line">y=x</div></pre></td></tr></table></figure>
</li>
</ul>
<p>　　执行del y会产生两个结果：1）从现在的名字空间中删除y。2）x的引用计数减少1。<br>　　引申一步，紧接着执行del x会删除该数值对象的最后一个引用，也就是该对象的引用计数会减少为0，这会导致该对象从此无法访问或无法抵达。从此刻起，该对象就成为垃圾回收机制的回收对象。注意<strong>任何追踪或调试程序会给一个对象增加一个额外的引用</strong>，这会推迟该对象被回收的时间。</p>
<h3 id="垃圾回收"><a href="#垃圾回收" class="headerlink" title="垃圾回收"></a>垃圾回收</h3><p>　　垃圾回收器会回收引用计数为0的对象，也负责检查那些虽然引用计数大于0但也应该被销毁的对象，特定情形会导致循环引用。<br>　　一个循环引用发生在当你有至少两个对象相互引用时，也就是所有的引用都消失了，这些引用仍然存在，这说明只靠计数是不够的。Python垃圾回收器实际上是一个引用计数器和一个循环垃圾回收器。Python会留心被分配的总量很大（即未通过引用计数销毁的那些）的对象，这种情况下，解释器会暂停下来，试图清理所有未引用的循环。</p>
<h1 id="Chapter-4-Python对象"><a href="#Chapter-4-Python对象" class="headerlink" title="Chapter 4 Python对象"></a>Chapter 4 Python对象</h1><p>　　未完待续……</p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;　　本文是Python核心编程的读书笔记。&lt;br&gt;
    
    </summary>
    
      <category term="学习笔记" scheme="xtf615.com/categories/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"/>
    
    
      <category term="Python" scheme="xtf615.com/tags/Python/"/>
    
  </entry>
  
  <entry>
    <title>卷积神经网络实践</title>
    <link href="xtf615.com/2017/08/17/cnn-written/"/>
    <id>xtf615.com/2017/08/17/cnn-written/</id>
    <published>2017-08-17T01:35:34.000Z</published>
    <updated>2017-08-22T13:47:00.000Z</updated>
    
    <content type="html"><![CDATA[<p>　　本文将介绍使用Python书写“卷积神经网络”代码的具体步骤和细节，本文会采用Python开源库Theano，Theano封装了卷积等操作，使用起来比较方便。具体代码可参考<a href="http://neuralnetworksanddeeplearning.com/chap6.html" target="_blank" rel="external">神经网络和深度学习教程</a>。在本文中，卷积神经网络对MNIST手写数字的预测性能，测试集准确率可以达到99.30%。<br><a id="more"></a></p>
<h1 id="Theano安装"><a href="#Theano安装" class="headerlink" title="Theano安装"></a>Theano安装</h1><h2 id="Anaconda"><a href="#Anaconda" class="headerlink" title="Anaconda"></a>Anaconda</h2><p>　　Theano依赖于numpy等库，故首先安装集成包Anaconda，这个软件内嵌了python，并且还包含了很多用于计算的库。本文使用的是4.3.24版本，内置python为2.7.13版本。下载地址为：<a href="https://www.continuum.io/downloads" target="_blank" rel="external">https://www.continuum.io/downloads</a></p>
<h2 id="MinGW"><a href="#MinGW" class="headerlink" title="MinGW"></a>MinGW</h2><p>　　Theano底层依赖于C编译器。一种方式是，在控制台输入conda install mingw libpython即可。注意libpython也必须要安装，否则后面导入theano后会报“no module named gof”的错误。另一种也可以手动去MinGW安装包安装，但libpython仍然需要安装。</p>
<h2 id="Theano"><a href="#Theano" class="headerlink" title="Theano"></a>Theano</h2><p>　　最后安装Theano，使用pip install theano即可。</p>
<h1 id="数据集加载"><a href="#数据集加载" class="headerlink" title="数据集加载"></a>数据集加载</h1><p>　　本文使用的数据集同样是来自MNIST手写数字库。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span>  <span class="title">load_data_shared</span><span class="params">(filename=<span class="string">"../data/mnist.pkl.gz"</span>)</span>:</span></div><div class="line">    f = gzip.open(filename, <span class="string">'rb'</span>)</div><div class="line">    training_data, validation_data, test_data = cPickle.load(f)</div><div class="line">    f.close()</div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">shared</span><span class="params">(data)</span>:</span></div><div class="line">        shared_x = theano.shared(</div><div class="line">            np.asarray(data[<span class="number">0</span>], dtype=theano.config.floatX), borrow=<span class="keyword">True</span>)</div><div class="line">        shared_y = theano.shared(</div><div class="line">            np.asarray(data[<span class="number">1</span>], dtype=theano.config.floatX), borrow=<span class="keyword">True</span>)</div><div class="line">        <span class="keyword">return</span> shared_x, T.cast(shared_y, <span class="string">"int32"</span>)</div><div class="line">    <span class="keyword">return</span> [shared(training_data), shared(validation_data), shared(test_data)]</div></pre></td></tr></table></figure></p>
<p>　　这里使用了theano的shared变量，shared变量意味着shared_x和shared_y可以在不同的函数中共享。在使用GPU时，Theano能够很方便的拷贝数据到不同的GPU中。borrow=True代表了浅拷贝。</p>
<h1 id="卷积神经网络结构"><a href="#卷积神经网络结构" class="headerlink" title="卷积神经网络结构"></a>卷积神经网络结构</h1><p>　　本文采取的神经网络结构如下:<br><img src="/picture/machine-learning/cnn1.png" alt="cnn"><br>　　输入层是5万条手写数字图像，每条数据是28*28的像素矩阵。然后在右边插入一个卷积层，使用5*5的局部感受野，跨距为1，20个特征映射对输入层进行特征提取。因此得到20*(28-5+1)*(28-5+1)=20*24*24规格的卷积层，这意味着每一副图像经过特征映射后都会得到20副24*24的卷积结果图像。接着再插入一个最大值混合层，使用2*2的混合窗口来合并特征。因此得到规格为20*(24/2)*(24/2)=20*12*12规格的混合层。紧接着再插入一个全连接层，有100个神经元。最后是输出层，和全连接层采用全连接的方式，这里使用柔性最大值来求得激活值。</p>
<h1 id="运行代码"><a href="#运行代码" class="headerlink" title="运行代码"></a>运行代码</h1><p>　　让我们先总体看一下最终运行的代码：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">training_data,validation_data,test_data=load_data_shared()</div><div class="line">mini_batch_size= <span class="number">10</span></div><div class="line">net2 = Network([ConvPoolLayer(image_shape=(mini_batch_size,<span class="number">1</span>,<span class="number">28</span>,<span class="number">28</span>),filter_shape=(<span class="number">20</span>,<span class="number">1</span>,<span class="number">5</span>,<span class="number">5</span>),poolsize=(<span class="number">2</span>,<span class="number">2</span>)),</div><div class="line">               FullyConnectedLayer(n_in=<span class="number">20</span>*<span class="number">12</span>*<span class="number">12</span>,n_out=<span class="number">100</span>),SoftmaxLayer(n_in=<span class="number">100</span>,n_out=<span class="number">10</span>)],mini_batch_size)</div><div class="line">net2.SGD(training_data,<span class="number">60</span>,mini_batch_size,<span class="number">0.1</span>,validation_data,test_data)</div></pre></td></tr></table></figure></p>
<p>　　可以看到运行的代码包括加载数据集、构建神经网络结构、运行学习算法SGD。下面将介绍不同层的代码细节。</p>
<h1 id="卷积层和混合层"><a href="#卷积层和混合层" class="headerlink" title="卷积层和混合层"></a>卷积层和混合层</h1><p>　　下面将描述运行代码中的细节。首先介绍卷积层和混合层的代码。这里面将卷积层和混合层统一封装，使得代码更加紧凑。首先看一下代码：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div></pre></td><td class="code"><pre><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">ConvPoolLayer</span><span class="params">(object)</span>:</span></div><div class="line">    <span class="string">"""Used to create a combination of a convolutional and a max-pooling</span></div><div class="line">    layer.  A more sophisticated implementation would separate the</div><div class="line">    two, but for our purposes we'll always use them together, and it</div><div class="line">    simplifies the code, so it makes sense to combine them.</div><div class="line">    """</div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, filter_shape, image_shape, poolsize=<span class="params">(<span class="number">2</span>, <span class="number">2</span>)</span>,</span></span></div><div class="line">                 activation_fn=sigmoid):</div><div class="line">        <span class="string">"""`filter_shape` is a tuple of length 4, whose entries are the number</span></div><div class="line">        of filters, the number of input feature maps, the filter height, and the</div><div class="line">        filter width.</div><div class="line"></div><div class="line">        `image_shape` is a tuple of length 4, whose entries are the</div><div class="line">        mini-batch size, the number of input feature maps, the image</div><div class="line">        height, and the image width.</div><div class="line"></div><div class="line">        `poolsize` is a tuple of length 2, whose entries are the y and</div><div class="line">        x pooling sizes.</div><div class="line"></div><div class="line">        """</div><div class="line">        self.filter_shape = filter_shape</div><div class="line">        self.image_shape = image_shape</div><div class="line">        self.poolsize = poolsize</div><div class="line">        self.activation_fn=activation_fn</div><div class="line">        <span class="comment"># initialize weights and biases</span></div><div class="line">        n_out = (filter_shape[<span class="number">0</span>]*np.prod(filter_shape[<span class="number">2</span>:])/np.prod(poolsize))</div><div class="line">        self.w = theano.shared(</div><div class="line">            np.asarray(</div><div class="line">                np.random.normal(loc=<span class="number">0</span>, scale=np.sqrt(<span class="number">1.0</span>/n_out), size=filter_shape),</div><div class="line">                dtype=theano.config.floatX),</div><div class="line">            borrow=<span class="keyword">True</span>)</div><div class="line">        self.b = theano.shared(</div><div class="line">            np.asarray(</div><div class="line">                np.random.normal(loc=<span class="number">0</span>, scale=<span class="number">1.0</span>, size=(filter_shape[<span class="number">0</span>],)),</div><div class="line">                dtype=theano.config.floatX),</div><div class="line">            borrow=<span class="keyword">True</span>)</div><div class="line">        self.params = [self.w, self.b]</div><div class="line"></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">set_inpt</span><span class="params">(self, inpt, inpt_dropout, mini_batch_size)</span>:</span></div><div class="line">        self.inpt = inpt.reshape(self.image_shape)</div><div class="line">        conv_out = conv.conv2d(</div><div class="line">            input=self.inpt, filters=self.w, filter_shape=self.filter_shape,</div><div class="line">            image_shape=self.image_shape)</div><div class="line">        pooled_out = pool.pool_2d(</div><div class="line">            input=conv_out, ds=self.poolsize, ignore_border=<span class="keyword">True</span>)</div><div class="line">        self.output = self.activation_fn(</div><div class="line">            pooled_out + self.b.dimshuffle(<span class="string">'x'</span>, <span class="number">0</span>, <span class="string">'x'</span>, <span class="string">'x'</span>))</div><div class="line">        self.output_dropout = self.output <span class="comment"># no dropout in the convolutional layers</span></div></pre></td></tr></table></figure></p>
<h2 id="初始化"><a href="#初始化" class="headerlink" title="初始化　　"></a>初始化　　</h2><p>　　首先看一下初始化方法。其中参数filter_shape是一个四元组，由卷积核(filter)个数、输入特征集(input feature maps)个数、卷积核行数、卷积核的列数构成。这里的滤波器个数对应的是前面图中的20，也就是使用20个卷积核进行特征提取。行数和列数大小也就是局部感受野的大小。而输入特征集个数代表的是输入数据，例如一般图都有3个通道Channel，每个图初始有3张feature map(R,G,B)，对于一张图，其所有feature map用一个filter卷成一张feature map。用20个filter，则卷成20张feature map。本文的数据使用的是手写图像的灰度值，因此只有1个feature map。image_shape参数也是一个四元组，由随机梯度下降选择的样本量mini_batch_size，输入特征集个数，每张图的宽度和高度构成。poolsize是卷积层到混合层使用的混合窗口大小。<br>　　首先进行参数的保存，前面的层使用的是sigmoid激活函数。n_out在初始化权重的时候指定方差使用，在本文中n_out=125,具体该数值含义目前不是非常理解。np.random.normal(loc=0, scale=np.sqrt(1.0/n_out), size=filter_shape使用高斯分布初始化了一个四维数组，规格为(20L,1L,5L,5L),由于一个特征映射有5*5=25个参数，则20个特征映射<br>就有500个权重参数。np.random.normal(loc=0, scale=1.0, size=(filter_shape<a href="http://neuralnetworksanddeeplearning.com/chap6.html" target="_blank" rel="external">0</a>,))初始化偏置值，每个特征映射只需要对应1个偏置，20个特征映射需要20个偏置即可。可以看出这里的参数个数总共有520个。如果不使用共享权重的方式，而是使用全连接的话，若隐藏层有30个神经元，则有784*30+30=23550个参数，几乎比卷积层多了40倍的参数。使用shared变量包装w和b后，w就是一个TensorType<float64,4d>类型的变量,b是一个TensorType<float64,vector>变量。</float64,vector></float64,4d></p>
<h2 id="卷积操作"><a href="#卷积操作" class="headerlink" title="卷积操作"></a>卷积操作</h2><p>　　set_inpt方法会根据参数来计算当前层的输出。参数inpt是前一层的输出，参数inpt_dropout是经过弃权策略后的前一层的输出。self.inpt = inpt.reshape(self.image_shape)将输入数据按照(10L,1L,28L,28L)的规格进行重塑。10代表每个小批量有10条数据，1L代表只有1个输入特征集，28*28则代表图像的大小。<br>　　接着是卷积操作：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">conv_out = conv.conv2d(input=self.inpt, filters=self.w, filter_shape=self.filter_shape,image_shape=self.image_shape)</div></pre></td></tr></table></figure></p>
<p>　　使用的是theano.tensor.nnet.conv封装好的conv2d卷积操作方法。对于该方法的理解，下面通过一个例子来理解。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</div><div class="line"><span class="keyword">import</span> theano</div><div class="line"><span class="keyword">import</span> theano.tensor <span class="keyword">as</span> T</div><div class="line"><span class="keyword">from</span> theano <span class="keyword">import</span> function</div><div class="line"><span class="keyword">from</span> theano.tensor.nnet.conv <span class="keyword">import</span> conv2d</div><div class="line"></div><div class="line">inputs = T.tensor4(name=<span class="string">'input'</span>, dtype=<span class="string">'float64'</span>)<span class="comment">#输入时一个4维向量，[图片数量，RGB通道数，图片行数，图片列数]</span></div><div class="line"></div><div class="line"><span class="comment">#w_shp = (2, 2, 1, 2) # [卷积核数量，输入特征通道数，卷积核行数，卷积核列数]</span></div><div class="line">W = theano.shared(</div><div class="line">    np.asarray(</div><div class="line">        [</div><div class="line">            [[[<span class="number">1.</span>, <span class="number">1.</span>]],</div><div class="line">             [[<span class="number">1.</span>, <span class="number">1.</span>]]],</div><div class="line">            [[[<span class="number">2.</span>, <span class="number">2.</span>]],</div><div class="line">             [[<span class="number">1.</span>, <span class="number">1.</span>]]]</div><div class="line">        ],</div><div class="line">        dtype=inputs.dtype),</div><div class="line">    name=<span class="string">'W'</span>)<span class="comment">#第一维代表卷积核，第二维对应通道，每种通道都有相对应的权重</span></div><div class="line"></div><div class="line">conv_out = conv2d(inputs, W)<span class="comment"># 卷积操作</span></div><div class="line"></div><div class="line">f = theano.function([inputs], conv_out)</div><div class="line"></div><div class="line"><span class="comment">#img_shp = [1,2,1,5] #[多少张图片，RGB通道数，图片行数，图片列数]</span></div><div class="line">i = np.asarray([</div><div class="line">    [[[<span class="number">1.</span>, <span class="number">2.</span>, <span class="number">3.</span>, <span class="number">4.</span>, <span class="number">5.</span>]],</div><div class="line">     [[<span class="number">1.</span>, <span class="number">2.</span>, <span class="number">3.</span>, <span class="number">4.</span>, <span class="number">5.</span>]]]</div><div class="line">], dtype=<span class="string">'float64'</span>) <span class="comment">#只输入1张图片</span></div><div class="line">ii = f(i)</div><div class="line">print(i.shape)</div><div class="line">print(W.get_value().shape)</div><div class="line">print(ii)</div><div class="line">print(ii.shape)</div><div class="line"></div><div class="line"><span class="comment"># out result:</span></div><div class="line">(<span class="number">1L</span>, <span class="number">2L</span>, <span class="number">1L</span>, <span class="number">5L</span>)</div><div class="line">(<span class="number">2L</span>, <span class="number">2L</span>, <span class="number">1L</span>, <span class="number">2L</span>)</div><div class="line">[[[[  <span class="number">6.</span>  <span class="number">10.</span>  <span class="number">14.</span>  <span class="number">18.</span>]]</div><div class="line"></div><div class="line">  [[  <span class="number">9.</span>  <span class="number">15.</span>  <span class="number">21.</span>  <span class="number">27.</span>]]]]</div><div class="line">(<span class="number">1L</span>, <span class="number">2L</span>, <span class="number">1L</span>, <span class="number">4L</span>)</div></pre></td></tr></table></figure></p>
<p>　　上述使用两个卷积核，因此对于每幅图，有两个输出。对于第一个卷积结果[  6.  10.  14.  18.]，是如下计算得到的：<br>　　利用第一个卷积核前半部分[1,1]扫描第一个通道数据，根据线性组合\(wx+b\)，得到[1*1+1*2,1*2+1*3,1*3+1*4,1*4+1*5]=[3,5,7,9]；同理利用第一个卷积核后半部分[1,1]扫描得到[3,5,7,9];两个向量相加得到[6.  10.  14.  18.]<br>　　同样，对于第二个卷积结果[  9.  15.  21.  27.]。利用第二个卷积核扫描两个通道的数据，再相加即可得到。</p>
<h2 id="池化操作"><a href="#池化操作" class="headerlink" title="池化操作"></a>池化操作</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">pooled_out = pool.pool_2d(</div><div class="line">            input=conv_out, ds=self.poolsize, ignore_border=<span class="keyword">True</span>,mode=<span class="string">'max'</span>)</div></pre></td></tr></table></figure>
<p>　　对卷积结果进行池化。这里使用的是最大池化(混合)方法。使用poolsize大小的窗口扫描卷积后的结果，取poolsize范围内的最大值作为结果。我们可以把最大值混合看作一种网络询问是否有一个给定的特征在一个图像区域中。然后扔掉确切的位置信息。直观上，一旦一个特征被发现，它的确切位置并不如它相对于其它特征的位置重要。同时这有助于减少在以后的层所需的参数的数目。</p>
<h2 id="激活操作"><a href="#激活操作" class="headerlink" title="激活操作　"></a>激活操作　</h2><p>　　接着使用激活函数来求得输出。由于池化后的结果仍然是四维的TensorType<float64,4d>，b的结构是TensorType<float64,vector>,因此我们需要调整b的结构为4维。这里使用dimshuffle，它是用来改变一个数组张量结构的一个工具，我们需要利用这个工具将b张成4维结构。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">self.output = self.activation_fn(</div><div class="line">            pooled_out + self.b.dimshuffle(<span class="string">'x'</span>, <span class="number">0</span>, <span class="string">'x'</span>, <span class="string">'x'</span>))</div></pre></td></tr></table></figure></float64,vector></float64,4d></p>
<p>　　上式dimshuffle的参数可以’0，1或’x’。0代表原始的行数，1代表原始的列数，x代表增加1维。因此张量后的b为(1L,20L,1L,1L)。</p>
<h1 id="全连接层"><a href="#全连接层" class="headerlink" title="全连接层"></a>全连接层</h1><p>　　全连接层和之前讨论前馈神经网络中的隐藏层类似。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div></pre></td><td class="code"><pre><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">FullyConnectedLayer</span><span class="params">(object)</span>:</span></div><div class="line"></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, n_in, n_out, activation_fn=sigmoid, p_dropout=<span class="number">0.0</span>)</span>:</span></div><div class="line">        self.n_in = n_in</div><div class="line">        self.n_out = n_out</div><div class="line">        self.activation_fn = activation_fn</div><div class="line">        self.p_dropout = p_dropout</div><div class="line">        <span class="comment"># Initialize weights and biases</span></div><div class="line">        self.w = theano.shared(</div><div class="line">            np.asarray(</div><div class="line">                np.random.normal(</div><div class="line">                    loc=<span class="number">0.0</span>, scale=np.sqrt(<span class="number">1.0</span>/n_out), size=(n_in, n_out)),</div><div class="line">                dtype=theano.config.floatX),</div><div class="line">            name=<span class="string">'w'</span>, borrow=<span class="keyword">True</span>)</div><div class="line">        self.b = theano.shared(</div><div class="line">            np.asarray(np.random.normal(loc=<span class="number">0.0</span>, scale=<span class="number">1.0</span>, size=(n_out,)),</div><div class="line">                       dtype=theano.config.floatX),</div><div class="line">            name=<span class="string">'b'</span>, borrow=<span class="keyword">True</span>)</div><div class="line">        self.params = [self.w, self.b]</div><div class="line"></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">set_inpt</span><span class="params">(self, inpt, inpt_dropout, mini_batch_size)</span>:</span></div><div class="line">        self.inpt = inpt.reshape((mini_batch_size, self.n_in))</div><div class="line">        self.output = self.activation_fn(</div><div class="line">            (<span class="number">1</span>-self.p_dropout)*T.dot(self.inpt, self.w) + self.b)</div><div class="line">        self.y_out = T.argmax(self.output, axis=<span class="number">1</span>)</div><div class="line">        self.inpt_dropout = dropout_layer(</div><div class="line">            inpt_dropout.reshape((mini_batch_size, self.n_in)), self.p_dropout)</div><div class="line">        self.output_dropout = self.activation_fn(</div><div class="line">            T.dot(self.inpt_dropout, self.w) + self.b)</div><div class="line"></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">accuracy</span><span class="params">(self, y)</span>:</span></div><div class="line">        <span class="string">"Return the accuracy for the mini-batch."</span></div><div class="line">        <span class="keyword">return</span> T.mean(T.eq(y, self.y_out))</div></pre></td></tr></table></figure></p>
<h2 id="初始化-1"><a href="#初始化-1" class="headerlink" title="初始化　"></a>初始化　</h2><p>　　初始化参数包括该层神经元个数和后一层神经元个数。该层神经元个数n_in=20*12*12=2880,下一层神经元个数n_out=100。因此共有2880*100+100=288100个参数。这里激活函数仍然使用sigmoid。p_dropout指定了弃权概率，是为了防止过拟合。</p>
<h2 id="dropout弃权"><a href="#dropout弃权" class="headerlink" title="dropout弃权"></a>dropout弃权</h2><p>　　首先按照(mini_batch_size, self.n_in)调整输入结构，再计算激活值，计算激活值时使用到了弃权策略。弃权策略是为了防止过拟合，对于神经网络单元，按照一定的概率将其暂时从网络中丢弃。注意是暂时，对于随机梯度下降来说，由于是随机丢弃，故而每一个mini-batch都在训练不同的网络。具体过程如下所示：<br><img src="/picture/machine-learning/cnn2.png" alt="cnn"><br>　　这是原始的结构。特别地，假设我们有一个训练数据x和对应的目标输出y。通常我们会通过在网络中前向传播x，然后进行反向传播来确定对梯度的贡献。使用弃权技术，这个过程就改了。我们会随机（临时）地删除网络中部分的隐藏神经元，输入层和输出层的神经元保持不变。在此之后，我们会得到最终如下线条所示的网络。注意那些被弃权的神经元，即那些临时被删除的神经元，用虚圈表示在图中：<br><img src="/picture/machine-learning/cnn3.png" alt="cnn"><br>　　具体关于dropout的理解可参考<a href="http://blog.csdn.net/stdcoutzyx/article/details/49022443" target="_blank" rel="external">理解dropout</a>。<br>　　代码中的self.output和self.output_dropout是有区别的。<br>　　self.output是在<strong>测试阶段</strong>使用的，计算时需要乘以(1-self.p_dropout)。即在网络前向传播到输出层前时隐含层节点的输出值都要缩减到（1-v）倍。例如正常的隐层输出为a，此时需要缩减为a（1-v）。这里我的解释是：假设比例v=0.5，即在训练阶段，以0.5的比例忽略隐层节点；那么假设隐层有80个节点，每个节点输出值为1，那么此时只有40个节点正常工作；也就是说总的输出为40个1和40个0；输出总和为40；而在测试阶段，由于我们的权值已经训练完成，此时就不在按照0.5的比例忽略隐层输出，假设此时每个隐层的输出还是1，那么此时总的输出为80个1，明显比dropout训练时输出大一倍（由于dropout比例为0.5）；所以为了得到和训练时一样的输出结果，就缩减隐层输出为a（1-v）；即此时输出80个0.5，总和也为40.这样就使得测试阶段和训练阶段的输出“一致”了。可参考<a href="/picture/machine-learning/cnn3.png">机器学习——Dropout原理介绍</a>中测试阶段一节。<br>　　而self.output_dropout是在<strong>训练阶段</strong>使用的。self.output_dropout的计算是根据上述dropout定义来做的。使用伯努利分布生成掩码,来随机忽略部分神经元。公式参考如下：<br><img src="/picture/machine-learning/cnn4.png" alt="cnn"><br>　　具体代码如下：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">dropout_layer</span><span class="params">(layer, p_dropout)</span>:</span></div><div class="line">       srng = shared_randomstreams.RandomStreams(</div><div class="line">           np.random.RandomState(<span class="number">0</span>).randint(<span class="number">999999</span>))</div><div class="line">       mask = srng.binomial(n=<span class="number">1</span>, p=<span class="number">1</span>-p_dropout, size=layer.shape)</div><div class="line">       <span class="keyword">return</span> layer*T.cast(mask, theano.config.floatX)</div></pre></td></tr></table></figure></p>
<p>　　另外，还有一个问题，为什么只对全连接层应用弃权？<br>　　原则上我们可以在卷积层上应用一个类似的程序。但是，实际上那没必要：卷积层有相当大的先天的对于过度拟合的抵抗。原因是共享权重意味着卷积核被强制从整个图像中学习。这使他们不太可能去选择在训练数据中的局部特质。于是就很少有必要来应用其它规范化，例如弃权。</p>
<h1 id="输出层"><a href="#输出层" class="headerlink" title="输出层"></a>输出层</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div></pre></td><td class="code"><pre><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">SoftmaxLayer</span><span class="params">(object)</span>:</span></div><div class="line"></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, n_in, n_out, p_dropout=<span class="number">0.0</span>)</span>:</span></div><div class="line">        self.n_in = n_in</div><div class="line">        self.n_out = n_out</div><div class="line">        self.p_dropout = p_dropout</div><div class="line">        <span class="comment"># Initialize weights and biases</span></div><div class="line">        self.w = theano.shared(</div><div class="line">            np.zeros((n_in, n_out), dtype=theano.config.floatX),</div><div class="line">            name=<span class="string">'w'</span>, borrow=<span class="keyword">True</span>)</div><div class="line">        self.b = theano.shared(</div><div class="line">            np.zeros((n_out,), dtype=theano.config.floatX),</div><div class="line">            name=<span class="string">'b'</span>, borrow=<span class="keyword">True</span>)</div><div class="line">        self.params = [self.w, self.b]</div><div class="line"></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">set_inpt</span><span class="params">(self, inpt, inpt_dropout, mini_batch_size)</span>:</span></div><div class="line">        self.inpt = inpt.reshape((mini_batch_size, self.n_in))</div><div class="line">        self.output = softmax((<span class="number">1</span>-self.p_dropout)*T.dot(self.inpt, self.w) + self.b)</div><div class="line">        self.y_out = T.argmax(self.output, axis=<span class="number">1</span>)</div><div class="line">        self.inpt_dropout = dropout_layer(</div><div class="line">            inpt_dropout.reshape((mini_batch_size, self.n_in)), self.p_dropout)</div><div class="line">        self.output_dropout = softmax(T.dot(self.inpt_dropout, self.w) + self.b)</div><div class="line"></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">cost</span><span class="params">(self, net)</span>:</span></div><div class="line">        <span class="string">"Return the log-likelihood cost."</span></div><div class="line">        <span class="keyword">return</span> -T.mean(T.log(self.output_dropout)[T.arange(net.y.shape[<span class="number">0</span>]), net.y])</div><div class="line"></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">accuracy</span><span class="params">(self, y)</span>:</span></div><div class="line">        <span class="string">"Return the accuracy for the mini-batch."</span></div><div class="line">        <span class="keyword">return</span> T.mean(T.eq(y, self.y_out))</div></pre></td></tr></table></figure>
<p>　　输出层和全连接层大同小异。主要区别包括，使用softmax激活函数代替sigmoid以及使用对数似然代价函数代替交叉熵代价函数。</p>
<h2 id="柔性最大值"><a href="#柔性最大值" class="headerlink" title="柔性最大值"></a>柔性最大值</h2><p>　　柔性最大值公式如下：<br>$$a_j^L = \frac{e^{z_j^L}}{\sum_k e^{z_k^L}}$$<br>　　同时有：<br>$$\sum_j a_j^L = \frac{\sum_j e^{z_j^L}}{\sum_k e^{z_k^L}}=1$$<br>　　因此柔性最大值层的输出可以看作是一个概率分布。在MNIST分类问题中，可以将\(a_j^L\)理解为网络估计正确数字分类为\(j\)的概率。</p>
<h2 id="对数似然代价函数"><a href="#对数似然代价函数" class="headerlink" title="对数似然代价函数"></a>对数似然代价函数</h2><p>　　前面说到可以把柔性最大值的输出看作是一个概率分布，因此我们可以使用对数似然代价函数。根据：<br>$$\mathcal{L} (\theta=\{W,b\}, \mathcal{D}) = \sum_{i=0}^{|\mathcal{D}|} \log(P(Y=y^{(i)}|x^{(i)}, W,b)) \\\ \ell (\theta=\{W,b\}, \mathcal{D}) = - \mathcal{L} (\theta=\{W,b\}, \mathcal{D})$$<br>　　具体代码：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">loss = -T.mean(T.log(p_y_given_x)[T.arange(y.shape[<span class="number">0</span>]), y])</div><div class="line"><span class="comment"># note on syntax: T.arange(y.shape[0]) is a vector of integers [0,1,2,...,len(y)].</span></div><div class="line"><span class="comment"># Indexing a matrix M by the two vectors [0,1,...,K], [a,b,...,k] returns the</span></div><div class="line"><span class="comment"># elements M[0,a], M[1,b], ..., M[K,k] as a vector.  Here, we use this</span></div><div class="line"><span class="comment"># syntax to retrieve the log-probability of the correct labels, y.</span></div></pre></td></tr></table></figure></p>
<p>　　具体解释可参考上述注释部分。每次求得是一个mini-batch-size里的总代价。y的索引不一定是从0开始，因此使用[T.arange(y.shape[0]), y]来索引。<br>　　注意每次计算代价的时候，需要用到self.output_dropout，而self.output_dropout是在set_inpt定义的，cost和self.output_dropout都是预定义的符号运算，实际上每次计算代价时，都会进行前向传播计算出self.output_dropout。</p>
<h1 id="Network初始化"><a href="#Network初始化" class="headerlink" title="Network初始化"></a>Network初始化</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div></pre></td><td class="code"><pre><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">Network</span><span class="params">(object)</span>:</span></div><div class="line"></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, layers, mini_batch_size)</span>:</span></div><div class="line">        <span class="string">"""Takes a list of `layers`, describing the network architecture, and</span></div><div class="line">        a value for the `mini_batch_size` to be used during training</div><div class="line">        by stochastic gradient descent.</div><div class="line"></div><div class="line">        """</div><div class="line">        self.layers = layers</div><div class="line">        self.mini_batch_size = mini_batch_size</div><div class="line">        self.params = [param <span class="keyword">for</span> layer <span class="keyword">in</span> self.layers <span class="keyword">for</span> param <span class="keyword">in</span> layer.params]</div><div class="line">        self.x = T.matrix(<span class="string">"x"</span>)</div><div class="line">        self.y = T.ivector(<span class="string">"y"</span>)</div><div class="line">        init_layer = self.layers[<span class="number">0</span>]</div><div class="line">        init_layer.set_inpt(self.x, self.x, self.mini_batch_size)</div><div class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> xrange(<span class="number">1</span>, len(self.layers)):</div><div class="line">            prev_layer, layer  = self.layers[j<span class="number">-1</span>], self.layers[j]</div><div class="line">            layer.set_inpt(</div><div class="line">                prev_layer.output, prev_layer.output_dropout, self.mini_batch_size)</div><div class="line">        self.output = self.layers[<span class="number">-1</span>].output</div><div class="line">        self.output_dropout = self.layers[<span class="number">-1</span>].output_dropout</div></pre></td></tr></table></figure>
<p>　　self.x定义了数据的符号化矩阵，参数是名称；self.y定义了分类的符号化向量，参数同样是名称。接着定义符号化前向传播算法，来计算输出。<strong>实际上初始化的时候，没有代入具体实际数据来进行前向传播计算输出，而是定义了前向传播计算图，后面层的输入依赖于前面层的前向传播输出，这样后面计算某一层的输出时，直接调用该层的输出符号，即可自动根据计算图去计算出前面层的输出结果</strong>。</p>
<h1 id="学习算法"><a href="#学习算法" class="headerlink" title="学习算法"></a>学习算法</h1><p>　　最后，让我们关注下核心的学习算法SGD。这里面还会介绍theano符号化计算,Theano会将符号数学化计算表示成graph,之后实际运行时才开始真正的计算。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div><div class="line">64</div><div class="line">65</div><div class="line">66</div><div class="line">67</div><div class="line">68</div><div class="line">69</div><div class="line">70</div><div class="line">71</div><div class="line">72</div><div class="line">73</div><div class="line">74</div><div class="line">75</div><div class="line">76</div><div class="line">77</div><div class="line">78</div><div class="line">79</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">SGD</span><span class="params">(self, training_data, epochs, mini_batch_size, eta,</span></span></div><div class="line">        validation_data, test_data, lmbda=<span class="number">0.0</span>):</div><div class="line">    <span class="string">"""Train the network using mini-batch stochastic gradient descent."""</span></div><div class="line">    training_x, training_y = training_data</div><div class="line">    validation_x, validation_y = validation_data</div><div class="line">    test_x, test_y = test_data</div><div class="line"></div><div class="line">    <span class="comment"># compute number of minibatches for training, validation and testing</span></div><div class="line">    num_training_batches = size(training_data)/mini_batch_size</div><div class="line">    num_validation_batches = size(validation_data)/mini_batch_size</div><div class="line">    num_test_batches = size(test_data)/mini_batch_size</div><div class="line"></div><div class="line">    <span class="comment"># define the (regularized) cost function, symbolic gradients, and updates</span></div><div class="line">    l2_norm_squared = sum([(layer.w**<span class="number">2</span>).sum() <span class="keyword">for</span> layer <span class="keyword">in</span> self.layers])</div><div class="line">    cost = self.layers[<span class="number">-1</span>].cost(self)+\</div><div class="line">           <span class="number">0.5</span>*lmbda*l2_norm_squared/num_training_batches</div><div class="line">    grads = T.grad(cost, self.params)</div><div class="line">    updates = [(param, param-eta*grad)</div><div class="line">               <span class="keyword">for</span> param, grad <span class="keyword">in</span> zip(self.params, grads)]</div><div class="line"></div><div class="line">    <span class="comment"># define functions to train a mini-batch, and to compute the</span></div><div class="line">    <span class="comment"># accuracy in validation and test mini-batches.</span></div><div class="line">    i = T.lscalar() <span class="comment"># mini-batch index</span></div><div class="line">    train_mb = theano.function(</div><div class="line">        [i], cost, updates=updates,</div><div class="line">        givens=&#123;</div><div class="line">            self.x:</div><div class="line">            training_x[i*self.mini_batch_size: (i+<span class="number">1</span>)*self.mini_batch_size],</div><div class="line">            self.y:</div><div class="line">            training_y[i*self.mini_batch_size: (i+<span class="number">1</span>)*self.mini_batch_size]</div><div class="line">        &#125;)</div><div class="line">    validate_mb_accuracy = theano.function(</div><div class="line">        [i], self.layers[<span class="number">-1</span>].accuracy(self.y),</div><div class="line">        givens=&#123;</div><div class="line">            self.x:</div><div class="line">            validation_x[i*self.mini_batch_size: (i+<span class="number">1</span>)*self.mini_batch_size],</div><div class="line">            self.y:</div><div class="line">            validation_y[i*self.mini_batch_size: (i+<span class="number">1</span>)*self.mini_batch_size]</div><div class="line">        &#125;)</div><div class="line">    test_mb_accuracy = theano.function(</div><div class="line">        [i], self.layers[<span class="number">-1</span>].accuracy(self.y),</div><div class="line">        givens=&#123;</div><div class="line">            self.x:</div><div class="line">            test_x[i*self.mini_batch_size: (i+<span class="number">1</span>)*self.mini_batch_size],</div><div class="line">            self.y:</div><div class="line">            test_y[i*self.mini_batch_size: (i+<span class="number">1</span>)*self.mini_batch_size]</div><div class="line">        &#125;)</div><div class="line">    self.test_mb_predictions = theano.function(</div><div class="line">        [i], self.layers[<span class="number">-1</span>].y_out,</div><div class="line">        givens=&#123;</div><div class="line">            self.x:</div><div class="line">            test_x[i*self.mini_batch_size: (i+<span class="number">1</span>)*self.mini_batch_size]</div><div class="line">        &#125;)</div><div class="line">    <span class="comment"># Do the actual training</span></div><div class="line">    best_validation_accuracy = <span class="number">0.0</span></div><div class="line">    <span class="keyword">for</span> epoch <span class="keyword">in</span> xrange(epochs):</div><div class="line">        <span class="keyword">for</span> minibatch_index <span class="keyword">in</span> xrange(num_training_batches):</div><div class="line">            iteration = num_training_batches*epoch+minibatch_index</div><div class="line">            <span class="keyword">if</span> iteration % <span class="number">1000</span> == <span class="number">0</span>:</div><div class="line">                print(<span class="string">"Training mini-batch number &#123;0&#125;"</span>.format(iteration))</div><div class="line">            cost_ij = train_mb(minibatch_index)</div><div class="line">            <span class="keyword">if</span> (iteration+<span class="number">1</span>) % num_training_batches == <span class="number">0</span>:</div><div class="line">                validation_accuracy = np.mean(</div><div class="line">                    [validate_mb_accuracy(j) <span class="keyword">for</span> j <span class="keyword">in</span> xrange(num_validation_batches)])</div><div class="line">                print(<span class="string">"Epoch &#123;0&#125;: validation accuracy &#123;1:.2%&#125;"</span>.format(</div><div class="line">                    epoch, validation_accuracy))</div><div class="line">                <span class="keyword">if</span> validation_accuracy &gt;= best_validation_accuracy:</div><div class="line">                    print(<span class="string">"This is the best validation accuracy to date."</span>)</div><div class="line">                    best_validation_accuracy = validation_accuracy</div><div class="line">                    best_iteration = iteration</div><div class="line">                    <span class="keyword">if</span> test_data:</div><div class="line">                        test_accuracy = np.mean(</div><div class="line">                            [test_mb_accuracy(j) <span class="keyword">for</span> j <span class="keyword">in</span> xrange(num_test_batches)])</div><div class="line">                        print(<span class="string">'The corresponding test accuracy is &#123;0:.2%&#125;'</span>.format(</div><div class="line">                            test_accuracy))</div><div class="line">    print(<span class="string">"Finished training network."</span>)</div><div class="line">    print(<span class="string">"Best validation accuracy of &#123;0:.2%&#125; obtained at iteration &#123;1&#125;"</span>.format(</div><div class="line">        best_validation_accuracy, best_iteration))</div><div class="line">    print(<span class="string">"Corresponding test accuracy of &#123;0:.2%&#125;"</span>.format(test_accuracy))</div></pre></td></tr></table></figure></p>
<h2 id="正则化和梯度求解"><a href="#正则化和梯度求解" class="headerlink" title="正则化和梯度求解　"></a>正则化和梯度求解　</h2><p>首先求每个迭代期需要计算多少次mini-batch。接着定义正则化代价函数，这里使用的是L2正则化。紧接着定义符号化求梯度方法以及梯度更新方法。具体代码含义如下：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">grads = T.grad(cost, self.params)<span class="comment">#第一个参数是要求梯度的式子，第二个参数是要对什么参数求梯度</span></div><div class="line">updates = [(param, param-eta*grad) <span class="keyword">for</span> param, grad <span class="keyword">in</span> zip(self.params, grads)]<span class="comment">#更新</span></div></pre></td></tr></table></figure></p>
<p>　　注意上述都是符号化定义，并未实际运行。</p>
<h2 id="训练方法定义"><a href="#训练方法定义" class="headerlink" title="训练方法定义　　"></a>训练方法定义　　</h2><p>　　紧接着定义训练方法，使用theano.function方法。第一个参数是输入，即mini-batch的编号，使用的是long类型，即T.lscalar。第二个参数是返回值，即代价值。其他参数updates定义参数的更新，givens用于提供mini-batch训练数据及分类。<br>　　对于验证方法和测试方法，第一个参数仍然是输入编号，第二个参数是返回值，即正确率，updates参数用于提供mini-batch数据及分类。<br>　　上述都是属于符号化定义。下面开始实际的训练。</p>
<h2 id="实际训练"><a href="#实际训练" class="headerlink" title="实际训练"></a>实际训练</h2><p>　　最外层循环是epoch轮次，再里面一层是每个轮次需要运行的Mini_batch数目。首先，计算目前为止的迭代数，一个mini_batch的计算代表一次迭代。每1000次迭代就输出提示，按照一次训练mini-batch-size=10条数据，1000次迭代就有10000条数据经过了训练。每次训练cost_ij = train_mb(minibatch_index)，都会先进行前向传播计算输出，然后计算代价，进而计算梯度并更新参数。(iteration+1) % num_training_batches == 0代表一次轮次epoch结束。此时对验证集进行验证，验证过程中，会对验证集数据进行前向传播计算输出，具体计算符号定义是在set_inpt中定义的。如果此时验证集准确率比之前轮次的高，则使用得到的模型计算测试集准确率，测试集准确率计算同验证集。<br>　　最后输出最好的结果以及迭代次数。</p>
<h1 id="结果分析"><a href="#结果分析" class="headerlink" title="结果分析"></a>结果分析</h1><h2 id="迭代过程"><a href="#迭代过程" class="headerlink" title="迭代过程"></a>迭代过程</h2><p>　　尝试对比不采取弃权策略和采取弃权策略的结果。<br>　　下面是不采取弃权策略,即p_dropout=0的结果，截取了20个轮次。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div><div class="line">64</div><div class="line">65</div><div class="line">66</div><div class="line">67</div><div class="line">68</div><div class="line">69</div><div class="line">70</div><div class="line">71</div><div class="line">72</div><div class="line">73</div><div class="line">74</div><div class="line">75</div><div class="line">76</div><div class="line">77</div><div class="line">78</div><div class="line">79</div><div class="line">80</div><div class="line">81</div><div class="line">82</div><div class="line">83</div><div class="line">84</div><div class="line">85</div><div class="line">86</div><div class="line">87</div><div class="line">88</div><div class="line">89</div><div class="line">90</div><div class="line">91</div><div class="line">92</div><div class="line">93</div><div class="line">94</div><div class="line">95</div><div class="line">96</div><div class="line">97</div><div class="line">98</div><div class="line">99</div><div class="line">100</div><div class="line">101</div><div class="line">102</div><div class="line">103</div><div class="line">104</div><div class="line">105</div><div class="line">106</div><div class="line">107</div><div class="line">108</div><div class="line">109</div><div class="line">110</div><div class="line">111</div><div class="line">112</div><div class="line">113</div><div class="line">114</div><div class="line">115</div><div class="line">116</div><div class="line">117</div><div class="line">118</div><div class="line">119</div><div class="line">120</div><div class="line">121</div><div class="line">122</div><div class="line">123</div><div class="line">124</div><div class="line">125</div><div class="line">126</div><div class="line">127</div><div class="line">128</div><div class="line">129</div><div class="line">130</div><div class="line">131</div><div class="line">132</div><div class="line">133</div><div class="line">134</div><div class="line">135</div><div class="line">136</div><div class="line">137</div><div class="line">138</div><div class="line">139</div><div class="line">140</div><div class="line">141</div><div class="line">142</div><div class="line">143</div><div class="line">144</div><div class="line">145</div><div class="line">146</div><div class="line">147</div><div class="line">148</div><div class="line">149</div><div class="line">150</div><div class="line">151</div><div class="line">152</div><div class="line">153</div><div class="line">154</div><div class="line">155</div><div class="line">156</div><div class="line">157</div><div class="line">158</div><div class="line">159</div><div class="line">160</div><div class="line">161</div><div class="line">162</div><div class="line">163</div><div class="line">164</div></pre></td><td class="code"><pre><div class="line">Training mini-batch number <span class="number">0</span></div><div class="line">Training mini-batch number <span class="number">1000</span></div><div class="line">Training mini-batch number <span class="number">2000</span></div><div class="line">Training mini-batch number <span class="number">3000</span></div><div class="line">Training mini-batch number <span class="number">4000</span></div><div class="line">Epoch <span class="number">0</span>: validation accuracy <span class="number">93.92</span>%</div><div class="line">This <span class="keyword">is</span> the best validation accuracy to date.</div><div class="line">The corresponding test accuracy <span class="keyword">is</span> <span class="number">93.25</span>%</div><div class="line">Training mini-batch number <span class="number">5000</span></div><div class="line">Training mini-batch number <span class="number">6000</span></div><div class="line">Training mini-batch number <span class="number">7000</span></div><div class="line">Training mini-batch number <span class="number">8000</span></div><div class="line">Training mini-batch number <span class="number">9000</span></div><div class="line">Epoch <span class="number">1</span>: validation accuracy <span class="number">96.26</span>%</div><div class="line">This <span class="keyword">is</span> the best validation accuracy to date.</div><div class="line">The corresponding test accuracy <span class="keyword">is</span> <span class="number">96.03</span>%</div><div class="line">Training mini-batch number <span class="number">10000</span></div><div class="line">Training mini-batch number <span class="number">11000</span></div><div class="line">Training mini-batch number <span class="number">12000</span></div><div class="line">Training mini-batch number <span class="number">13000</span></div><div class="line">Training mini-batch number <span class="number">14000</span></div><div class="line">Epoch <span class="number">2</span>: validation accuracy <span class="number">97.14</span>%</div><div class="line">This <span class="keyword">is</span> the best validation accuracy to date.</div><div class="line">The corresponding test accuracy <span class="keyword">is</span> <span class="number">96.93</span>%</div><div class="line">Training mini-batch number <span class="number">15000</span></div><div class="line">Training mini-batch number <span class="number">16000</span></div><div class="line">Training mini-batch number <span class="number">17000</span></div><div class="line">Training mini-batch number <span class="number">18000</span></div><div class="line">Training mini-batch number <span class="number">19000</span></div><div class="line">Epoch <span class="number">3</span>: validation accuracy <span class="number">97.62</span>%</div><div class="line">This <span class="keyword">is</span> the best validation accuracy to date.</div><div class="line">The corresponding test accuracy <span class="keyword">is</span> <span class="number">97.40</span>%</div><div class="line">Training mini-batch number <span class="number">20000</span></div><div class="line">Training mini-batch number <span class="number">21000</span></div><div class="line">Training mini-batch number <span class="number">22000</span></div><div class="line">Training mini-batch number <span class="number">23000</span></div><div class="line">Training mini-batch number <span class="number">24000</span></div><div class="line">Epoch <span class="number">4</span>: validation accuracy <span class="number">97.88</span>%</div><div class="line">This <span class="keyword">is</span> the best validation accuracy to date.</div><div class="line">The corresponding test accuracy <span class="keyword">is</span> <span class="number">97.73</span>%</div><div class="line">Training mini-batch number <span class="number">25000</span></div><div class="line">Training mini-batch number <span class="number">26000</span></div><div class="line">Training mini-batch number <span class="number">27000</span></div><div class="line">Training mini-batch number <span class="number">28000</span></div><div class="line">Training mini-batch number <span class="number">29000</span></div><div class="line">Epoch <span class="number">5</span>: validation accuracy <span class="number">98.08</span>%</div><div class="line">This <span class="keyword">is</span> the best validation accuracy to date.</div><div class="line">The corresponding test accuracy <span class="keyword">is</span> <span class="number">98.00</span>%</div><div class="line">Training mini-batch number <span class="number">30000</span></div><div class="line">Training mini-batch number <span class="number">31000</span></div><div class="line">Training mini-batch number <span class="number">32000</span></div><div class="line">Training mini-batch number <span class="number">33000</span></div><div class="line">Training mini-batch number <span class="number">34000</span></div><div class="line">Epoch <span class="number">6</span>: validation accuracy <span class="number">98.19</span>%</div><div class="line">This <span class="keyword">is</span> the best validation accuracy to date.</div><div class="line">The corresponding test accuracy <span class="keyword">is</span> <span class="number">98.18</span>%</div><div class="line">Training mini-batch number <span class="number">35000</span></div><div class="line">Training mini-batch number <span class="number">36000</span></div><div class="line">Training mini-batch number <span class="number">37000</span></div><div class="line">Training mini-batch number <span class="number">38000</span></div><div class="line">Training mini-batch number <span class="number">39000</span></div><div class="line">Epoch <span class="number">7</span>: validation accuracy <span class="number">98.28</span>%</div><div class="line">This <span class="keyword">is</span> the best validation accuracy to date.</div><div class="line">The corresponding test accuracy <span class="keyword">is</span> <span class="number">98.25</span>%</div><div class="line">Training mini-batch number <span class="number">40000</span></div><div class="line">Training mini-batch number <span class="number">41000</span></div><div class="line">Training mini-batch number <span class="number">42000</span></div><div class="line">Training mini-batch number <span class="number">43000</span></div><div class="line">Training mini-batch number <span class="number">44000</span></div><div class="line">Epoch <span class="number">8</span>: validation accuracy <span class="number">98.32</span>%</div><div class="line">This <span class="keyword">is</span> the best validation accuracy to date.</div><div class="line">The corresponding test accuracy <span class="keyword">is</span> <span class="number">98.34</span>%</div><div class="line">Training mini-batch number <span class="number">45000</span></div><div class="line">Training mini-batch number <span class="number">46000</span></div><div class="line">Training mini-batch number <span class="number">47000</span></div><div class="line">Training mini-batch number <span class="number">48000</span></div><div class="line">Training mini-batch number <span class="number">49000</span></div><div class="line">Epoch <span class="number">9</span>: validation accuracy <span class="number">98.34</span>%</div><div class="line">This <span class="keyword">is</span> the best validation accuracy to date.</div><div class="line">The corresponding test accuracy <span class="keyword">is</span> <span class="number">98.41</span>%</div><div class="line">Training mini-batch number <span class="number">50000</span></div><div class="line">Training mini-batch number <span class="number">51000</span></div><div class="line">Training mini-batch number <span class="number">52000</span></div><div class="line">Training mini-batch number <span class="number">53000</span></div><div class="line">Training mini-batch number <span class="number">54000</span></div><div class="line">Epoch <span class="number">10</span>: validation accuracy <span class="number">98.42</span>%</div><div class="line">This <span class="keyword">is</span> the best validation accuracy to date.</div><div class="line">The corresponding test accuracy <span class="keyword">is</span> <span class="number">98.45</span>%</div><div class="line">Training mini-batch number <span class="number">55000</span></div><div class="line">Training mini-batch number <span class="number">56000</span></div><div class="line">Training mini-batch number <span class="number">57000</span></div><div class="line">Training mini-batch number <span class="number">58000</span></div><div class="line">Training mini-batch number <span class="number">59000</span></div><div class="line">Epoch <span class="number">11</span>: validation accuracy <span class="number">98.44</span>%</div><div class="line">This <span class="keyword">is</span> the best validation accuracy to date.</div><div class="line">The corresponding test accuracy <span class="keyword">is</span> <span class="number">98.45</span>%</div><div class="line">Training mini-batch number <span class="number">60000</span></div><div class="line">Training mini-batch number <span class="number">61000</span></div><div class="line">Training mini-batch number <span class="number">62000</span></div><div class="line">Training mini-batch number <span class="number">63000</span></div><div class="line">Training mini-batch number <span class="number">64000</span></div><div class="line">Epoch <span class="number">12</span>: validation accuracy <span class="number">98.40</span>%</div><div class="line">Training mini-batch number <span class="number">65000</span></div><div class="line">Training mini-batch number <span class="number">66000</span></div><div class="line">Training mini-batch number <span class="number">67000</span></div><div class="line">Training mini-batch number <span class="number">68000</span></div><div class="line">Training mini-batch number <span class="number">69000</span></div><div class="line">Epoch <span class="number">13</span>: validation accuracy <span class="number">98.44</span>%</div><div class="line">Training mini-batch number <span class="number">70000</span></div><div class="line">Training mini-batch number <span class="number">71000</span></div><div class="line">Training mini-batch number <span class="number">72000</span></div><div class="line">Training mini-batch number <span class="number">73000</span></div><div class="line">Training mini-batch number <span class="number">74000</span></div><div class="line">Epoch <span class="number">14</span>: validation accuracy <span class="number">98.44</span>%</div><div class="line">This <span class="keyword">is</span> the best validation accuracy to date.</div><div class="line">The corresponding test accuracy <span class="keyword">is</span> <span class="number">98.40</span>%</div><div class="line">Training mini-batch number <span class="number">75000</span></div><div class="line">Training mini-batch number <span class="number">76000</span></div><div class="line">Training mini-batch number <span class="number">77000</span></div><div class="line">Training mini-batch number <span class="number">78000</span></div><div class="line">Training mini-batch number <span class="number">79000</span></div><div class="line">Epoch <span class="number">15</span>: validation accuracy <span class="number">98.46</span>%</div><div class="line">This <span class="keyword">is</span> the best validation accuracy to date.</div><div class="line">The corresponding test accuracy <span class="keyword">is</span> <span class="number">98.42</span>%</div><div class="line">Training mini-batch number <span class="number">80000</span></div><div class="line">Training mini-batch number <span class="number">81000</span></div><div class="line">Training mini-batch number <span class="number">82000</span></div><div class="line">Training mini-batch number <span class="number">83000</span></div><div class="line">Training mini-batch number <span class="number">84000</span></div><div class="line">Epoch <span class="number">16</span>: validation accuracy <span class="number">98.48</span>%</div><div class="line">This <span class="keyword">is</span> the best validation accuracy to date.</div><div class="line">The corresponding test accuracy <span class="keyword">is</span> <span class="number">98.42</span>%</div><div class="line">Training mini-batch number <span class="number">85000</span></div><div class="line">Training mini-batch number <span class="number">86000</span></div><div class="line">Training mini-batch number <span class="number">87000</span></div><div class="line">Training mini-batch number <span class="number">88000</span></div><div class="line">Training mini-batch number <span class="number">89000</span></div><div class="line">Epoch <span class="number">17</span>: validation accuracy <span class="number">98.48</span>%</div><div class="line">This <span class="keyword">is</span> the best validation accuracy to date.</div><div class="line">The corresponding test accuracy <span class="keyword">is</span> <span class="number">98.42</span>%</div><div class="line">Training mini-batch number <span class="number">90000</span></div><div class="line">Training mini-batch number <span class="number">91000</span></div><div class="line">Training mini-batch number <span class="number">92000</span></div><div class="line">Training mini-batch number <span class="number">93000</span></div><div class="line">Training mini-batch number <span class="number">94000</span></div><div class="line">Epoch <span class="number">18</span>: validation accuracy <span class="number">98.50</span>%</div><div class="line">This <span class="keyword">is</span> the best validation accuracy to date.</div><div class="line">The corresponding test accuracy <span class="keyword">is</span> <span class="number">98.42</span>%</div><div class="line">Training mini-batch number <span class="number">95000</span></div><div class="line">Training mini-batch number <span class="number">96000</span></div><div class="line">Training mini-batch number <span class="number">97000</span></div><div class="line">Training mini-batch number <span class="number">98000</span></div><div class="line">Training mini-batch number <span class="number">99000</span></div><div class="line">Epoch <span class="number">19</span>: validation accuracy <span class="number">98.51</span>%</div><div class="line">This <span class="keyword">is</span> the best validation accuracy to date.</div><div class="line">The corresponding test accuracy <span class="keyword">is</span> <span class="number">98.43</span>%</div><div class="line">Training mini-batch number <span class="number">100000</span></div><div class="line">Training mini-batch number <span class="number">101000</span></div><div class="line">Training mini-batch number <span class="number">102000</span></div><div class="line">Training mini-batch number <span class="number">103000</span></div><div class="line">Training mini-batch number <span class="number">104000</span></div><div class="line">Epoch <span class="number">20</span>: validation accuracy <span class="number">98.57</span>%</div><div class="line">This <span class="keyword">is</span> the best validation accuracy to date.</div><div class="line">The corresponding test accuracy <span class="keyword">is</span> <span class="number">98.47</span>%</div></pre></td></tr></table></figure></p>
<p>　　下面是采取弃权策略，并设p_dropout=0.3的结果。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div><div class="line">64</div><div class="line">65</div><div class="line">66</div><div class="line">67</div><div class="line">68</div><div class="line">69</div><div class="line">70</div><div class="line">71</div><div class="line">72</div><div class="line">73</div><div class="line">74</div><div class="line">75</div><div class="line">76</div><div class="line">77</div><div class="line">78</div><div class="line">79</div><div class="line">80</div><div class="line">81</div><div class="line">82</div><div class="line">83</div><div class="line">84</div><div class="line">85</div><div class="line">86</div><div class="line">87</div><div class="line">88</div><div class="line">89</div><div class="line">90</div><div class="line">91</div><div class="line">92</div><div class="line">93</div><div class="line">94</div><div class="line">95</div><div class="line">96</div><div class="line">97</div><div class="line">98</div><div class="line">99</div><div class="line">100</div><div class="line">101</div><div class="line">102</div><div class="line">103</div><div class="line">104</div><div class="line">105</div><div class="line">106</div><div class="line">107</div><div class="line">108</div><div class="line">109</div><div class="line">110</div><div class="line">111</div><div class="line">112</div><div class="line">113</div><div class="line">114</div><div class="line">115</div><div class="line">116</div><div class="line">117</div><div class="line">118</div><div class="line">119</div><div class="line">120</div><div class="line">121</div><div class="line">122</div><div class="line">123</div><div class="line">124</div><div class="line">125</div><div class="line">126</div><div class="line">127</div><div class="line">128</div><div class="line">129</div><div class="line">130</div><div class="line">131</div><div class="line">132</div><div class="line">133</div><div class="line">134</div><div class="line">135</div><div class="line">136</div><div class="line">137</div><div class="line">138</div><div class="line">139</div><div class="line">140</div><div class="line">141</div><div class="line">142</div><div class="line">143</div><div class="line">144</div><div class="line">145</div><div class="line">146</div><div class="line">147</div><div class="line">148</div><div class="line">149</div><div class="line">150</div><div class="line">151</div><div class="line">152</div><div class="line">153</div><div class="line">154</div><div class="line">155</div><div class="line">156</div><div class="line">157</div><div class="line">158</div><div class="line">159</div><div class="line">160</div><div class="line">161</div><div class="line">162</div><div class="line">163</div><div class="line">164</div><div class="line">165</div><div class="line">166</div><div class="line">167</div><div class="line">168</div><div class="line">169</div><div class="line">170</div><div class="line">171</div><div class="line">172</div><div class="line">173</div><div class="line">174</div><div class="line">175</div><div class="line">176</div><div class="line">177</div><div class="line">178</div><div class="line">179</div><div class="line">180</div><div class="line">181</div><div class="line">182</div><div class="line">183</div><div class="line">184</div><div class="line">185</div><div class="line">186</div><div class="line">187</div><div class="line">188</div><div class="line">189</div><div class="line">190</div><div class="line">191</div><div class="line">192</div><div class="line">193</div><div class="line">194</div><div class="line">195</div><div class="line">196</div><div class="line">197</div><div class="line">198</div><div class="line">199</div><div class="line">200</div><div class="line">201</div><div class="line">202</div><div class="line">203</div><div class="line">204</div><div class="line">205</div><div class="line">206</div><div class="line">207</div><div class="line">208</div><div class="line">209</div><div class="line">210</div><div class="line">211</div><div class="line">212</div><div class="line">213</div><div class="line">214</div><div class="line">215</div><div class="line">216</div><div class="line">217</div><div class="line">218</div><div class="line">219</div><div class="line">220</div></pre></td><td class="code"><pre><div class="line">Training mini-batch number <span class="number">0</span></div><div class="line">Training mini-batch number <span class="number">1000</span></div><div class="line">Training mini-batch number <span class="number">2000</span></div><div class="line">Training mini-batch number <span class="number">3000</span></div><div class="line">Training mini-batch number <span class="number">4000</span></div><div class="line">Epoch <span class="number">0</span>: validation accuracy <span class="number">93.39</span>%</div><div class="line">This <span class="keyword">is</span> the best validation accuracy to date.</div><div class="line">The corresponding test accuracy <span class="keyword">is</span> <span class="number">92.51</span>%</div><div class="line">Training mini-batch number <span class="number">5000</span></div><div class="line">Training mini-batch number <span class="number">6000</span></div><div class="line">Training mini-batch number <span class="number">7000</span></div><div class="line">Training mini-batch number <span class="number">8000</span></div><div class="line">Training mini-batch number <span class="number">9000</span></div><div class="line">Epoch <span class="number">1</span>: validation accuracy <span class="number">96.30</span>%</div><div class="line">This <span class="keyword">is</span> the best validation accuracy to date.</div><div class="line">The corresponding test accuracy <span class="keyword">is</span> <span class="number">95.87</span>%</div><div class="line">Training mini-batch number <span class="number">10000</span></div><div class="line">Training mini-batch number <span class="number">11000</span></div><div class="line">Training mini-batch number <span class="number">12000</span></div><div class="line">Training mini-batch number <span class="number">13000</span></div><div class="line">Training mini-batch number <span class="number">14000</span></div><div class="line">Epoch <span class="number">2</span>: validation accuracy <span class="number">97.35</span>%</div><div class="line">This <span class="keyword">is</span> the best validation accuracy to date.</div><div class="line">The corresponding test accuracy <span class="keyword">is</span> <span class="number">96.94</span>%</div><div class="line">Training mini-batch number <span class="number">15000</span></div><div class="line">Training mini-batch number <span class="number">16000</span></div><div class="line">Training mini-batch number <span class="number">17000</span></div><div class="line">Training mini-batch number <span class="number">18000</span></div><div class="line">Training mini-batch number <span class="number">19000</span></div><div class="line">Epoch <span class="number">3</span>: validation accuracy <span class="number">97.82</span>%</div><div class="line">This <span class="keyword">is</span> the best validation accuracy to date.</div><div class="line">The corresponding test accuracy <span class="keyword">is</span> <span class="number">97.45</span>%</div><div class="line">Training mini-batch number <span class="number">20000</span></div><div class="line">Training mini-batch number <span class="number">21000</span></div><div class="line">Training mini-batch number <span class="number">22000</span></div><div class="line">Training mini-batch number <span class="number">23000</span></div><div class="line">Training mini-batch number <span class="number">24000</span></div><div class="line">Epoch <span class="number">4</span>: validation accuracy <span class="number">98.14</span>%</div><div class="line">This <span class="keyword">is</span> the best validation accuracy to date.</div><div class="line">The corresponding test accuracy <span class="keyword">is</span> <span class="number">97.93</span>%</div><div class="line">Training mini-batch number <span class="number">25000</span></div><div class="line">Training mini-batch number <span class="number">26000</span></div><div class="line">Training mini-batch number <span class="number">27000</span></div><div class="line">Training mini-batch number <span class="number">28000</span></div><div class="line">Training mini-batch number <span class="number">29000</span></div><div class="line">Epoch <span class="number">5</span>: validation accuracy <span class="number">98.13</span>%</div><div class="line">Training mini-batch number <span class="number">30000</span></div><div class="line">Training mini-batch number <span class="number">31000</span></div><div class="line">Training mini-batch number <span class="number">32000</span></div><div class="line">Training mini-batch number <span class="number">33000</span></div><div class="line">Training mini-batch number <span class="number">34000</span></div><div class="line">Epoch <span class="number">6</span>: validation accuracy <span class="number">98.26</span>%</div><div class="line">This <span class="keyword">is</span> the best validation accuracy to date.</div><div class="line">The corresponding test accuracy <span class="keyword">is</span> <span class="number">98.23</span>%</div><div class="line">Training mini-batch number <span class="number">35000</span></div><div class="line">Training mini-batch number <span class="number">36000</span></div><div class="line">Training mini-batch number <span class="number">37000</span></div><div class="line">Training mini-batch number <span class="number">38000</span></div><div class="line">Training mini-batch number <span class="number">39000</span></div><div class="line">Epoch <span class="number">7</span>: validation accuracy <span class="number">98.56</span>%</div><div class="line">This <span class="keyword">is</span> the best validation accuracy to date.</div><div class="line">The corresponding test accuracy <span class="keyword">is</span> <span class="number">98.48</span>%</div><div class="line">Training mini-batch number <span class="number">40000</span></div><div class="line">Training mini-batch number <span class="number">41000</span></div><div class="line">Training mini-batch number <span class="number">42000</span></div><div class="line">Training mini-batch number <span class="number">43000</span></div><div class="line">Training mini-batch number <span class="number">44000</span></div><div class="line">Epoch <span class="number">8</span>: validation accuracy <span class="number">98.59</span>%</div><div class="line">This <span class="keyword">is</span> the best validation accuracy to date.</div><div class="line">The corresponding test accuracy <span class="keyword">is</span> <span class="number">98.53</span>%</div><div class="line">Training mini-batch number <span class="number">45000</span></div><div class="line">Training mini-batch number <span class="number">46000</span></div><div class="line">Training mini-batch number <span class="number">47000</span></div><div class="line">Training mini-batch number <span class="number">48000</span></div><div class="line">Training mini-batch number <span class="number">49000</span></div><div class="line">Epoch <span class="number">9</span>: validation accuracy <span class="number">98.59</span>%</div><div class="line">This <span class="keyword">is</span> the best validation accuracy to date.</div><div class="line">The corresponding test accuracy <span class="keyword">is</span> <span class="number">98.68</span>%</div><div class="line">Training mini-batch number <span class="number">50000</span></div><div class="line">Training mini-batch number <span class="number">51000</span></div><div class="line">Training mini-batch number <span class="number">52000</span></div><div class="line">Training mini-batch number <span class="number">53000</span></div><div class="line">Training mini-batch number <span class="number">54000</span></div><div class="line">Epoch <span class="number">10</span>: validation accuracy <span class="number">98.72</span>%</div><div class="line">This <span class="keyword">is</span> the best validation accuracy to date.</div><div class="line">The corresponding test accuracy <span class="keyword">is</span> <span class="number">98.78</span>%</div><div class="line">Training mini-batch number <span class="number">55000</span></div><div class="line">Training mini-batch number <span class="number">56000</span></div><div class="line">Training mini-batch number <span class="number">57000</span></div><div class="line">Training mini-batch number <span class="number">58000</span></div><div class="line">Training mini-batch number <span class="number">59000</span></div><div class="line">Epoch <span class="number">11</span>: validation accuracy <span class="number">98.69</span>%</div><div class="line">Training mini-batch number <span class="number">60000</span></div><div class="line">Training mini-batch number <span class="number">61000</span></div><div class="line">Training mini-batch number <span class="number">62000</span></div><div class="line">Training mini-batch number <span class="number">63000</span></div><div class="line">Training mini-batch number <span class="number">64000</span></div><div class="line">Epoch <span class="number">12</span>: validation accuracy <span class="number">98.74</span>%</div><div class="line">This <span class="keyword">is</span> the best validation accuracy to date.</div><div class="line">The corresponding test accuracy <span class="keyword">is</span> <span class="number">98.79</span>%</div><div class="line">Training mini-batch number <span class="number">65000</span></div><div class="line">Training mini-batch number <span class="number">66000</span></div><div class="line">Training mini-batch number <span class="number">67000</span></div><div class="line">Training mini-batch number <span class="number">68000</span></div><div class="line">Training mini-batch number <span class="number">69000</span></div><div class="line">Epoch <span class="number">13</span>: validation accuracy <span class="number">98.72</span>%</div><div class="line">Training mini-batch number <span class="number">70000</span></div><div class="line">Training mini-batch number <span class="number">71000</span></div><div class="line">Training mini-batch number <span class="number">72000</span></div><div class="line">Training mini-batch number <span class="number">73000</span></div><div class="line">Training mini-batch number <span class="number">74000</span></div><div class="line">Epoch <span class="number">14</span>: validation accuracy <span class="number">98.77</span>%</div><div class="line">This <span class="keyword">is</span> the best validation accuracy to date.</div><div class="line">The corresponding test accuracy <span class="keyword">is</span> <span class="number">98.75</span>%</div><div class="line">Training mini-batch number <span class="number">75000</span></div><div class="line">Training mini-batch number <span class="number">76000</span></div><div class="line">Training mini-batch number <span class="number">77000</span></div><div class="line">Training mini-batch number <span class="number">78000</span></div><div class="line">Training mini-batch number <span class="number">79000</span></div><div class="line">Epoch <span class="number">15</span>: validation accuracy <span class="number">98.78</span>%</div><div class="line">This <span class="keyword">is</span> the best validation accuracy to date.</div><div class="line">The corresponding test accuracy <span class="keyword">is</span> <span class="number">98.82</span>%</div><div class="line">Training mini-batch number <span class="number">80000</span></div><div class="line">Training mini-batch number <span class="number">81000</span></div><div class="line">Training mini-batch number <span class="number">82000</span></div><div class="line">Training mini-batch number <span class="number">83000</span></div><div class="line">Training mini-batch number <span class="number">84000</span></div><div class="line">Epoch <span class="number">16</span>: validation accuracy <span class="number">98.87</span>%</div><div class="line">This <span class="keyword">is</span> the best validation accuracy to date.</div><div class="line">The corresponding test accuracy <span class="keyword">is</span> <span class="number">98.95</span>%</div><div class="line">Training mini-batch number <span class="number">85000</span></div><div class="line">Training mini-batch number <span class="number">86000</span></div><div class="line">Training mini-batch number <span class="number">87000</span></div><div class="line">Training mini-batch number <span class="number">88000</span></div><div class="line">Training mini-batch number <span class="number">89000</span></div><div class="line">Epoch <span class="number">17</span>: validation accuracy <span class="number">98.83</span>%</div><div class="line">Training mini-batch number <span class="number">90000</span></div><div class="line">Training mini-batch number <span class="number">91000</span></div><div class="line">Training mini-batch number <span class="number">92000</span></div><div class="line">Training mini-batch number <span class="number">93000</span></div><div class="line">Training mini-batch number <span class="number">94000</span></div><div class="line">Epoch <span class="number">18</span>: validation accuracy <span class="number">98.77</span>%</div><div class="line">Training mini-batch number <span class="number">95000</span></div><div class="line">Training mini-batch number <span class="number">96000</span></div><div class="line">Training mini-batch number <span class="number">97000</span></div><div class="line">Training mini-batch number <span class="number">98000</span></div><div class="line">Training mini-batch number <span class="number">99000</span></div><div class="line">Epoch <span class="number">19</span>: validation accuracy <span class="number">98.81</span>%</div><div class="line">Training mini-batch number <span class="number">100000</span></div><div class="line">Training mini-batch number <span class="number">101000</span></div><div class="line">Training mini-batch number <span class="number">102000</span></div><div class="line">Training mini-batch number <span class="number">103000</span></div><div class="line">Training mini-batch number <span class="number">104000</span></div><div class="line">Epoch <span class="number">20</span>: validation accuracy <span class="number">98.79</span>%</div><div class="line">Training mini-batch number <span class="number">105000</span></div><div class="line">Training mini-batch number <span class="number">106000</span></div><div class="line">Training mini-batch number <span class="number">107000</span></div><div class="line">Training mini-batch number <span class="number">108000</span></div><div class="line">Training mini-batch number <span class="number">109000</span></div><div class="line">Epoch <span class="number">21</span>: validation accuracy <span class="number">98.82</span>%</div><div class="line">Training mini-batch number <span class="number">110000</span></div><div class="line">Training mini-batch number <span class="number">111000</span></div><div class="line">Training mini-batch number <span class="number">112000</span></div><div class="line">Training mini-batch number <span class="number">113000</span></div><div class="line">Training mini-batch number <span class="number">114000</span></div><div class="line">Epoch <span class="number">22</span>: validation accuracy <span class="number">98.91</span>%</div><div class="line">This <span class="keyword">is</span> the best validation accuracy to date.</div><div class="line">The corresponding test accuracy <span class="keyword">is</span> <span class="number">99.00</span>%</div><div class="line">Training mini-batch number <span class="number">115000</span></div><div class="line">Training mini-batch number <span class="number">116000</span></div><div class="line">Training mini-batch number <span class="number">117000</span></div><div class="line">Training mini-batch number <span class="number">118000</span></div><div class="line">Training mini-batch number <span class="number">119000</span></div><div class="line">Epoch <span class="number">23</span>: validation accuracy <span class="number">98.88</span>%</div><div class="line">Training mini-batch number <span class="number">120000</span></div><div class="line">Training mini-batch number <span class="number">121000</span></div><div class="line">Training mini-batch number <span class="number">122000</span></div><div class="line">Training mini-batch number <span class="number">123000</span></div><div class="line">Training mini-batch number <span class="number">124000</span></div><div class="line">Epoch <span class="number">24</span>: validation accuracy <span class="number">98.92</span>%</div><div class="line">This <span class="keyword">is</span> the best validation accuracy to date.</div><div class="line">The corresponding test accuracy <span class="keyword">is</span> <span class="number">98.93</span>%</div><div class="line">Training mini-batch number <span class="number">125000</span></div><div class="line">Training mini-batch number <span class="number">126000</span></div><div class="line">Training mini-batch number <span class="number">127000</span></div><div class="line">Training mini-batch number <span class="number">128000</span></div><div class="line">Training mini-batch number <span class="number">129000</span></div><div class="line">Epoch <span class="number">25</span>: validation accuracy <span class="number">98.94</span>%</div><div class="line">This <span class="keyword">is</span> the best validation accuracy to date.</div><div class="line">The corresponding test accuracy <span class="keyword">is</span> <span class="number">98.92</span>%</div><div class="line">Training mini-batch number <span class="number">130000</span></div><div class="line">Training mini-batch number <span class="number">131000</span></div><div class="line">Training mini-batch number <span class="number">132000</span></div><div class="line">Training mini-batch number <span class="number">133000</span></div><div class="line">Training mini-batch number <span class="number">134000</span></div><div class="line">Epoch <span class="number">26</span>: validation accuracy <span class="number">98.87</span>%</div><div class="line">Training mini-batch number <span class="number">135000</span></div><div class="line">Training mini-batch number <span class="number">136000</span></div><div class="line">Training mini-batch number <span class="number">137000</span> </div><div class="line">Training mini-batch number <span class="number">138000</span></div><div class="line">Training mini-batch number <span class="number">139000</span></div><div class="line">Epoch <span class="number">27</span>: validation accuracy <span class="number">98.83</span>%</div><div class="line">Training mini-batch number <span class="number">140000</span></div><div class="line">Training mini-batch number <span class="number">141000</span></div><div class="line">Training mini-batch number <span class="number">142000</span></div><div class="line">Training mini-batch number <span class="number">143000</span></div><div class="line">Training mini-batch number <span class="number">144000</span></div><div class="line">Epoch <span class="number">28</span>: validation accuracy <span class="number">98.88</span>%</div><div class="line">Training mini-batch number <span class="number">145000</span></div><div class="line">Training mini-batch number <span class="number">146000</span></div><div class="line">Training mini-batch number <span class="number">147000</span></div><div class="line">Training mini-batch number <span class="number">148000</span></div><div class="line">Training mini-batch number <span class="number">149000</span></div><div class="line">Epoch <span class="number">29</span>: validation accuracy <span class="number">98.91</span>%</div><div class="line">Training mini-batch number <span class="number">150000</span></div><div class="line">Training mini-batch number <span class="number">151000</span></div><div class="line">Training mini-batch number <span class="number">152000</span></div><div class="line">Training mini-batch number <span class="number">153000</span></div><div class="line">Training mini-batch number <span class="number">154000</span></div><div class="line">Epoch <span class="number">30</span>: validation accuracy <span class="number">98.91</span>%</div></pre></td></tr></table></figure></p>
<p>　　可以发现，刚开始的时候，不采取弃权策略的结果更好。但是随着轮次的增加，采取弃权策略的结果马上就超过了不采取弃权策略的结果，并且采取弃权策略的准确率提升也更快。例如在弃权策略中第16轮次测试集准确率就达到了98.95%，而不采取弃权策略，在更多的第20轮次的测试集结果反而才98.47%。<br>　　可以看到弃权策略最优结果为第22轮次的99%。<br>　　实际上可以通过再插入一个卷积层，实现准确率进一步提升到99.22%，如下代码：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line">net = Network([</div><div class="line">    ConvPoolLayer(image_shape=(mini_batch_size, <span class="number">1</span>, <span class="number">28</span>, <span class="number">28</span>),</div><div class="line">                filter_shape=(<span class="number">20</span>, <span class="number">1</span>, <span class="number">5</span>, <span class="number">5</span>),poolsize=(<span class="number">2</span>, <span class="number">2</span>)),</div><div class="line">    ConvPoolLayer(image_shape=(mini_batch_size, <span class="number">20</span>, <span class="number">12</span>, <span class="number">12</span>),</div><div class="line">                filter_shape=(<span class="number">40</span>, <span class="number">20</span>, <span class="number">5</span>, <span class="number">5</span>),poolsize=(<span class="number">2</span>, <span class="number">2</span>)),</div><div class="line">    FullyConnectedLayer(n_in=<span class="number">40</span>*<span class="number">4</span>*<span class="number">4</span>, n_out=<span class="number">100</span>,p_dropout=<span class="number">0.3</span>),</div><div class="line">    SoftmaxLayer(n_in=<span class="number">100</span>, n_out=<span class="number">10</span>)], mini_batch_size)</div><div class="line">net.SGD(training_data, <span class="number">60</span>, mini_batch_size, <span class="number">0.1</span>,validation_data, test_data)</div></pre></td></tr></table></figure></p>
<p>　　具体结果:<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div><div class="line">64</div><div class="line">65</div><div class="line">66</div><div class="line">67</div><div class="line">68</div><div class="line">69</div><div class="line">70</div><div class="line">71</div><div class="line">72</div><div class="line">73</div><div class="line">74</div><div class="line">75</div><div class="line">76</div><div class="line">77</div><div class="line">78</div><div class="line">79</div><div class="line">80</div><div class="line">81</div><div class="line">82</div><div class="line">83</div><div class="line">84</div><div class="line">85</div><div class="line">86</div><div class="line">87</div><div class="line">88</div><div class="line">89</div><div class="line">90</div><div class="line">91</div><div class="line">92</div><div class="line">93</div><div class="line">94</div><div class="line">95</div><div class="line">96</div><div class="line">97</div><div class="line">98</div><div class="line">99</div><div class="line">100</div><div class="line">101</div><div class="line">102</div><div class="line">103</div><div class="line">104</div><div class="line">105</div><div class="line">106</div><div class="line">107</div><div class="line">108</div><div class="line">109</div><div class="line">110</div><div class="line">111</div><div class="line">112</div><div class="line">113</div><div class="line">114</div><div class="line">115</div><div class="line">116</div><div class="line">117</div><div class="line">118</div><div class="line">119</div><div class="line">120</div><div class="line">121</div><div class="line">122</div><div class="line">123</div><div class="line">124</div><div class="line">125</div><div class="line">126</div><div class="line">127</div><div class="line">128</div><div class="line">129</div><div class="line">130</div><div class="line">131</div><div class="line">132</div><div class="line">133</div><div class="line">134</div><div class="line">135</div><div class="line">136</div><div class="line">137</div><div class="line">138</div><div class="line">139</div><div class="line">140</div><div class="line">141</div><div class="line">142</div><div class="line">143</div><div class="line">144</div><div class="line">145</div><div class="line">146</div><div class="line">147</div><div class="line">148</div><div class="line">149</div><div class="line">150</div><div class="line">151</div><div class="line">152</div><div class="line">153</div><div class="line">154</div><div class="line">155</div><div class="line">156</div><div class="line">157</div><div class="line">158</div><div class="line">159</div><div class="line">160</div><div class="line">161</div><div class="line">162</div><div class="line">163</div><div class="line">164</div><div class="line">165</div><div class="line">166</div><div class="line">167</div><div class="line">168</div><div class="line">169</div><div class="line">170</div><div class="line">171</div><div class="line">172</div><div class="line">173</div><div class="line">174</div><div class="line">175</div><div class="line">176</div><div class="line">177</div><div class="line">178</div><div class="line">179</div><div class="line">180</div><div class="line">181</div><div class="line">182</div><div class="line">183</div><div class="line">184</div><div class="line">185</div><div class="line">186</div><div class="line">187</div><div class="line">188</div><div class="line">189</div><div class="line">190</div><div class="line">191</div><div class="line">192</div><div class="line">193</div><div class="line">194</div><div class="line">195</div><div class="line">196</div><div class="line">197</div><div class="line">198</div><div class="line">199</div><div class="line">200</div><div class="line">201</div><div class="line">202</div><div class="line">203</div><div class="line">204</div><div class="line">205</div><div class="line">206</div><div class="line">207</div><div class="line">208</div><div class="line">209</div><div class="line">210</div><div class="line">211</div><div class="line">212</div><div class="line">213</div><div class="line">214</div><div class="line">215</div><div class="line">216</div><div class="line">217</div><div class="line">218</div><div class="line">219</div><div class="line">220</div><div class="line">221</div><div class="line">222</div><div class="line">223</div><div class="line">224</div><div class="line">225</div><div class="line">226</div><div class="line">227</div><div class="line">228</div><div class="line">229</div><div class="line">230</div><div class="line">231</div><div class="line">232</div><div class="line">233</div><div class="line">234</div><div class="line">235</div><div class="line">236</div><div class="line">237</div><div class="line">238</div><div class="line">239</div><div class="line">240</div><div class="line">241</div><div class="line">242</div><div class="line">243</div><div class="line">244</div><div class="line">245</div><div class="line">246</div><div class="line">247</div><div class="line">248</div><div class="line">249</div><div class="line">250</div><div class="line">251</div><div class="line">252</div><div class="line">253</div><div class="line">254</div><div class="line">255</div><div class="line">256</div><div class="line">257</div><div class="line">258</div><div class="line">259</div><div class="line">260</div><div class="line">261</div><div class="line">262</div><div class="line">263</div><div class="line">264</div><div class="line">265</div><div class="line">266</div><div class="line">267</div></pre></td><td class="code"><pre><div class="line">Training mini-batch number <span class="number">0</span></div><div class="line">Training mini-batch number <span class="number">1000</span></div><div class="line">Training mini-batch number <span class="number">2000</span></div><div class="line">Training mini-batch number <span class="number">3000</span></div><div class="line">Training mini-batch number <span class="number">4000</span></div><div class="line">Epoch <span class="number">0</span>: validation accuracy <span class="number">88.52</span>%</div><div class="line">This <span class="keyword">is</span> the best validation accuracy to date.</div><div class="line">The corresponding test accuracy <span class="keyword">is</span> <span class="number">88.12</span>%</div><div class="line">Training mini-batch number <span class="number">5000</span></div><div class="line">Training mini-batch number <span class="number">6000</span></div><div class="line">Training mini-batch number <span class="number">7000</span></div><div class="line">Training mini-batch number <span class="number">8000</span></div><div class="line">Training mini-batch number <span class="number">9000</span></div><div class="line">Epoch <span class="number">1</span>: validation accuracy <span class="number">96.09</span>%</div><div class="line">This <span class="keyword">is</span> the best validation accuracy to date.</div><div class="line">The corresponding test accuracy <span class="keyword">is</span> <span class="number">96.08</span>%</div><div class="line">Training mini-batch number <span class="number">10000</span></div><div class="line">Training mini-batch number <span class="number">11000</span></div><div class="line">Training mini-batch number <span class="number">12000</span></div><div class="line">Training mini-batch number <span class="number">13000</span></div><div class="line">Training mini-batch number <span class="number">14000</span></div><div class="line">Epoch <span class="number">2</span>: validation accuracy <span class="number">97.53</span>%</div><div class="line">This <span class="keyword">is</span> the best validation accuracy to date.</div><div class="line">The corresponding test accuracy <span class="keyword">is</span> <span class="number">97.25</span>%</div><div class="line">Training mini-batch number <span class="number">15000</span></div><div class="line">Training mini-batch number <span class="number">16000</span></div><div class="line">Training mini-batch number <span class="number">17000</span></div><div class="line">Training mini-batch number <span class="number">18000</span></div><div class="line">Training mini-batch number <span class="number">19000</span></div><div class="line">Epoch <span class="number">3</span>: validation accuracy <span class="number">97.80</span>%</div><div class="line">This <span class="keyword">is</span> the best validation accuracy to date.</div><div class="line">The corresponding test accuracy <span class="keyword">is</span> <span class="number">97.73</span>%</div><div class="line">Training mini-batch number <span class="number">20000</span></div><div class="line">Training mini-batch number <span class="number">21000</span></div><div class="line">Training mini-batch number <span class="number">22000</span></div><div class="line">Training mini-batch number <span class="number">23000</span></div><div class="line">Training mini-batch number <span class="number">24000</span></div><div class="line">Epoch <span class="number">4</span>: validation accuracy <span class="number">98.09</span>%</div><div class="line">This <span class="keyword">is</span> the best validation accuracy to date.</div><div class="line">The corresponding test accuracy <span class="keyword">is</span> <span class="number">98.11</span>%</div><div class="line">Training mini-batch number <span class="number">25000</span></div><div class="line">Training mini-batch number <span class="number">26000</span></div><div class="line">Training mini-batch number <span class="number">27000</span></div><div class="line">Training mini-batch number <span class="number">28000</span></div><div class="line">Training mini-batch number <span class="number">29000</span></div><div class="line">Epoch <span class="number">5</span>: validation accuracy <span class="number">98.36</span>%</div><div class="line">This <span class="keyword">is</span> the best validation accuracy to date.</div><div class="line">The corresponding test accuracy <span class="keyword">is</span> <span class="number">98.43</span>%</div><div class="line">Training mini-batch number <span class="number">30000</span></div><div class="line">Training mini-batch number <span class="number">31000</span></div><div class="line">Training mini-batch number <span class="number">32000</span></div><div class="line">Training mini-batch number <span class="number">33000</span></div><div class="line">Training mini-batch number <span class="number">34000</span></div><div class="line">Epoch <span class="number">6</span>: validation accuracy <span class="number">98.38</span>%</div><div class="line">This <span class="keyword">is</span> the best validation accuracy to date.</div><div class="line">The corresponding test accuracy <span class="keyword">is</span> <span class="number">98.38</span>%</div><div class="line">Training mini-batch number <span class="number">35000</span></div><div class="line">Training mini-batch number <span class="number">36000</span></div><div class="line">Training mini-batch number <span class="number">37000</span></div><div class="line">Training mini-batch number <span class="number">38000</span></div><div class="line">Training mini-batch number <span class="number">39000</span></div><div class="line">Epoch <span class="number">7</span>: validation accuracy <span class="number">98.50</span>%</div><div class="line">This <span class="keyword">is</span> the best validation accuracy to date.</div><div class="line">The corresponding test accuracy <span class="keyword">is</span> <span class="number">98.63</span>%</div><div class="line">Training mini-batch number <span class="number">40000</span></div><div class="line">Training mini-batch number <span class="number">41000</span></div><div class="line">Training mini-batch number <span class="number">42000</span></div><div class="line">Training mini-batch number <span class="number">43000</span></div><div class="line">Training mini-batch number <span class="number">44000</span></div><div class="line">Epoch <span class="number">8</span>: validation accuracy <span class="number">98.71</span>%</div><div class="line">This <span class="keyword">is</span> the best validation accuracy to date.</div><div class="line">The corresponding test accuracy <span class="keyword">is</span> <span class="number">98.83</span>%</div><div class="line">Training mini-batch number <span class="number">45000</span></div><div class="line">Training mini-batch number <span class="number">46000</span></div><div class="line">Training mini-batch number <span class="number">47000</span></div><div class="line">Training mini-batch number <span class="number">48000</span></div><div class="line">Training mini-batch number <span class="number">49000</span></div><div class="line">Epoch <span class="number">9</span>: validation accuracy <span class="number">98.75</span>%</div><div class="line">This <span class="keyword">is</span> the best validation accuracy to date.</div><div class="line">The corresponding test accuracy <span class="keyword">is</span> <span class="number">98.77</span>%</div><div class="line">Training mini-batch number <span class="number">50000</span></div><div class="line">Training mini-batch number <span class="number">51000</span></div><div class="line">Training mini-batch number <span class="number">52000</span></div><div class="line">Training mini-batch number <span class="number">53000</span></div><div class="line">Training mini-batch number <span class="number">54000</span></div><div class="line">Epoch <span class="number">10</span>: validation accuracy <span class="number">98.64</span>%</div><div class="line">Training mini-batch number <span class="number">55000</span></div><div class="line">Training mini-batch number <span class="number">56000</span></div><div class="line">Training mini-batch number <span class="number">57000</span></div><div class="line">Training mini-batch number <span class="number">58000</span></div><div class="line">Training mini-batch number <span class="number">59000</span></div><div class="line">Epoch <span class="number">11</span>: validation accuracy <span class="number">98.79</span>%</div><div class="line">This <span class="keyword">is</span> the best validation accuracy to date.</div><div class="line">The corresponding test accuracy <span class="keyword">is</span> <span class="number">98.86</span>%</div><div class="line">Training mini-batch number <span class="number">60000</span></div><div class="line">Training mini-batch number <span class="number">61000</span></div><div class="line">Training mini-batch number <span class="number">62000</span></div><div class="line">Training mini-batch number <span class="number">63000</span></div><div class="line">Training mini-batch number <span class="number">64000</span></div><div class="line">Epoch <span class="number">12</span>: validation accuracy <span class="number">98.89</span>%</div><div class="line">This <span class="keyword">is</span> the best validation accuracy to date.</div><div class="line">The corresponding test accuracy <span class="keyword">is</span> <span class="number">99.00</span>%</div><div class="line">Training mini-batch number <span class="number">65000</span></div><div class="line">Training mini-batch number <span class="number">66000</span></div><div class="line">Training mini-batch number <span class="number">67000</span></div><div class="line">Training mini-batch number <span class="number">68000</span></div><div class="line">Training mini-batch number <span class="number">69000</span></div><div class="line">Epoch <span class="number">13</span>: validation accuracy <span class="number">98.82</span>%</div><div class="line">Training mini-batch number <span class="number">70000</span></div><div class="line">Training mini-batch number <span class="number">71000</span></div><div class="line">Training mini-batch number <span class="number">72000</span></div><div class="line">Training mini-batch number <span class="number">73000</span></div><div class="line">Training mini-batch number <span class="number">74000</span></div><div class="line">Epoch <span class="number">14</span>: validation accuracy <span class="number">98.84</span>%</div><div class="line">Training mini-batch number <span class="number">75000</span></div><div class="line">Training mini-batch number <span class="number">76000</span></div><div class="line">Training mini-batch number <span class="number">77000</span></div><div class="line">Training mini-batch number <span class="number">78000</span></div><div class="line">Training mini-batch number <span class="number">79000</span></div><div class="line">Epoch <span class="number">15</span>: validation accuracy <span class="number">98.86</span>%</div><div class="line">Training mini-batch number <span class="number">80000</span></div><div class="line">Training mini-batch number <span class="number">81000</span></div><div class="line">Training mini-batch number <span class="number">82000</span></div><div class="line">Training mini-batch number <span class="number">83000</span></div><div class="line">Training mini-batch number <span class="number">84000</span></div><div class="line">Epoch <span class="number">16</span>: validation accuracy <span class="number">98.95</span>%</div><div class="line">This <span class="keyword">is</span> the best validation accuracy to date.</div><div class="line">The corresponding test accuracy <span class="keyword">is</span> <span class="number">99.05</span>%</div><div class="line">Training mini-batch number <span class="number">85000</span></div><div class="line">Training mini-batch number <span class="number">86000</span></div><div class="line">Training mini-batch number <span class="number">87000</span></div><div class="line">Training mini-batch number <span class="number">88000</span></div><div class="line">Training mini-batch number <span class="number">89000</span></div><div class="line">Epoch <span class="number">17</span>: validation accuracy <span class="number">99.01</span>%</div><div class="line">This <span class="keyword">is</span> the best validation accuracy to date.</div><div class="line">The corresponding test accuracy <span class="keyword">is</span> <span class="number">99.10</span>%</div><div class="line">Training mini-batch number <span class="number">90000</span></div><div class="line">Training mini-batch number <span class="number">91000</span></div><div class="line">Training mini-batch number <span class="number">92000</span></div><div class="line">Training mini-batch number <span class="number">93000</span></div><div class="line">Training mini-batch number <span class="number">94000</span></div><div class="line">Epoch <span class="number">18</span>: validation accuracy <span class="number">98.98</span>%</div><div class="line">Training mini-batch number <span class="number">95000</span></div><div class="line">Training mini-batch number <span class="number">96000</span></div><div class="line">Training mini-batch number <span class="number">97000</span></div><div class="line">Training mini-batch number <span class="number">98000</span></div><div class="line">Training mini-batch number <span class="number">99000</span></div><div class="line">Epoch <span class="number">19</span>: validation accuracy <span class="number">99.02</span>%</div><div class="line">This <span class="keyword">is</span> the best validation accuracy to date.</div><div class="line">The corresponding test accuracy <span class="keyword">is</span> <span class="number">98.99</span>%</div><div class="line">Training mini-batch number <span class="number">100000</span></div><div class="line">Training mini-batch number <span class="number">101000</span></div><div class="line">Training mini-batch number <span class="number">102000</span></div><div class="line">Training mini-batch number <span class="number">103000</span></div><div class="line">Training mini-batch number <span class="number">104000</span></div><div class="line">Epoch <span class="number">20</span>: validation accuracy <span class="number">99.04</span>%</div><div class="line">This <span class="keyword">is</span> the best validation accuracy to date.</div><div class="line">The corresponding test accuracy <span class="keyword">is</span> <span class="number">99.08</span>%</div><div class="line">Training mini-batch number <span class="number">105000</span></div><div class="line">Training mini-batch number <span class="number">106000</span></div><div class="line">Training mini-batch number <span class="number">107000</span></div><div class="line">Training mini-batch number <span class="number">108000</span></div><div class="line">Training mini-batch number <span class="number">109000</span></div><div class="line">Epoch <span class="number">21</span>: validation accuracy <span class="number">99.00</span>%</div><div class="line">Training mini-batch number <span class="number">110000</span></div><div class="line">Training mini-batch number <span class="number">111000</span></div><div class="line">Training mini-batch number <span class="number">112000</span></div><div class="line">Training mini-batch number <span class="number">113000</span></div><div class="line">Training mini-batch number <span class="number">114000</span></div><div class="line">Epoch <span class="number">22</span>: validation accuracy <span class="number">99.05</span>%</div><div class="line">This <span class="keyword">is</span> the best validation accuracy to date.</div><div class="line">The corresponding test accuracy <span class="keyword">is</span> <span class="number">99.07</span>%</div><div class="line">Training mini-batch number <span class="number">115000</span></div><div class="line">Training mini-batch number <span class="number">116000</span></div><div class="line">Training mini-batch number <span class="number">117000</span></div><div class="line">Training mini-batch number <span class="number">118000</span></div><div class="line">Training mini-batch number <span class="number">119000</span></div><div class="line">Epoch <span class="number">23</span>: validation accuracy <span class="number">98.97</span>%</div><div class="line">Training mini-batch number <span class="number">120000</span></div><div class="line">Training mini-batch number <span class="number">121000</span></div><div class="line">Training mini-batch number <span class="number">122000</span></div><div class="line">Training mini-batch number <span class="number">123000</span></div><div class="line">Training mini-batch number <span class="number">124000</span></div><div class="line">Epoch <span class="number">24</span>: validation accuracy <span class="number">99.07</span>%</div><div class="line">This <span class="keyword">is</span> the best validation accuracy to date.</div><div class="line">The corresponding test accuracy <span class="keyword">is</span> <span class="number">99.02</span>%</div><div class="line">Training mini-batch number <span class="number">125000</span></div><div class="line">Training mini-batch number <span class="number">126000</span></div><div class="line">Training mini-batch number <span class="number">127000</span></div><div class="line">Training mini-batch number <span class="number">128000</span></div><div class="line">Training mini-batch number <span class="number">129000</span></div><div class="line">Epoch <span class="number">25</span>: validation accuracy <span class="number">99.03</span>%</div><div class="line">Training mini-batch number <span class="number">130000</span></div><div class="line">Training mini-batch number <span class="number">131000</span></div><div class="line">Training mini-batch number <span class="number">132000</span></div><div class="line">Training mini-batch number <span class="number">133000</span></div><div class="line">Training mini-batch number <span class="number">134000</span></div><div class="line">Epoch <span class="number">26</span>: validation accuracy <span class="number">98.99</span>%</div><div class="line">Training mini-batch number <span class="number">135000</span></div><div class="line">Training mini-batch number <span class="number">136000</span></div><div class="line">Training mini-batch number <span class="number">137000</span></div><div class="line">Training mini-batch number <span class="number">138000</span></div><div class="line">Training mini-batch number <span class="number">139000</span> </div><div class="line">Epoch <span class="number">27</span>: validation accuracy <span class="number">98.97</span>%</div><div class="line">Training mini-batch number <span class="number">140000</span></div><div class="line">Training mini-batch number <span class="number">141000</span></div><div class="line">Training mini-batch number <span class="number">142000</span></div><div class="line">Training mini-batch number <span class="number">143000</span></div><div class="line">Training mini-batch number <span class="number">144000</span></div><div class="line">Epoch <span class="number">28</span>: validation accuracy <span class="number">99.12</span>%</div><div class="line">This <span class="keyword">is</span> the best validation accuracy to date.</div><div class="line">The corresponding test accuracy <span class="keyword">is</span> <span class="number">99.22</span>%</div><div class="line">Training mini-batch number <span class="number">145000</span></div><div class="line">Training mini-batch number <span class="number">146000</span></div><div class="line">Training mini-batch number <span class="number">147000</span></div><div class="line">Training mini-batch number <span class="number">148000</span></div><div class="line">Training mini-batch number <span class="number">149000</span></div><div class="line">Epoch <span class="number">29</span>: validation accuracy <span class="number">99.01</span>%</div><div class="line">Training mini-batch number <span class="number">150000</span></div><div class="line">Training mini-batch number <span class="number">151000</span></div><div class="line">Training mini-batch number <span class="number">152000</span></div><div class="line">Training mini-batch number <span class="number">153000</span></div><div class="line">Training mini-batch number <span class="number">154000</span></div><div class="line">Epoch <span class="number">30</span>: validation accuracy <span class="number">99.01</span>%</div><div class="line">Training mini-batch number <span class="number">155000</span></div><div class="line">Training mini-batch number <span class="number">156000</span></div><div class="line">Training mini-batch number <span class="number">157000</span></div><div class="line">Training mini-batch number <span class="number">158000</span></div><div class="line">Training mini-batch number <span class="number">159000</span></div><div class="line">Epoch <span class="number">31</span>: validation accuracy <span class="number">99.09</span>%</div><div class="line">Training mini-batch number <span class="number">160000</span></div><div class="line">Training mini-batch number <span class="number">161000</span></div><div class="line">Training mini-batch number <span class="number">162000</span></div><div class="line">Training mini-batch number <span class="number">163000</span></div><div class="line">Training mini-batch number <span class="number">164000</span></div><div class="line">Epoch <span class="number">32</span>: validation accuracy <span class="number">99.08</span>%</div><div class="line">Training mini-batch number <span class="number">165000</span></div><div class="line">Training mini-batch number <span class="number">166000</span></div><div class="line">Training mini-batch number <span class="number">167000</span></div><div class="line">Training mini-batch number <span class="number">168000</span></div><div class="line">Training mini-batch number <span class="number">169000</span></div><div class="line">Epoch <span class="number">33</span>: validation accuracy <span class="number">99.04</span>%</div><div class="line">Training mini-batch number <span class="number">170000</span></div><div class="line">Training mini-batch number <span class="number">171000</span></div><div class="line">Training mini-batch number <span class="number">172000</span></div><div class="line">Training mini-batch number <span class="number">173000</span></div><div class="line">Training mini-batch number <span class="number">174000</span></div><div class="line">Epoch <span class="number">34</span>: validation accuracy <span class="number">99.06</span>%</div><div class="line">Training mini-batch number <span class="number">175000</span></div><div class="line">Training mini-batch number <span class="number">176000</span></div><div class="line">Training mini-batch number <span class="number">177000</span></div><div class="line">Training mini-batch number <span class="number">178000</span></div><div class="line">Training mini-batch number <span class="number">179000</span></div><div class="line">Epoch <span class="number">35</span>: validation accuracy <span class="number">99.08</span>%</div><div class="line">Training mini-batch number <span class="number">180000</span></div><div class="line">Training mini-batch number <span class="number">181000</span></div><div class="line">Training mini-batch number <span class="number">182000</span></div><div class="line">Training mini-batch number <span class="number">183000</span></div><div class="line">Training mini-batch number <span class="number">184000</span></div><div class="line">Epoch <span class="number">36</span>: validation accuracy <span class="number">99.09</span>%</div><div class="line">Training mini-batch number <span class="number">185000</span></div><div class="line">Training mini-batch number <span class="number">186000</span></div><div class="line">Training mini-batch number <span class="number">187000</span></div><div class="line">Training mini-batch number <span class="number">188000</span></div><div class="line">Training mini-batch number <span class="number">189000</span></div><div class="line">Epoch <span class="number">37</span>: validation accuracy <span class="number">99.07</span>%</div><div class="line">Training mini-batch number <span class="number">190000</span></div></pre></td></tr></table></figure></p>
<p>　　可以看到，第28 Epoch的时候，测试集准确率达到最高的99.22%。<br>　　后面，我闲暇之余又进行了更多次的迭代，测试集准确率最终在68 epoch时达到了<strong>99.30%</strong>。</p>
<h2 id="可视化"><a href="#可视化" class="headerlink" title="可视化"></a>可视化</h2><p>　　本部分将挑选出10000条测试集数据中被误分类的数字进行可视化，选择的分类模型是使用弃权策略并且包含一个卷积层、一个全连接层、一个柔性最大值输出层的神经网络结构，测试集准确率达到90%。因此10000条测试集中有100条数据被误分类。可视化代码如下，该部分代码根据示例代码进行改造得到：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_error_locations</span><span class="params">(net, test_data)</span>:</span></div><div class="line">    test_x, test_y = test_data</div><div class="line"></div><div class="line">    i = T.lscalar()  <span class="comment"># mini-batch index</span></div><div class="line">    net.test_mb_predictions = theano.function(</div><div class="line">        [i], net.layers[<span class="number">-1</span>].y_out,</div><div class="line">        givens=&#123;</div><div class="line">            net.x:</div><div class="line">                test_x[i * net.mini_batch_size: (i + <span class="number">1</span>) * net.mini_batch_size]</div><div class="line">        &#125;)</div><div class="line">    test_predictions = list(np.concatenate(</div><div class="line">        [net.test_mb_predictions(i) <span class="keyword">for</span> i <span class="keyword">in</span> xrange(size(test_data)/net.mini_batch_size)]))</div><div class="line"></div><div class="line">    test_y_eval = test_y.eval()</div><div class="line"></div><div class="line">    error_locations = [j <span class="keyword">for</span> j <span class="keyword">in</span> xrange(len(test_y_eval))</div><div class="line">                       <span class="keyword">if</span> test_predictions[j] != test_y_eval[j]]</div><div class="line"></div><div class="line">    erroneous_predictions = [test_predictions[j]</div><div class="line">                             <span class="keyword">for</span> j <span class="keyword">in</span> error_locations]</div><div class="line">    <span class="keyword">return</span> error_locations, erroneous_predictions</div><div class="line"></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">plot_errors</span><span class="params">(error_locations, erroneous_predictions=None, test_data=None)</span>:</span></div><div class="line">    test_x, test_y = test_data[<span class="number">0</span>].eval(), test_data[<span class="number">1</span>].eval()</div><div class="line">    fig = plt.figure()</div><div class="line">    error_images = [np.array(test_x[i]).reshape(<span class="number">28</span>, <span class="number">-1</span>) <span class="keyword">for</span> i <span class="keyword">in</span> error_locations]</div><div class="line">    row,col = <span class="number">12</span>,<span class="number">10</span></div><div class="line">    n = min(row*col, len(error_locations))</div><div class="line">    <span class="keyword">for</span> j <span class="keyword">in</span> range(n):</div><div class="line">        ax = plt.subplot2grid((row, col), (j/col, j % col))</div><div class="line">        ax.matshow(error_images[j], cmap = matplotlib.cm.binary)</div><div class="line">        ax.text(<span class="number">24</span>, <span class="number">5</span>, test_y[error_locations[j]])</div><div class="line">        <span class="keyword">if</span> erroneous_predictions:</div><div class="line">            ax.text(<span class="number">24</span>, <span class="number">24</span>, erroneous_predictions[j])</div><div class="line">        plt.xticks(np.array([]))</div><div class="line">        plt.yticks(np.array([]))</div><div class="line">    plt.tight_layout()</div><div class="line">    plt.show()</div><div class="line"></div><div class="line"><span class="comment"># draw</span></div><div class="line">error_locations, erroneous_predictions = get_error_locations(net, test_data)</div><div class="line">plot_errors(error_locations, erroneous_predictions,test_data)</div></pre></td></tr></table></figure></p>
<p>　　得到下图：<br><img src="/picture/machine-learning/cnn_error_digits.png" alt="cnn_error_digits"><br>　　如图是误分类的数字，每个数字的右上角是真实的分类，右下角是模型预测的分类。我们可以观察下这100个数字，有些数字即使是我们自己去分类，也很难分辨出来，确实很模棱两可，甚至有些分类我们更认同模型的结果。比如，第一行第8个数，我更认为是9而不是4;第一行第9个数，长得更像9而不是8。因此模型的输出很大程度上是可以接受的。<br>　　更进一步，我使用上述两层卷积结构的神经网络达到的99.30%测试集准确率模型，进行了一次绘图，得到下图，只有70个误分类的数字：<br><img src="/picture/machine-learning/cnn_error_digits_double_conv.png" alt="cnn_error_digits_double_conv"><br>　　我们可以对比一下上面两幅图，看看哪些之前误分类的数字，双层卷积神经网络进行了正确的识别。</p>
<h2 id="保存和加载模型"><a href="#保存和加载模型" class="headerlink" title="保存和加载模型"></a>保存和加载模型</h2><p>　　根据训练过程，我们可以知道上述过程是非常缓慢的。为了测试方便，我们不希望每次都要重新进行训练。因此需要找一个保存和加载模型的方法，这也是书上课后的一个小作业。具体代码如下：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> cPickle</div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">save</span><span class="params">(layers, filename=<span class="string">"../model/params.pkl"</span>)</span>:</span></div><div class="line">    save_file = open(filename, <span class="string">'wb'</span>)  <span class="comment"># this will overwrite current contents</span></div><div class="line">    <span class="keyword">for</span> layer <span class="keyword">in</span> layers:</div><div class="line">        cPickle.dump(layer.w.get_value(borrow=<span class="keyword">True</span>), save_file, <span class="number">-1</span>)</div><div class="line">        cPickle.dump(layer.b.get_value(borrow=<span class="keyword">True</span>), save_file, <span class="number">-1</span>)</div><div class="line">    save_file.close()</div><div class="line"></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">load</span><span class="params">(layers, filename=<span class="string">"../model/params.pkl"</span>)</span>:</span></div><div class="line">    save_file = open(filename,<span class="string">'rb'</span>)</div><div class="line">    <span class="keyword">for</span> layer <span class="keyword">in</span> layers:</div><div class="line">        w_param = cPickle.load(save_file)</div><div class="line">        b_param = cPickle.load(save_file)</div><div class="line">        layer.w.set_value(w_param, borrow=<span class="keyword">True</span>)</div><div class="line">        layer.b.set_value(b_param, borrow=<span class="keyword">True</span>)</div><div class="line">    save_file.close()</div><div class="line"></div><div class="line"></div><div class="line"><span class="comment">#load(net.layers)#加载模型</span></div><div class="line"><span class="comment">#save(net.layers)#保存模型</span></div></pre></td></tr></table></figure></p>
<p>　　这里有个保存模型小小的技巧。如果不想在所有迭代都跑完才进行模型保存的话，可以使用该技巧在任意迭代期手动进行模型的保存。这里针对的是使用pycharm进行python代码coding的同学，可以直接debug整个程序，开始时忽略断点直接运行，然后在你感觉某个epoch后，模型的结果不错的时候，想保存下模型，那么可以打开断点，使程序暂时停止执行，然后使用evaluate expression(alt+F8)功能，直接调用save(net.layers,”../model/param-epoch-轮次数-准确率.pkl”)进行模型的保存。当然，也可以直接写代码，让结果超过你的预期性能的时候，自动进行保存也是可以的。<br>　　这样，只需要简单的修改下SGD方法，当模型是从文件中加载进来的时候，就不需要进行迭代训练，直接进行结果的预测即可。修改过的完整代码如下。</p>
<h1 id="完整代码"><a href="#完整代码" class="headerlink" title="完整代码"></a>完整代码</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div><div class="line">64</div><div class="line">65</div><div class="line">66</div><div class="line">67</div><div class="line">68</div><div class="line">69</div><div class="line">70</div><div class="line">71</div><div class="line">72</div><div class="line">73</div><div class="line">74</div><div class="line">75</div><div class="line">76</div><div class="line">77</div><div class="line">78</div><div class="line">79</div><div class="line">80</div><div class="line">81</div><div class="line">82</div><div class="line">83</div><div class="line">84</div><div class="line">85</div><div class="line">86</div><div class="line">87</div><div class="line">88</div><div class="line">89</div><div class="line">90</div><div class="line">91</div><div class="line">92</div><div class="line">93</div><div class="line">94</div><div class="line">95</div><div class="line">96</div><div class="line">97</div><div class="line">98</div><div class="line">99</div><div class="line">100</div><div class="line">101</div><div class="line">102</div><div class="line">103</div><div class="line">104</div><div class="line">105</div><div class="line">106</div><div class="line">107</div><div class="line">108</div><div class="line">109</div><div class="line">110</div><div class="line">111</div><div class="line">112</div><div class="line">113</div><div class="line">114</div><div class="line">115</div><div class="line">116</div><div class="line">117</div><div class="line">118</div><div class="line">119</div><div class="line">120</div><div class="line">121</div><div class="line">122</div><div class="line">123</div><div class="line">124</div><div class="line">125</div><div class="line">126</div><div class="line">127</div><div class="line">128</div><div class="line">129</div><div class="line">130</div><div class="line">131</div><div class="line">132</div><div class="line">133</div><div class="line">134</div><div class="line">135</div><div class="line">136</div><div class="line">137</div><div class="line">138</div><div class="line">139</div><div class="line">140</div><div class="line">141</div><div class="line">142</div><div class="line">143</div><div class="line">144</div><div class="line">145</div><div class="line">146</div><div class="line">147</div><div class="line">148</div><div class="line">149</div><div class="line">150</div><div class="line">151</div><div class="line">152</div><div class="line">153</div><div class="line">154</div><div class="line">155</div><div class="line">156</div><div class="line">157</div><div class="line">158</div><div class="line">159</div><div class="line">160</div><div class="line">161</div><div class="line">162</div><div class="line">163</div><div class="line">164</div><div class="line">165</div><div class="line">166</div><div class="line">167</div><div class="line">168</div><div class="line">169</div><div class="line">170</div><div class="line">171</div><div class="line">172</div><div class="line">173</div><div class="line">174</div><div class="line">175</div><div class="line">176</div><div class="line">177</div><div class="line">178</div><div class="line">179</div><div class="line">180</div><div class="line">181</div><div class="line">182</div><div class="line">183</div><div class="line">184</div><div class="line">185</div><div class="line">186</div><div class="line">187</div><div class="line">188</div><div class="line">189</div><div class="line">190</div><div class="line">191</div><div class="line">192</div><div class="line">193</div><div class="line">194</div><div class="line">195</div><div class="line">196</div><div class="line">197</div><div class="line">198</div><div class="line">199</div><div class="line">200</div><div class="line">201</div><div class="line">202</div><div class="line">203</div><div class="line">204</div><div class="line">205</div><div class="line">206</div><div class="line">207</div><div class="line">208</div><div class="line">209</div><div class="line">210</div><div class="line">211</div><div class="line">212</div><div class="line">213</div><div class="line">214</div><div class="line">215</div><div class="line">216</div><div class="line">217</div><div class="line">218</div><div class="line">219</div><div class="line">220</div><div class="line">221</div><div class="line">222</div><div class="line">223</div><div class="line">224</div><div class="line">225</div><div class="line">226</div><div class="line">227</div><div class="line">228</div><div class="line">229</div><div class="line">230</div><div class="line">231</div><div class="line">232</div><div class="line">233</div><div class="line">234</div><div class="line">235</div><div class="line">236</div><div class="line">237</div><div class="line">238</div><div class="line">239</div><div class="line">240</div><div class="line">241</div><div class="line">242</div><div class="line">243</div><div class="line">244</div><div class="line">245</div><div class="line">246</div><div class="line">247</div><div class="line">248</div><div class="line">249</div><div class="line">250</div><div class="line">251</div><div class="line">252</div><div class="line">253</div><div class="line">254</div><div class="line">255</div><div class="line">256</div><div class="line">257</div><div class="line">258</div><div class="line">259</div><div class="line">260</div><div class="line">261</div><div class="line">262</div><div class="line">263</div><div class="line">264</div><div class="line">265</div><div class="line">266</div><div class="line">267</div><div class="line">268</div><div class="line">269</div><div class="line">270</div><div class="line">271</div><div class="line">272</div><div class="line">273</div><div class="line">274</div><div class="line">275</div><div class="line">276</div><div class="line">277</div><div class="line">278</div><div class="line">279</div><div class="line">280</div><div class="line">281</div><div class="line">282</div><div class="line">283</div><div class="line">284</div><div class="line">285</div><div class="line">286</div><div class="line">287</div><div class="line">288</div><div class="line">289</div><div class="line">290</div><div class="line">291</div><div class="line">292</div><div class="line">293</div><div class="line">294</div><div class="line">295</div><div class="line">296</div><div class="line">297</div><div class="line">298</div><div class="line">299</div><div class="line">300</div><div class="line">301</div><div class="line">302</div><div class="line">303</div><div class="line">304</div><div class="line">305</div><div class="line">306</div><div class="line">307</div><div class="line">308</div><div class="line">309</div><div class="line">310</div><div class="line">311</div><div class="line">312</div><div class="line">313</div><div class="line">314</div><div class="line">315</div><div class="line">316</div><div class="line">317</div><div class="line">318</div><div class="line">319</div><div class="line">320</div><div class="line">321</div><div class="line">322</div><div class="line">323</div><div class="line">324</div><div class="line">325</div><div class="line">326</div><div class="line">327</div><div class="line">328</div><div class="line">329</div><div class="line">330</div><div class="line">331</div><div class="line">332</div><div class="line">333</div><div class="line">334</div><div class="line">335</div><div class="line">336</div><div class="line">337</div><div class="line">338</div><div class="line">339</div><div class="line">340</div><div class="line">341</div><div class="line">342</div><div class="line">343</div><div class="line">344</div><div class="line">345</div><div class="line">346</div><div class="line">347</div><div class="line">348</div><div class="line">349</div><div class="line">350</div><div class="line">351</div><div class="line">352</div><div class="line">353</div><div class="line">354</div><div class="line">355</div><div class="line">356</div><div class="line">357</div><div class="line">358</div><div class="line">359</div><div class="line">360</div><div class="line">361</div><div class="line">362</div><div class="line">363</div><div class="line">364</div><div class="line">365</div><div class="line">366</div><div class="line">367</div><div class="line">368</div><div class="line">369</div><div class="line">370</div><div class="line">371</div><div class="line">372</div><div class="line">373</div><div class="line">374</div><div class="line">375</div><div class="line">376</div><div class="line">377</div><div class="line">378</div><div class="line">379</div><div class="line">380</div><div class="line">381</div><div class="line">382</div><div class="line">383</div><div class="line">384</div><div class="line">385</div><div class="line">386</div><div class="line">387</div><div class="line">388</div><div class="line">389</div><div class="line">390</div><div class="line">391</div><div class="line">392</div><div class="line">393</div><div class="line">394</div><div class="line">395</div><div class="line">396</div><div class="line">397</div><div class="line">398</div><div class="line">399</div><div class="line">400</div><div class="line">401</div><div class="line">402</div><div class="line">403</div><div class="line">404</div><div class="line">405</div><div class="line">406</div><div class="line">407</div><div class="line">408</div><div class="line">409</div><div class="line">410</div><div class="line">411</div><div class="line">412</div><div class="line">413</div><div class="line">414</div><div class="line">415</div><div class="line">416</div><div class="line">417</div></pre></td><td class="code"><pre><div class="line"><span class="string">"""network3.py</span></div><div class="line">~~~~~~~~~~~~~~</div><div class="line"></div><div class="line">A Theano-based program for training and running simple neural</div><div class="line">networks.</div><div class="line"></div><div class="line">Supports several layer types (fully connected, convolutional, max</div><div class="line">pooling, softmax), and activation functions (sigmoid, tanh, and</div><div class="line">rectified linear units, with more easily added).</div><div class="line"></div><div class="line">When run on a CPU, this program is much faster than network.py and</div><div class="line">network2.py.  However, unlike network.py and network2.py it can also</div><div class="line">be run on a GPU, which makes it faster still.</div><div class="line"></div><div class="line">Because the code is based on Theano, the code is different in many</div><div class="line">ways from network.py and network2.py.  However, where possible I have</div><div class="line">tried to maintain consistency with the earlier programs.  In</div><div class="line">particular, the API is similar to network2.py.  Note that I have</div><div class="line">focused on making the code simple, easily readable, and easily</div><div class="line">modifiable.  It is not optimized, and omits many desirable features.</div><div class="line"></div><div class="line">This program incorporates ideas from the Theano documentation on</div><div class="line">convolutional neural nets (notably,</div><div class="line">http://deeplearning.net/tutorial/lenet.html ), from Misha Denil's</div><div class="line">implementation of dropout (https://github.com/mdenil/dropout ), and</div><div class="line">from Chris Olah (http://colah.github.io ).</div><div class="line"></div><div class="line">Written for Theano 0.6 and 0.7, needs some changes for more recent</div><div class="line">versions of Theano.</div><div class="line"></div><div class="line">"""</div><div class="line"></div><div class="line"><span class="comment">#### Libraries</span></div><div class="line"><span class="comment"># Standard library</span></div><div class="line"><span class="keyword">import</span> cPickle</div><div class="line"><span class="keyword">import</span> gzip</div><div class="line"></div><div class="line"><span class="comment"># Third-party libraries</span></div><div class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</div><div class="line"><span class="keyword">import</span> theano</div><div class="line"><span class="keyword">import</span> theano.tensor <span class="keyword">as</span> T</div><div class="line"><span class="keyword">from</span> theano.tensor.nnet <span class="keyword">import</span> conv</div><div class="line"><span class="keyword">from</span> theano.tensor.nnet <span class="keyword">import</span> softmax</div><div class="line"><span class="keyword">from</span> theano.tensor <span class="keyword">import</span> shared_randomstreams</div><div class="line"><span class="keyword">from</span> theano.tensor.signal <span class="keyword">import</span> pool</div><div class="line"></div><div class="line"><span class="comment"># Activation functions for neurons</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">linear</span><span class="params">(z)</span>:</span> <span class="keyword">return</span> z</div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">ReLU</span><span class="params">(z)</span>:</span> <span class="keyword">return</span> T.maximum(<span class="number">0.0</span>, z)</div><div class="line"><span class="keyword">from</span> theano.tensor.nnet <span class="keyword">import</span> sigmoid</div><div class="line"><span class="keyword">from</span> theano.tensor <span class="keyword">import</span> tanh</div><div class="line"><span class="keyword">import</span> matplotlib</div><div class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</div><div class="line"><span class="comment">#import seaborn as sns</span></div><div class="line"><span class="keyword">import</span> cPickle</div><div class="line"></div><div class="line"></div><div class="line"><span class="comment">#### Constants</span></div><div class="line">GPU = <span class="keyword">False</span></div><div class="line"><span class="keyword">if</span> GPU:</div><div class="line">    <span class="keyword">print</span> <span class="string">"Trying to run under a GPU.  If this is not desired, then modify "</span>+\</div><div class="line">        <span class="string">"network3.py\nto set the GPU flag to False."</span></div><div class="line">    <span class="keyword">try</span>: theano.config.device = <span class="string">'gpu'</span></div><div class="line">    <span class="keyword">except</span>: <span class="keyword">pass</span> <span class="comment"># it's already set</span></div><div class="line">    theano.config.floatX = <span class="string">'float32'</span></div><div class="line"><span class="keyword">else</span>:</div><div class="line">    <span class="keyword">print</span> <span class="string">"Running with a CPU.  If this is not desired, then the modify "</span>+\</div><div class="line">        <span class="string">"network3.py to set\nthe GPU flag to True."</span></div><div class="line"></div><div class="line"><span class="comment">#### Load the MNIST data</span></div><div class="line"><span class="function"><span class="keyword">def</span>  <span class="title">load_data_shared</span><span class="params">(filename=<span class="string">"../data/mnist.pkl.gz"</span>)</span>:</span></div><div class="line">    f = gzip.open(filename, <span class="string">'rb'</span>)</div><div class="line">    training_data, validation_data, test_data = cPickle.load(f)</div><div class="line">    <span class="keyword">print</span> <span class="string">"test len:"</span>,len(test_data[<span class="number">0</span>])</div><div class="line">    f.close()</div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">shared</span><span class="params">(data)</span>:</span></div><div class="line">        <span class="string">"""Place the data into shared variables.  This allows Theano to copy</span></div><div class="line">        the data to the GPU, if one is available.</div><div class="line"></div><div class="line">        """</div><div class="line">        shared_x = theano.shared(</div><div class="line">            np.asarray(data[<span class="number">0</span>], dtype=theano.config.floatX), borrow=<span class="keyword">True</span>)</div><div class="line">        shared_y = theano.shared(</div><div class="line">            np.asarray(data[<span class="number">1</span>], dtype=theano.config.floatX), borrow=<span class="keyword">True</span>)</div><div class="line">        <span class="keyword">return</span> shared_x, T.cast(shared_y, <span class="string">"int32"</span>)</div><div class="line">    <span class="keyword">return</span> [shared(training_data), shared(validation_data), shared(test_data)]</div><div class="line"></div><div class="line"><span class="comment">#### Main class used to construct and train networks</span></div><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">Network</span><span class="params">(object)</span>:</span></div><div class="line"></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, layers, mini_batch_size)</span>:</span></div><div class="line">        <span class="string">"""Takes a list of `layers`, describing the network architecture, and</span></div><div class="line">        a value for the `mini_batch_size` to be used during training</div><div class="line">        by stochastic gradient descent.</div><div class="line"></div><div class="line">        """</div><div class="line">        self.layers = layers</div><div class="line">        self.mini_batch_size = mini_batch_size</div><div class="line">        self.params = [param <span class="keyword">for</span> layer <span class="keyword">in</span> self.layers <span class="keyword">for</span> param <span class="keyword">in</span> layer.params]</div><div class="line">        self.x = T.matrix(<span class="string">"x"</span>)</div><div class="line">        self.y = T.ivector(<span class="string">"y"</span>)</div><div class="line">        init_layer = self.layers[<span class="number">0</span>]</div><div class="line">        init_layer.set_inpt(self.x, self.x, self.mini_batch_size)</div><div class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> xrange(<span class="number">1</span>, len(self.layers)):</div><div class="line">            prev_layer, layer  = self.layers[j<span class="number">-1</span>], self.layers[j]</div><div class="line">            layer. set_inpt(</div><div class="line">                prev_layer.output, prev_layer.output_dropout, self.mini_batch_size)</div><div class="line">        self.output = self.layers[<span class="number">-1</span>].output</div><div class="line">        self.output_dropout = self.layers[<span class="number">-1</span>].output_dropout</div><div class="line"></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">SGD</span><span class="params">(self, training_data, epochs, mini_batch_size, eta,</span></span></div><div class="line">            validation_data, test_data, lmbda=<span class="number">0.0</span>, is_load_model_from_file=False):</div><div class="line">        <span class="string">"""Train the network using mini-batch stochastic gradient descent."""</span></div><div class="line">        training_x, training_y = training_data</div><div class="line">        validation_x, validation_y = validation_data</div><div class="line">        test_x, test_y = test_data</div><div class="line"></div><div class="line">        <span class="comment"># compute number of minibatches for training, validation and testing</span></div><div class="line">        num_training_batches = size(training_data)/mini_batch_size</div><div class="line">        num_validation_batches = size(validation_data)/mini_batch_size</div><div class="line">        num_test_batches = size(test_data)/mini_batch_size</div><div class="line"></div><div class="line">        <span class="comment"># define the (regularized) cost function, symbolic gradients, and updates</span></div><div class="line">        l2_norm_squared = sum([(layer.w**<span class="number">2</span>).sum() <span class="keyword">for</span> layer <span class="keyword">in</span> self.layers])</div><div class="line">        cost = self.layers[<span class="number">-1</span>].cost(self)+\</div><div class="line">               <span class="number">0.5</span>*lmbda*l2_norm_squared/num_training_batches</div><div class="line">        grads = T.grad(cost, self.params)</div><div class="line">        updates = [(param, param-eta*grad)</div><div class="line">                   <span class="keyword">for</span> param, grad <span class="keyword">in</span> zip(self.params, grads)]</div><div class="line"></div><div class="line">        <span class="comment"># define functions to train a mini-batch, and to compute the</span></div><div class="line">        <span class="comment"># accuracy in validation and test mini-batches.</span></div><div class="line">        i = T.lscalar() <span class="comment"># mini-batch index</span></div><div class="line">        train_mb = theano.function(</div><div class="line">            [i], cost, updates=updates,</div><div class="line">            givens=&#123;</div><div class="line">                self.x:</div><div class="line">                training_x[i*self.mini_batch_size: (i+<span class="number">1</span>)*self.mini_batch_size],</div><div class="line">                self.y:</div><div class="line">                training_y[i*self.mini_batch_size: (i+<span class="number">1</span>)*self.mini_batch_size]</div><div class="line">            &#125;)</div><div class="line">        validate_mb_accuracy = theano.function(</div><div class="line">            [i], self.layers[<span class="number">-1</span>].accuracy(self.y),</div><div class="line">            givens=&#123;</div><div class="line">                self.x:</div><div class="line">                validation_x[i*self.mini_batch_size: (i+<span class="number">1</span>)*self.mini_batch_size],</div><div class="line">                self.y:</div><div class="line">                validation_y[i*self.mini_batch_size: (i+<span class="number">1</span>)*self.mini_batch_size]</div><div class="line">            &#125;)</div><div class="line">        test_mb_accuracy = theano.function(</div><div class="line">            [i], self.layers[<span class="number">-1</span>].accuracy(self.y),</div><div class="line">            givens=&#123;</div><div class="line">                self.x:</div><div class="line">                test_x[i*self.mini_batch_size: (i+<span class="number">1</span>)*self.mini_batch_size],</div><div class="line">                self.y:</div><div class="line">                test_y[i*self.mini_batch_size: (i+<span class="number">1</span>)*self.mini_batch_size]</div><div class="line">            &#125;)</div><div class="line">        self.test_mb_predictions = theano.function(</div><div class="line">            [i], self.layers[<span class="number">-1</span>].y_out,</div><div class="line">            givens=&#123;</div><div class="line">                self.x:</div><div class="line">                test_x[i*self.mini_batch_size: (i+<span class="number">1</span>)*self.mini_batch_size]</div><div class="line">            &#125;)</div><div class="line"></div><div class="line">        <span class="comment"># Load model from file</span></div><div class="line">        <span class="keyword">if</span> is_load_model_from_file:</div><div class="line">            <span class="keyword">print</span> <span class="string">'use the model loaded from file...'</span></div><div class="line">            test_accuracy = np.mean(</div><div class="line">                [test_mb_accuracy(j) <span class="keyword">for</span> j <span class="keyword">in</span> xrange(num_test_batches)])</div><div class="line">            print(<span class="string">'The corresponding test accuracy is &#123;0:.2%&#125;'</span>.format(</div><div class="line">                test_accuracy))</div><div class="line"></div><div class="line">        <span class="comment"># Do the actual training</span></div><div class="line">        <span class="keyword">else</span>:</div><div class="line">            <span class="keyword">print</span> <span class="string">"begin training..."</span></div><div class="line">            best_validation_accuracy = <span class="number">0.0</span></div><div class="line">            <span class="keyword">for</span> epoch <span class="keyword">in</span> xrange(epochs):</div><div class="line">                <span class="keyword">for</span> minibatch_index <span class="keyword">in</span> xrange(num_training_batches):</div><div class="line">                    iteration = num_training_batches * epoch + minibatch_index</div><div class="line">                    <span class="keyword">if</span> iteration % <span class="number">1000</span> == <span class="number">0</span>:</div><div class="line">                        print(<span class="string">"Training mini-batch number &#123;0&#125;"</span>.format(iteration))</div><div class="line">                    cost_ij = train_mb(minibatch_index)</div><div class="line">                    <span class="keyword">if</span> (iteration + <span class="number">1</span>) % num_training_batches == <span class="number">0</span>:</div><div class="line">                        validation_accuracy = np.mean(</div><div class="line">                            [validate_mb_accuracy(j) <span class="keyword">for</span> j <span class="keyword">in</span> xrange(num_validation_batches)])</div><div class="line">                        print(<span class="string">"Epoch &#123;0&#125;: validation accuracy &#123;1:.2%&#125;"</span>.format(</div><div class="line">                            epoch, validation_accuracy))</div><div class="line">                        <span class="keyword">if</span> validation_accuracy &gt;= best_validation_accuracy:</div><div class="line">                            print(<span class="string">"This is the best validation accuracy to date."</span>)</div><div class="line">                            best_validation_accuracy = validation_accuracy</div><div class="line">                            best_iteration = iteration</div><div class="line">                            <span class="keyword">if</span> test_data:</div><div class="line">                                test_accuracy = np.mean(</div><div class="line">                                    [test_mb_accuracy(j) <span class="keyword">for</span> j <span class="keyword">in</span> xrange(num_test_batches)])</div><div class="line">                                print(<span class="string">'The corresponding test accuracy is &#123;0:.2%&#125;'</span>.format(</div><div class="line">                                    test_accuracy))</div><div class="line">            print(<span class="string">"Finished training network."</span>)</div><div class="line">            print(<span class="string">"Best validation accuracy of &#123;0:.2%&#125; obtained at iteration &#123;1&#125;"</span>.format(</div><div class="line">                best_validation_accuracy, best_iteration))</div><div class="line">            print(<span class="string">"Corresponding test accuracy of &#123;0:.2%&#125;"</span>.format(test_accuracy))</div><div class="line"></div><div class="line"></div><div class="line"><span class="comment">#### Define layer types</span></div><div class="line"></div><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">ConvPoolLayer</span><span class="params">(object)</span>:</span></div><div class="line">    <span class="string">"""Used to create a combination of a convolutional and a max-pooling</span></div><div class="line">    layer.  A more sophisticated implementation would separate the</div><div class="line">    two, but for our purposes we'll always use them together, and it</div><div class="line">    simplifies the code, so it makes sense to combine them.</div><div class="line"></div><div class="line">    """</div><div class="line"></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, filter_shape, image_shape, poolsize=<span class="params">(<span class="number">2</span>, <span class="number">2</span>)</span>,</span></span></div><div class="line">                 activation_fn=sigmoid):</div><div class="line">        <span class="string">"""`filter_shape` is a tuple of length 4, whose entries are the number</span></div><div class="line">        of filters, the number of input feature maps, the filter height, and the</div><div class="line">        filter width.</div><div class="line"></div><div class="line">        `image_shape` is a tuple of length 4, whose entries are the</div><div class="line">        mini-batch size, the number of input feature maps, the image</div><div class="line">        height, and the image width.</div><div class="line"></div><div class="line">        `poolsize` is a tuple of length 2, whose entries are the y and</div><div class="line">        x pooling sizes.</div><div class="line"></div><div class="line">        """</div><div class="line">        self.filter_shape = filter_shape</div><div class="line">        self.image_shape = image_shape</div><div class="line">        self.poolsize = poolsize</div><div class="line">        self.activation_fn=activation_fn</div><div class="line">        <span class="comment"># initialize weights and biases</span></div><div class="line">        n_out = (filter_shape[<span class="number">0</span>]*np.prod(filter_shape[<span class="number">2</span>:])/np.prod(poolsize))</div><div class="line">        self.w = theano.shared(</div><div class="line">            np.asarray(</div><div class="line">                np.random.normal(loc=<span class="number">0</span>, scale=np.sqrt(<span class="number">1.0</span>/n_out), size=filter_shape),</div><div class="line">                dtype=theano.config.floatX),</div><div class="line">            borrow=<span class="keyword">True</span>)</div><div class="line">        self.b = theano.shared(</div><div class="line">            np.asarray(</div><div class="line">                np.random.normal(loc=<span class="number">0</span>, scale=<span class="number">1.0</span>, size=(filter_shape[<span class="number">0</span>],)),</div><div class="line">                dtype=theano.config.floatX),</div><div class="line">            borrow=<span class="keyword">True</span>)</div><div class="line">        self.params = [self.w, self.b]</div><div class="line"></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">set_inpt</span><span class="params">(self, inpt, inpt_dropout, mini_batch_size)</span>:</span></div><div class="line">        self.inpt = inpt.reshape(self.image_shape)</div><div class="line">        conv_out = conv.conv2d(</div><div class="line">            input=self.inpt, filters=self.w, filter_shape=self.filter_shape,</div><div class="line">            image_shape=self.image_shape)</div><div class="line">        pooled_out = pool.pool_2d(</div><div class="line">            input=conv_out, ds=self.poolsize, ignore_border=<span class="keyword">True</span>, mode=<span class="string">'max'</span>)</div><div class="line">        self.output = self.activation_fn(</div><div class="line">            pooled_out + self.b.dimshuffle(<span class="string">'x'</span>, <span class="number">0</span>, <span class="string">'x'</span>, <span class="string">'x'</span>))</div><div class="line">        self.output_dropout = self.output <span class="comment"># no dropout in the convolutional layers</span></div><div class="line"></div><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">FullyConnectedLayer</span><span class="params">(object)</span>:</span></div><div class="line"></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, n_in, n_out, activation_fn=sigmoid, p_dropout=<span class="number">0.0</span>)</span>:</span></div><div class="line">        self.n_in = n_in</div><div class="line">        self.n_out = n_out</div><div class="line">        self.activation_fn = activation_fn</div><div class="line">        self.p_dropout = p_dropout</div><div class="line">        <span class="comment"># Initialize weights and biases</span></div><div class="line">        self.w = theano.shared(</div><div class="line">            np.asarray(</div><div class="line">                np.random.normal(</div><div class="line">                    loc=<span class="number">0.0</span>, scale=np.sqrt(<span class="number">1.0</span>/n_out), size=(n_in, n_out)),</div><div class="line">                dtype=theano.config.floatX),</div><div class="line">            name=<span class="string">'w'</span>, borrow=<span class="keyword">True</span>)</div><div class="line">        self.b = theano.shared(</div><div class="line">            np.asarray(np.random.normal(loc=<span class="number">0.0</span>, scale=<span class="number">1.0</span>, size=(n_out,)),</div><div class="line">                       dtype=theano.config.floatX),</div><div class="line">            name=<span class="string">'b'</span>, borrow=<span class="keyword">True</span>)</div><div class="line">        self.params = [self.w, self.b]</div><div class="line"></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">set_inpt</span><span class="params">(self, inpt, inpt_dropout, mini_batch_size)</span>:</span></div><div class="line">        self.inpt = inpt.reshape((mini_batch_size, self.n_in))</div><div class="line">        self.output = self.activation_fn(</div><div class="line">            (<span class="number">1</span>-self.p_dropout)*T.dot(self.inpt, self.w) + self.b)</div><div class="line">        self.y_out = T.argmax(self.output, axis=<span class="number">1</span>)</div><div class="line">        self.inpt_dropout = dropout_layer(</div><div class="line">            inpt_dropout.reshape((mini_batch_size, self.n_in)), self.p_dropout)</div><div class="line">        self.output_dropout = self.activation_fn(</div><div class="line">            T.dot(self.inpt_dropout, self.w) + self.b)</div><div class="line"></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">accuracy</span><span class="params">(self, y)</span>:</span></div><div class="line">        <span class="string">"Return the accuracy for the mini-batch."</span></div><div class="line">        <span class="keyword">return</span> T.mean(T.eq(y, self.y_out))</div><div class="line"></div><div class="line"></div><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">SoftmaxLayer</span><span class="params">(object)</span>:</span></div><div class="line"></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, n_in, n_out, p_dropout=<span class="number">0.0</span>)</span>:</span></div><div class="line">        self.n_in = n_in</div><div class="line">        self.n_out = n_out</div><div class="line">        self.p_dropout = p_dropout</div><div class="line">        <span class="comment"># Initialize weights and biases</span></div><div class="line">        self.w = theano.shared(</div><div class="line">            np.zeros((n_in, n_out), dtype=theano.config.floatX),</div><div class="line">            name=<span class="string">'w'</span>, borrow=<span class="keyword">True</span>)</div><div class="line">        self.b = theano.shared(</div><div class="line">            np.zeros((n_out,), dtype=theano.config.floatX),</div><div class="line">            name=<span class="string">'b'</span>, borrow=<span class="keyword">True</span>)</div><div class="line">        self.params = [self.w, self.b]</div><div class="line"></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">set_inpt</span><span class="params">(self, inpt, inpt_dropout, mini_batch_size)</span>:</span></div><div class="line">        self.inpt = inpt.reshape((mini_batch_size, self.n_in))</div><div class="line">        self.output = softmax((<span class="number">1</span>-self.p_dropout)*T.dot(self.inpt, self.w) + self.b)</div><div class="line">        self.y_out = T.argmax(self.output, axis=<span class="number">1</span>)</div><div class="line">        self.inpt_dropout = dropout_layer(</div><div class="line">            inpt_dropout.reshape((mini_batch_size, self.n_in)), self.p_dropout)</div><div class="line">        self.output_dropout = softmax(T.dot(self.inpt_dropout, self.w) + self.b)</div><div class="line"></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">cost</span><span class="params">(self, net)</span>:</span></div><div class="line">        <span class="string">"Return the log-likelihood cost."</span></div><div class="line">        <span class="keyword">return</span> -T.mean(T.log(self.output_dropout)[T.arange(net.y.shape[<span class="number">0</span>]), net.y])</div><div class="line"></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">accuracy</span><span class="params">(self, y)</span>:</span></div><div class="line">        <span class="string">"Return the accuracy for the mini-batch."</span></div><div class="line">        <span class="keyword">return</span> T.mean(T.eq(y, self.y_out))</div><div class="line"></div><div class="line"></div><div class="line"><span class="comment">#### Miscellanea</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">size</span><span class="params">(data)</span>:</span></div><div class="line">    <span class="string">"Return the size of the dataset `data`."</span></div><div class="line">    <span class="keyword">return</span> data[<span class="number">0</span>].get_value(borrow=<span class="keyword">True</span>).shape[<span class="number">0</span>]</div><div class="line"></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">dropout_layer</span><span class="params">(layer, p_dropout)</span>:</span></div><div class="line">    srng = shared_randomstreams.RandomStreams(</div><div class="line">        np.random.RandomState(<span class="number">0</span>).randint(<span class="number">999999</span>))</div><div class="line">    mask = srng.binomial(n=<span class="number">1</span>, p=<span class="number">1</span>-p_dropout, size=layer.shape)</div><div class="line">    <span class="keyword">return</span> layer*T.cast(mask, theano.config.floatX)</div><div class="line"></div><div class="line"></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_error_locations</span><span class="params">(net, test_data)</span>:</span></div><div class="line">    test_x, test_y = test_data</div><div class="line"></div><div class="line">    i = T.lscalar()  <span class="comment"># mini-batch index</span></div><div class="line">    net.test_mb_predictions = theano.function(</div><div class="line">        [i], net.layers[<span class="number">-1</span>].y_out,</div><div class="line">        givens=&#123;</div><div class="line">            net.x:</div><div class="line">                test_x[i * net.mini_batch_size: (i + <span class="number">1</span>) * net.mini_batch_size]</div><div class="line">        &#125;)</div><div class="line">    test_predictions = list(np.concatenate(</div><div class="line">        [net.test_mb_predictions(i) <span class="keyword">for</span> i <span class="keyword">in</span> xrange(size(test_data)/net.mini_batch_size)]))</div><div class="line"></div><div class="line">    test_y_eval = test_y.eval()</div><div class="line"></div><div class="line">    error_locations = [j <span class="keyword">for</span> j <span class="keyword">in</span> xrange(len(test_y_eval))</div><div class="line">                       <span class="keyword">if</span> test_predictions[j] != test_y_eval[j]]</div><div class="line"></div><div class="line">    erroneous_predictions = [test_predictions[j]</div><div class="line">                             <span class="keyword">for</span> j <span class="keyword">in</span> error_locations]</div><div class="line"></div><div class="line">    <span class="keyword">return</span> error_locations, erroneous_predictions</div><div class="line"></div><div class="line"></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">plot_errors</span><span class="params">(error_locations, erroneous_predictions=None, test_data=None)</span>:</span></div><div class="line">    test_x, test_y = test_data[<span class="number">0</span>].eval(), test_data[<span class="number">1</span>].eval()</div><div class="line">    fig = plt.figure()</div><div class="line">    error_images = [np.array(test_x[i]).reshape(<span class="number">28</span>, <span class="number">-1</span>) <span class="keyword">for</span> i <span class="keyword">in</span> error_locations]</div><div class="line">    row,col = <span class="number">12</span>,<span class="number">10</span></div><div class="line">    n = min(row*col, len(error_locations))</div><div class="line">    <span class="keyword">for</span> j <span class="keyword">in</span> range(n):</div><div class="line">        ax = plt.subplot2grid((row, col), (j/col, j % col))</div><div class="line">        ax.matshow(error_images[j], cmap = matplotlib.cm.binary)</div><div class="line">        ax.text(<span class="number">24</span>, <span class="number">5</span>, test_y[error_locations[j]])</div><div class="line">        <span class="keyword">if</span> erroneous_predictions:</div><div class="line">            ax.text(<span class="number">24</span>, <span class="number">24</span>, erroneous_predictions[j])</div><div class="line">        plt.xticks(np.array([]))</div><div class="line">        plt.yticks(np.array([]))</div><div class="line">    plt.tight_layout()</div><div class="line">    plt.show()</div><div class="line"></div><div class="line"></div><div class="line"></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">save</span><span class="params">(layers, filename=<span class="string">"../model/params.pkl"</span>)</span>:</span></div><div class="line">    save_file = open(filename, <span class="string">'wb'</span>)  <span class="comment"># this will overwrite current contents</span></div><div class="line">    <span class="keyword">for</span> layer <span class="keyword">in</span> layers:</div><div class="line">        cPickle.dump(layer.w.get_value(borrow=<span class="keyword">True</span>), save_file, <span class="number">-1</span>)</div><div class="line">        cPickle.dump(layer.b.get_value(borrow=<span class="keyword">True</span>), save_file, <span class="number">-1</span>)</div><div class="line">    save_file.close()</div><div class="line"></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">load</span><span class="params">(layers, filename=<span class="string">"../model/params.pkl"</span>)</span>:</span></div><div class="line">    save_file = open(filename,<span class="string">'rb'</span>)</div><div class="line">    <span class="keyword">for</span> layer <span class="keyword">in</span> layers:</div><div class="line">        w_param = cPickle.load(save_file)</div><div class="line">        b_param = cPickle.load(save_file)</div><div class="line">        layer.w.set_value(w_param, borrow=<span class="keyword">True</span>)</div><div class="line">        layer.b.set_value(b_param, borrow=<span class="keyword">True</span>)</div><div class="line">    save_file.close()</div><div class="line"></div><div class="line"></div><div class="line"><span class="comment"># running</span></div><div class="line"><span class="keyword">if</span> __name__ == <span class="string">"__main__"</span>:</div><div class="line">    is_load_model_from_file = <span class="keyword">False</span></div><div class="line"></div><div class="line">    training_data, validation_data, test_data = load_data_shared()</div><div class="line">    mini_batch_size = <span class="number">10</span></div><div class="line">    net = Network(</div><div class="line">         [ConvPoolLayer(image_shape=(mini_batch_size, <span class="number">1</span>, <span class="number">28</span>, <span class="number">28</span>), filter_shape=(<span class="number">20</span>, <span class="number">1</span>, <span class="number">5</span>, <span class="number">5</span>), poolsize=(<span class="number">2</span>, <span class="number">2</span>)),</div><div class="line">            FullyConnectedLayer(n_in=<span class="number">20</span> * <span class="number">12</span> * <span class="number">12</span>, n_out=<span class="number">100</span>, p_dropout=<span class="number">0.3</span>),</div><div class="line">            SoftmaxLayer(n_in=<span class="number">100</span>, n_out=<span class="number">10</span>)], mini_batch_size)</div><div class="line"></div><div class="line">    <span class="keyword">if</span> is_load_model_from_file:</div><div class="line">        load(net.layers)</div><div class="line"></div><div class="line">    net.SGD(training_data, <span class="number">30</span>, mini_batch_size, <span class="number">0.1</span>, validation_data, test_data, is_load_model_from_file=is_load_model_from_file)</div><div class="line"></div><div class="line">    <span class="comment"># if training,then save model</span></div><div class="line">    <span class="keyword">if</span> is_load_model_from_file == <span class="keyword">False</span>:</div><div class="line">        save(net.layers)</div><div class="line"></div><div class="line">    <span class="comment"># draw error classification digits</span></div><div class="line">    error_locations, erroneous_predictions=get_error_locations(net, test_data)</div><div class="line">    plot_errors(error_locations, erroneous_predictions,test_data)</div></pre></td></tr></table></figure>
<h1 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h1><p><a href="http://neuralnetworksanddeeplearning.com/chap6.html" target="_blank" rel="external">神经网络和深度学习入门</a><br><a href="http://deeplearning.net/software/theano/" target="_blank" rel="external">Theano教程</a></p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;　　本文将介绍使用Python书写“卷积神经网络”代码的具体步骤和细节，本文会采用Python开源库Theano，Theano封装了卷积等操作，使用起来比较方便。具体代码可参考&lt;a href=&quot;http://neuralnetworksanddeeplearning.com/chap6.html&quot;&gt;神经网络和深度学习教程&lt;/a&gt;。在本文中，卷积神经网络对MNIST手写数字的预测性能，测试集准确率可以达到99.30%。&lt;br&gt;
    
    </summary>
    
      <category term="深度学习" scheme="xtf615.com/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="神经网络" scheme="xtf615.com/tags/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/"/>
    
      <category term="深度学习" scheme="xtf615.com/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>前馈神经网络实践</title>
    <link href="xtf615.com/2017/08/08/feedforward-neural-network-handwritten/"/>
    <id>xtf615.com/2017/08/08/feedforward-neural-network-handwritten/</id>
    <published>2017-08-08T06:07:05.000Z</published>
    <updated>2017-08-17T01:34:57.261Z</updated>
    
    <content type="html"><![CDATA[<p>　　本文将介绍使用Python手写“前馈神经网络”代码的具体步骤和细节。前馈神经网络采用随机梯度下降算法进行学习，代价函数的梯度计算方法使用的是反向传播算法。具体代码可参考<a href="http://neuralnetworksanddeeplearning.com/chap1.html" target="_blank" rel="external">神经网络和深度学习教程</a>。<br><a id="more"></a></p>
<h1 id="数据集的加载"><a href="#数据集的加载" class="headerlink" title="数据集的加载"></a>数据集的加载</h1><p>　　本文使用的数据集来自MNIST。MNIST是一个手写数字的数据库，它提供了六万的训练集和一万的测试集。每个手写数字图片都已经被规范处理过，是一张放在中间部位的28px*28px的灰度图。本文将训练集进一步划分成训练集和验证集，得到50000条训练集，10000条验证集，10000万条测试集。</p>
<h2 id="细节代码"><a href="#细节代码" class="headerlink" title="细节代码"></a>细节代码</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line">mnist_loader.py：</div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">load_data</span><span class="params">()</span>:</span></div><div class="line">    f = gzip.open(<span class="string">'../data/mnist.pkl.gz'</span>, <span class="string">'rb'</span>)</div><div class="line">    training_data, validation_data, test_data = cPickle.load(f)</div><div class="line">    f.close()</div><div class="line">    <span class="keyword">return</span> (training_data, validation_data, test_data)</div></pre></td></tr></table></figure>
<p>　　上述代码cPickle.load(f)返回一个元组，包含训练集，验证集和测试集。训练集返回的是一个由两个元素构成的元组，第一个元素是包含50000条数据的numpy.ndarray，规格是(50000L, 784L)。即每条数据有784个值，代表了该手写数字的28*28=784个像素点。第二个元素是numpy ndarray数组，规格的是(50000L,)，每个分量代表的是第一个元素中相对应位置的数据的数字分类。<br>　　可以发现该数据虽然很规整，但在神经网络中用起来不够方便，我们需要对数据进一步处理一下。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div></pre></td><td class="code"><pre><div class="line">mnist_loader.py：</div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">load_data_wrapper</span><span class="params">()</span>:</span></div><div class="line">    tr_d, va_d, te_d = load_data()</div><div class="line">    training_inputs = [np.reshape(x, (<span class="number">784</span>, <span class="number">1</span>)) <span class="keyword">for</span> x <span class="keyword">in</span> tr_d[<span class="number">0</span>]]<span class="comment">#把数据处理成列向量形式</span></div><div class="line">    training_results = [vectorized_result(y) <span class="keyword">for</span> y <span class="keyword">in</span> tr_d[<span class="number">1</span>]]<span class="comment">#把分类标记向量化</span></div><div class="line">    training_data = zip(training_inputs, training_results)<span class="comment">#每条数据都是2-tuples形式</span></div><div class="line">    validation_inputs = [np.reshape(x, (<span class="number">784</span>, <span class="number">1</span>)) <span class="keyword">for</span> x <span class="keyword">in</span> va_d[<span class="number">0</span>]]</div><div class="line">    validation_data = zip(validation_inputs, va_d[<span class="number">1</span>])</div><div class="line">    test_inputs = [np.reshape(x, (<span class="number">784</span>, <span class="number">1</span>)) <span class="keyword">for</span> x <span class="keyword">in</span> te_d[<span class="number">0</span>]]</div><div class="line">    test_data = zip(test_inputs, te_d[<span class="number">1</span>])</div><div class="line">    <span class="keyword">return</span> (training_data, validation_data, test_data)</div><div class="line"></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">vectorized_result</span><span class="params">(j)</span>:</span></div><div class="line">    e = np.zeros((<span class="number">10</span>, <span class="number">1</span>))</div><div class="line">    e[j] = <span class="number">1.0</span></div><div class="line">    <span class="keyword">return</span> e</div></pre></td></tr></table></figure></p>
<p>　　上述处理后，训练集是一个包含50000条数据，每一条数据都是2-tuples(x,y)形式，x是一个784维度的numpy.ndarray，规格是(784L, 1L)，y是一个10维numpy.ndarray，规格是(10L, 1L)，代表的是相应数字分类的单位向量。验证集和测试集类似，只不过y是具体的数字分类，而不是向量。</p>
<h2 id="具体使用"><a href="#具体使用" class="headerlink" title="具体使用"></a>具体使用</h2><p>　　具体使用时，只要如下代码即可：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> mnist_loader</div><div class="line">training_data, validation_data, test_data = mnist_loader.load_data_wrapper()</div></pre></td></tr></table></figure></p>
<h1 id="神经网络算法"><a href="#神经网络算法" class="headerlink" title="神经网络算法"></a>神经网络算法</h1><h2 id="Network对象初始化"><a href="#Network对象初始化" class="headerlink" title="Network对象初始化"></a>Network对象初始化</h2><p>　　我们需要对神经网络结构进行初始化，具体而言就是对权重和偏置进行初始化。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div></pre></td><td class="code"><pre><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">Network</span><span class="params">(object)</span>:</span></div><div class="line"></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, sizes)</span>:</span></div><div class="line">        self.num_layers = len(sizes)</div><div class="line">        self.sizes = sizes</div><div class="line">        self.biases = [np.random.randn(y, <span class="number">1</span>) <span class="keyword">for</span> y <span class="keyword">in</span> sizes[<span class="number">1</span>:]]</div><div class="line">        self.weights = [np.random.randn(y, x)</div><div class="line">                        <span class="keyword">for</span> x, y <span class="keyword">in</span> zip(sizes[:<span class="number">-1</span>], sizes[<span class="number">1</span>:])]</div><div class="line">net = Network([<span class="number">784</span>, <span class="number">30</span>, <span class="number">10</span>])</div></pre></td></tr></table></figure></p>
<p>　　sizes参数代表的是神经网络的层数以及每层的神经元个数。例如[784,30,10]代表输入层有784个神经元，隐藏层有30个神经元，输出层有10个神经元。后面的例子都以该神经网络形状来阐述。<br>　　biases是神经元的偏置，输入层神经元不存在偏置，故从隐藏层开始，使用np.random.randn生成\(N \sim (0,1)\)的高斯分布。np.random.randn的参数代表维度数，例如(y,1)代表y行,1列的数组。因此biases是一个list，按照层的顺序进行存放，每个元素代表一层神经元的偏置的列向量(np.ndarray类型)。<br>　　而weights代表的是权重list。第一个元素代表从输入层到隐藏层的权重矩阵，矩阵的行是隐藏层(后一层)的神经元个数，列是输入层(前一层)的神经元个数。以此类推，按顺序存放。</p>
<h2 id="前向传播算法"><a href="#前向传播算法" class="headerlink" title="前向传播算法"></a>前向传播算法</h2><p>　　对于特定的输入，返回对应的输出的方法如下：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">feedforward</span><span class="params">(self, a)</span>:</span></div><div class="line">        <span class="keyword">for</span> b, w <span class="keyword">in</span> zip(self.biases, self.weights):</div><div class="line">            a = sigmoid(np.dot(w, a)+b)</div><div class="line">        <span class="keyword">return</span> a</div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">sigmoid</span><span class="params">(z)</span>:</span></div><div class="line">        <span class="keyword">return</span> <span class="number">1.0</span>/(<span class="number">1.0</span>+np.exp(-z))</div><div class="line">net.feedforward([[<span class="number">5</span>],[<span class="number">6</span>]])<span class="comment">#假设两个输入神经元数据</span></div></pre></td></tr></table></figure></p>
<p>　　对于[784, 30, 10]结构的神经元，在第一次循环时，因为输入层到隐藏层的权重矩阵的规格是30*784，则w规格为(30L,784L)；输入神经元a的规格是(784L,1L)。因此w.a矩阵相乘结果即(30L,784L)*(784L,1L)，则为(30L,1L)，行数即隐藏层神经元的个数。<br>　　后面循环的过程中，直接将前一层的输出作为下一层的输入即可。</p>
<h2 id="随机梯度下降算法"><a href="#随机梯度下降算法" class="headerlink" title="随机梯度下降算法"></a>随机梯度下降算法</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">SGD</span><span class="params">(self, training_data, epochs, mini_batch_size, eta,</span></span></div><div class="line">        test_data=None):</div><div class="line">    <span class="keyword">if</span> test_data: n_test = len(test_data)</div><div class="line">    n = len(training_data)</div><div class="line">    <span class="keyword">for</span> j <span class="keyword">in</span> xrange(epochs):</div><div class="line">        random.shuffle(training_data)<span class="comment">#打乱数据</span></div><div class="line">        mini_batches = [</div><div class="line">            training_data[k:k+mini_batch_size]</div><div class="line">            <span class="keyword">for</span> k <span class="keyword">in</span> xrange(<span class="number">0</span>, n, mini_batch_size)]<span class="comment">#等大小划分数据</span></div><div class="line">        <span class="keyword">for</span> mini_batch <span class="keyword">in</span> mini_batches:</div><div class="line">            self.update_mini_batch(mini_batch, eta)<span class="comment">#随机梯度下降</span></div><div class="line">        <span class="keyword">if</span> test_data:</div><div class="line">            <span class="keyword">print</span> <span class="string">"Epoch &#123;0&#125;: &#123;1&#125; / &#123;2&#125;"</span>.format(</div><div class="line">                j, self.evaluate(test_data), n_test)</div><div class="line">        <span class="keyword">else</span>:</div><div class="line">            <span class="keyword">print</span> <span class="string">"Epoch &#123;0&#125; complete"</span>.format(j)</div></pre></td></tr></table></figure>
<p>　　training_data每条数据都是一个(x,y)元组的列表，表示训练输入和其对应的期望输出。变量epochs代表迭代数量，mini_batch_size代表采样时的小批量数据的大小。eta是学习速率\(\eta\)。如果给出了可选参数test_data，那么程序会在每个训练器后评估神经网络，并打印出部分进展，这对于追踪进度很有用。<br>　　代码如下工作。在每个迭代期，首先随机地将训练数据打乱，然后将它分成多个适当大小的小批量数据。然后对于每一个mini_batch应用一次梯度下降。<br>　　注意上述迭代是在上一次迭代结果的基础上继续迭代的。每次迭代唯一不同的是划分的数据mini_batch不相同。因此结果应该是不断优化的。当然应该会存在一个阈值。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">update_mini_batch</span><span class="params">(self, mini_batch, eta)</span>:</span></div><div class="line">    nabla_b = [np.zeros(b.shape) <span class="keyword">for</span> b <span class="keyword">in</span> self.biases]</div><div class="line">    nabla_w = [np.zeros(w.shape) <span class="keyword">for</span> w <span class="keyword">in</span> self.weights]</div><div class="line">    <span class="keyword">for</span> x, y <span class="keyword">in</span> mini_batch:</div><div class="line">        delta_nabla_b, delta_nabla_w = self.backprop(x, y)</div><div class="line">        nabla_b = [nb+dnb <span class="keyword">for</span> nb, dnb <span class="keyword">in</span> zip(nabla_b, delta_nabla_b)]</div><div class="line">        nabla_w = [nw+dnw <span class="keyword">for</span> nw, dnw <span class="keyword">in</span> zip(nabla_w, delta_nabla_w)]</div><div class="line">    self.weights = [w-(eta/len(mini_batch))*nw</div><div class="line">                    <span class="keyword">for</span> w, nw <span class="keyword">in</span> zip(self.weights, nabla_w)]<span class="comment">#更新权重矩阵</span></div><div class="line">    self.biases = [b-(eta/len(mini_batch))*nb</div><div class="line">                   <span class="keyword">for</span> b, nb <span class="keyword">in</span> zip(self.biases, nabla_b)]<span class="comment">#更新偏置矩阵</span></div></pre></td></tr></table></figure></p>
<p>　　上述代码是用来更新权重和偏置。首先根据权重和偏置矩阵的规格对\(\nabla b\)和\(\nabla w\)梯度进行初始化。然后对每个min_batch使用后向传播算法计算梯度。根据:<br>$$\frac{\sum_{j=1}^m \nabla C_{X_j}}{m} \approx \frac{\sum_x \nabla C_x}{n}=\nabla C$$<br>　　这里的第二个求和符号是在整个训练集数据上进行的。交换两边得到：<br>$$\nabla C \approx \frac{1}{m}\sum_{j=1}^m \nabla C_{X_j}$$<br>　　即小批量数据上\(\nabla C_{X_j}\)的平均值大致相等于整个\(\nabla C_{X}\)的平均值。<br>　　因此：<br>$$w_k := w_k - \frac{\eta}{m} \frac{\partial C_{X_j}}{\partial w_k}$$<br>$$b_l := b_l - \frac{\eta}{m} \frac{\partial C_{X_j}}{\partial b_l}$$<br>　　对应代码是上述最后两句。<br>　　实际上，偏置的更新不需要独立出来，只需要在每层增加一个神经元\(x_0=1\),这样就可以将偏置当作权重来对待了，使得代码更加简洁。<br>　　另外，测试集上性能评估函数如下：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">evaluate</span><span class="params">(self, test_data)</span>:</span></div><div class="line">    test_results = [(np.argmax(self.feedforward(x)), y)</div><div class="line">                    <span class="keyword">for</span> (x, y) <span class="keyword">in</span> test_data]<span class="comment">#输出层有10个输出，数值最大的那个神经元的下标即为数字分类结果</span></div><div class="line">    <span class="keyword">return</span> sum(int(x == y) <span class="keyword">for</span> (x, y) <span class="keyword">in</span> test_results)<span class="comment">#一共多少个相等</span></div></pre></td></tr></table></figure></p>
<p>　　输出层有10个输出，根据sigmoid函数的图像，数值越大，代表可能性越高。因此数值最大的那个神经元的下标即为数字分类结果。</p>
<h2 id="后向传播算法"><a href="#后向传播算法" class="headerlink" title="后向传播算法"></a>后向传播算法</h2><p>　　最后我们来重点研究下后向传播算法如何快速求得参数的梯度。<br>　　注意，<strong>本部分采用的代价函数是二次型的，下面所有的公式都是针对二次型代价函数而言</strong>。之后我还会讨论交叉熵代价函数。<br>　　首先回顾一下，BP算法的四个重要公式：<br><img src="/picture/machine-learning/network_writting3.png" alt="bp"><br>　　第一个公式BP1代表输出层的误差计算方法。BP2代表其他层的误差计算方法。BP3代表偏置的梯度求法。BP4代表权重的梯度求法。注意，\(\odot\)是Hadamard乘积，即相同位置上的数相乘。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div></pre></td><td class="code"><pre><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">backprop</span><span class="params">(self, x, y)</span>:</span></div><div class="line">        nabla_b = [np.zeros(b.shape) <span class="keyword">for</span> b <span class="keyword">in</span> self.biases]</div><div class="line">        nabla_w = [np.zeros(w.shape) <span class="keyword">for</span> w <span class="keyword">in</span> self.weights]</div><div class="line">        </div><div class="line">        <span class="comment"># feedforward</span></div><div class="line">        activation = x</div><div class="line">        activations = [x] <span class="comment"># list to store all the activations, layer by layer</span></div><div class="line">        zs = [] <span class="comment"># list to store all the z vectors, layer by layer</span></div><div class="line">        <span class="keyword">for</span> b, w <span class="keyword">in</span> zip(self.biases, self.weights):</div><div class="line">            z = np.dot(w, activation)+b</div><div class="line">            zs.append(z)<span class="comment">#未激活</span></div><div class="line">            activation = sigmoid(z)<span class="comment">#激活单元</span></div><div class="line">            activations.append(activation)</div><div class="line"></div><div class="line">        <span class="comment"># backward pass 先计算输出层</span></div><div class="line">        delta = self.cost_derivative(activations[<span class="number">-1</span>], y) * \</div><div class="line">            sigmoid_prime(zs[<span class="number">-1</span>])</div><div class="line">        nabla_b[<span class="number">-1</span>] = delta</div><div class="line">        nabla_w[<span class="number">-1</span>] = np.dot(delta, activations[<span class="number">-2</span>].transpose())</div><div class="line">        <span class="comment"># Note that the variable l in the loop below is used a little</span></div><div class="line">        <span class="comment"># differently to the notation in Chapter 2 of the book.  Here,</span></div><div class="line">        <span class="comment"># l = 1 means the last layer of neurons, l = 2 is the</span></div><div class="line">        <span class="comment"># second-last layer, and so on.  It's a renumbering of the</span></div><div class="line">        <span class="comment"># scheme in the book, used here to take advantage of the fact</span></div><div class="line">        <span class="comment"># that Python can use negative indices in lists.</span></div><div class="line">        <span class="keyword">for</span> l <span class="keyword">in</span> xrange(<span class="number">2</span>, self.num_layers):</div><div class="line">            z = zs[-l]</div><div class="line">            sp = sigmoid_prime(z)</div><div class="line">            delta = np.dot(self.weights[-l+<span class="number">1</span>].transpose(), delta) * sp</div><div class="line">            nabla_b[-l] = delta</div><div class="line">            nabla_w[-l] = np.dot(delta, activations[-l<span class="number">-1</span>].transpose())</div><div class="line">        <span class="keyword">return</span> (nabla_b, nabla_w)</div><div class="line"></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">cost_derivative</span><span class="params">(self, output_activations, y)</span>:</span></div><div class="line">    <span class="keyword">return</span> (output_activations-y)</div><div class="line"></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">sigmoid_prime</span><span class="params">(z)</span>:</span></div><div class="line">    <span class="keyword">return</span> sigmoid(z)*(<span class="number">1</span>-sigmoid(z))</div></pre></td></tr></table></figure></p>
<p>　　这里的参数x,y代表一条数据，因此该方法一次只对一条数据进行后向传播来计算梯度。首先进行初始化，根据权重矩阵和偏置矩阵的规格进行初始化。activation代表当前激活单元，activations保存所有的激活单元，按层存放，每层使用一个数组来存储。zs保存未激活前的单元，同理按层存放，每层使用一个数组来存储。因此这里的运算都是矩阵运算。<br>　　接着从输出层开始，反向进行误差的计算。cost_derivative方法根据代价函数来求得对输出激活单元的导数。这里使用的是二次代价函数:<br>$$C(w,b)=\frac{1}{2n} \sum_{x} ||y(x)-a||^2$$<br>　　这里的n代表样本量，a代表输入为x时输出的向量(激活单元)，y(x)是数据的真实标记向量。<br>　　如果样本量为1，则：<br>$$\frac{\partial C}{\partial a}=-(y-a)=a-y$$<br>　　sigmoid_prime是对sigmoid函数求导。<br>　　我们想求输出层偏置的梯度\(\frac{\partial C}{\partial b}\)。首先分析下输出层偏置影响了哪些东西。令输出层偏置为\(b\)，输出层最终输出为\(a\)(激活了)，输出层激活前为\(z\)(前一层输出和权重线性组合后，未经过sigmoid函数处理)。<br>　　因为\(z=wa’+b\),其中\(a’\)是前一层的输出，因此\(b\)首先影响\(z\)。紧接着因为\(a=sigmoid(z)\),因此\(z\)影响\(a\)。而根据代价函数,\(a\)最终再影响代价函数。因此，根据链式求导法则：<br>$$\frac{\partial C}{\partial b} = \frac{\partial C}{\partial a} * \frac{\partial a}{\partial z} * \frac{\partial z}{\partial b} \\\ <br>=(a-y)* [sigmoid(z)(1-sigmoid(z)] * 1 \\\ $$<br>　　上述式子可以逆着写，即从最后一个式子开始往前写。该结果和上述代码相符。<br>　　注意到，这里的乘法使用的是Hadamard乘积。向量的点乘np.dot是我们熟悉的矩阵运算。因此delta = self.cost_derivative(activations[-1], y)*sigmoid_prime(zs[-1])是第一种情况，得到的delta规格也是10*1。<br>　　紧接着我们想求是输出层权重的梯度\(\frac{\partial C}{\partial w}\)。同理分析，\(w\)首先影响\(z\),\(z=wa’+b\)。\(z\)再影响\(a\),\(a=sigmoid(z)\)。\(a\)最终影响代价函数，因此根据链式求导法则：<br>$$\frac{\partial C}{\partial w} = \frac{\partial C}{\partial a} * \frac{\partial a}{\partial z} * \frac{\partial z}{\partial w} \\\ <br>=(a-y)* [sigmoid(z)(1-sigmoid(z)] * a’ \\\ $$<br>　　其中\(a’\)是前一层的输出。可以发现只要定义前一层\(a_0=1\)，则\(w,b\)的求导可以统一起来，不需要分开求。<br>　　注意delta的规格10*1，也就是输出层10个神经元每个都有一个分量。activations[-2]的规格是30*1，也就是前一层的神经元每个输出都有一个分量。activations[-2]转置后规格为1*30,delta*activations[-2].transpose()的结果矩阵的规格就是10*30。也就是说对于输出层每一个神经元的误差，都需要分摊到连接该神经元的所有权重上。<br>　　接着看误差继续往前传播。<br>　　我们分析隐藏层的偏置\(b’\)的梯度。\(b’\)首先影响该层的输出\(z’\),\(z’=w’a’’+b’\)；\(z’\)影响\(a’=sigmoid(z’)\)；\(a’\)作为最后一层的输入影响z，\(z=wa’+b\)；\(z\)再影响\(a=sigmoid(z)\)，\(a\)最终影响代价函数\(C\)，根据链式法则：<br>$$\frac{\partial C}{\partial b’} = (\frac{\partial C}{\partial a} * \frac{\partial a}{\partial z})<br>* (\frac{\partial z}{\partial a’}) * (\frac{\partial a’}{\partial z’}) * (\frac{\partial z’}{\partial b’}) \\\\<br>=delta * w * sigmoid\_prime(z’) * 1$$<br>　　可以看出delta是之前后一层求得的，\(w\)是后一层的权重，\(z’\)是该层的输出(未激活)。<br>　　和代码中的公式相符。可以看出权重矩阵w规格是10*30, delta是10*1，sigmoid_prime(z’)是30*1, 则np.dot(self.weights[-l+1].transpose(), delta) * sp规格是30*1，也就是隐藏层30个神经元每个都有一个新的delta分量。<br>　　\(w’\)梯度的分析类似。</p>
<h1 id="运行结果"><a href="#运行结果" class="headerlink" title="运行结果"></a>运行结果</h1><p>　　具体调用代码如下图所示：<br><img src="/picture/machine-learning/network_writting1.png" alt="network"><br>　　这里使用的是3层神经网络，输入层有784个神经元，隐藏层30个神经元，输出层10个神经元，进行30次迭代，每个mini_batch有10个数据，学习率设置为3。<br>　　可以看出随着迭代次数的增加，性能基本上也都有稳步提升。<br><img src="/picture/machine-learning/network_writting2.png" alt="network"></p>
<h1 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h1><p><a href="http://neuralnetworksanddeeplearning.com/chap1.html" target="_blank" rel="external">神经网络和深度学习入门</a></p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;　　本文将介绍使用Python手写“前馈神经网络”代码的具体步骤和细节。前馈神经网络采用随机梯度下降算法进行学习，代价函数的梯度计算方法使用的是反向传播算法。具体代码可参考&lt;a href=&quot;http://neuralnetworksanddeeplearning.com/chap1.html&quot;&gt;神经网络和深度学习教程&lt;/a&gt;。&lt;br&gt;
    
    </summary>
    
      <category term="神经网络" scheme="xtf615.com/categories/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/"/>
    
    
      <category term="机器学习" scheme="xtf615.com/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="神经网络" scheme="xtf615.com/tags/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/"/>
    
      <category term="梯度下降" scheme="xtf615.com/tags/%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D/"/>
    
      <category term="反向传播" scheme="xtf615.com/tags/%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD/"/>
    
  </entry>
  
  <entry>
    <title>强化学习：马尔科夫决策过程</title>
    <link href="xtf615.com/2017/07/15/RL/"/>
    <id>xtf615.com/2017/07/15/RL/</id>
    <published>2017-07-15T01:18:10.000Z</published>
    <updated>2017-07-16T07:42:16.133Z</updated>
    
    <content type="html"><![CDATA[<p>　　强化学习（Reinforcement Learning, RL）又叫做增强学习，是近年来机器学习和智能控制领域的主要方法之一。本文将重点介绍强化学习的概念以及马尔科夫决策过程。<br><a id="more"></a></p>
<h1 id="引入"><a href="#引入" class="headerlink" title="引入"></a>引入</h1><h2 id="定义"><a href="#定义" class="headerlink" title="定义"></a>定义</h2><p>　　英文定义：Reinforcement learning is learning what to do —-how to map situations to actions —- so as to maximize a numerical reward signal.<br>　　也就是说强化学习关注的是智能体如何在环境中采取一系列行为，从而获得最大的累积回报。通过强化学习，一个智能体应该知道在<strong>什么状态下应该采取什么行为</strong>。RL是从环境状态到动作的映射的学习，我们把这个映射称为策略。</p>
<h2 id="举例"><a href="#举例" class="headerlink" title="举例"></a>举例</h2><p>　　那么强化学习具体解决哪些问题呢，我们来举几个例子：<br>　　flappy bird 是现在很流行的一款小游戏，现在我们假设有一只机器小鸟，让它自行进行游戏，但是我们却没有小鸟的动力学模型，也不打算了解它的动力学。要怎么做呢？ 这时就可以给它设计一个增强学习算法，然后让小鸟不断的进行游戏，如果小鸟撞到柱子了，那就获得-1的回报，否则获得0回报。通过这样的若干次训练，我们最终可以得到一只飞行技能高超的小鸟，它知道在什么情况下采取什么动作来躲避柱子。<br>　　假设我们要构建一个下国际象棋的机器，这种情况不能使用监督学习，首先，我们本身不是优秀的棋手，而请象棋老师来遍历每个状态下的最佳棋步则代价过于昂贵。其次，每个棋步好坏判断不是孤立的，要依赖于对手的选择和局势的变化。是一系列的棋步组成的策略决定了是否能赢得比赛。下棋过程的唯一的反馈是在最后赢得或是输掉棋局时才产生的。这种情况我们可以采用增强学习算法，通过不断的探索和试错学习，增强学习可以获得某种下棋的策略，并在每个状态下都选择最有可能获胜的棋步。目前这种算法已经在棋类游戏中得到了广泛应用。</p>
<h2 id="强化学习和监督学习区别"><a href="#强化学习和监督学习区别" class="headerlink" title="强化学习和监督学习区别"></a>强化学习和监督学习区别</h2><p>　　可以看到，增强学习和监督学习的区别较大，在之前的讨论中，我们总是给定一个样本x，然后给或者不给\(label y\)。之后对样本进行拟合、分类、聚类或者降维等操作。然而对于很多序列决策或者控制问题，很难有这么规则的样本。比如，四足机器人的控制问题，刚开始都不知道应该让其动那条腿，在移动过程中，也不知道怎么让机器人自动找到合适的前进方向。另外如要设计一个下象棋的AI，每走一步实际上也是一个决策过程，虽然对于简单的棋有启发式方法，但在局势复杂时，仍然要让机器向后面多考虑几步后才能决定走哪一步比较好，因此需要更好的决策方法。<br>　　因此这里面主要的区别包括：</p>
<ul>
<li>增强学习是试错学习(Trail-and-error)，由于没有直接的指导信息，智能体要以不断与环境进行交互，通过试错的方式来获得最佳策略。</li>
<li>延迟回报，增强学习的指导信息很少，而且往往是在事后（最后一个状态）才给出的，这就导致了一个问题，就是获得正回报或者负回报以后，如何将回报分配给前面的状态。</li>
</ul>
<h2 id="思路"><a href="#思路" class="headerlink" title="思路"></a>思路</h2><p>　　对于这种控制决策问题，有这么一种解决思路。我们设计一个回报函数（reward function），如果learning agent（如上面的四足机器人、象棋AI程序）在决定一步后，获得了较好的结果，那么我们给agent一些回报（比如回报函数结果为正），得到较差的结果，那么回报函数为负。比如，四足机器人，如果他向前走了一步（接近目标），那么回报函数为正，后退为负。如果我们能够对每一步进行评价，得到相应的回报函数，那么就好办了，我们只需要找到一条回报值最大的路径（每步的回报之和最大），就认为是最佳的路径。<br>　　增强学习是机器学习中一个非常活跃且有趣的领域，相比其他学习方法，增强学习更接近生物学习的本质，因此有望获得更高的智能。增强学习在很多领域已经获得成功应用，比如自动直升机，机器人控制，手机网络路由，市场决策，工业控制，高效网页索引等。特别是在棋类游戏中。Tesauro(1995)描述的TD-Gammon程序，使用增强学习成为了世界级的西洋双陆棋选手。这个程序经过150万个自生成的对弈训练后，已经近似达到了人类最佳选手的水平，并在和人类顶级高手的较量中取得40盘仅输1盘的好成绩。<br>　　接下来，先介绍一下马尔科夫决策过程（MDP，Markov decision processes）。</p>
<h1 id="马尔科夫模型类型"><a href="#马尔科夫模型类型" class="headerlink" title="马尔科夫模型类型"></a>马尔科夫模型类型</h1><p>　　大家应该还记得马尔科夫链(Markov Chain)，了解机器学习的也都知道隐马尔可夫模型(Hidden Markov Model，HMM)。它们具有的一个共同性质就是马尔可夫性(无后效性)，也就是指系统的下个状态只与当前状态信息有关，而与更早之前的状态无关。<br>　　马尔可夫决策过程(Markov Decision Process, MDP)也具有马尔可夫性，与上面不同的是MDP考虑了动作，即系统下个状态不仅和当前的状态有关，也和当前采取的动作有关。还是举下棋的例子，当我们在某个局面（状态s）走了一步(动作a)，这时对手的选择（导致下个状态s’）我们是不能确定的，但是他的选择只和s和a有关，而不用考虑更早之前的状态和动作，即s’是根据s和a随机生成的。<br>　　我们用一个二维表格表示一下，各种马尔可夫子模型的关系就很清楚了：<br><img src="/picture/machine-learning/rl1.png" alt="rl"></p>
<h1 id="马尔科夫决策过程"><a href="#马尔科夫决策过程" class="headerlink" title="马尔科夫决策过程"></a>马尔科夫决策过程</h1><h2 id="形式化定义"><a href="#形式化定义" class="headerlink" title="形式化定义"></a>形式化定义</h2><p>　　一个马尔科夫决策过程由一个五元组构成（\(S,A,\{P_{sa}\},\gamma,R\)）<br>　　\(S\)表示状态集(states)。（比如，在自动直升机系统中，直升机当前位置坐标组成状态集）<br>　　\(A\)表示一组动作(actions)。(比如，使用控制杆操纵的直升机飞机方向，让其向前，向后蹬)<br>　　\(P_{sa}\)是状态转移概率。S中的一个状态到另一个状态的转变，需要A来参与。\(P_{sa}\)表示的是在当前\(s \in S\)状态下，经过\(a \in A\)作用后，会转移到的其他状态的概率分布情况(当前状态执行a后可能跳转到很多状态)。比如在状态\(s\)下执行动作\(a\)，转移到\(s’\)的概率可以表示为\(P(s’|s,a)\)。<br>　　\(\gamma \in [0,1)\)是阻尼系数(discount factor)<br>　　\(R:S×A→\mathbb{R}\),R是回报函数(reward function)，如果一组\((s,a)\)转移到了下个状态\(s’\),那么回报函数可记为\(R(s’|s,a)\)。回报函数经常写作S的函数(只与S有关)，这样，R可重新写作\(R：S→\mathbb{R}\)。<br>　　MDP的动态过程如下：某个agent的初始状态为\(s_0\)，然后从A中挑选一个动作\(a_0\)，执行后，agent按\(P_{sa}\)概率随机转移到了下一个\(s_1\)状态，\(s_1 \in P_{s_0 a_0}\)。然后再执行一个动作\(a_1\),就转移到了\(s_2\)，接下来再执行\(a_2\)…,我们可以用下面的图表示整个过程：<br><img src="/picture/machine-learning/rl2.png" alt="rl"><br>　　如果回报r是根据状态s和动作a得到的，则MDP还可以表示成下图：<br><img src="/picture/machine-learning/rl3.jpg" alt="rl"><br>　　注意，上述挑选动作是有一定策略的，即从状态→动作的映射是需要根据一定的策略的，将会在下面阐述。而状态转移概率可能是模型的参数，需要从多种现成状态中进行学习得到。<br>　　我们定义经过上面转移路径后，得到的回报函数之和如下：<br>$$R(s_0,a_0)+\gamma R(s_1,a_1)+\gamma^2 R(s_2,a_2) + \ldots$$<br>　　如果R只和S有关，那么上式可以写作：<br>$$R(s_0)+\gamma R(s_1)+\gamma^2 R(s_2) + \ldots$$<br>　　我们的目标是选择一组最佳的Action，使得全部的回报加权和期望最大。<br>$$\max E[R(s_0)+\gamma R(s_1)+\gamma^2 R(s_2) + \ldots]$$<br>　　从上式可以发现，在t时刻的回报值被打了\(\gamma^t\)的折扣，是一个逐步衰减的过程，越靠后的状态对回报和影响越小。最大化期望值也就是要将大的\(R(s_i)\)尽量放到前面，小的尽量放到后面。</p>
<h2 id="策略"><a href="#策略" class="headerlink" title="策略"></a>策略</h2><p>　　已经处于某个状态s时，我们会以一定的策略\(\pi\)来选择下一个动作a执行，然后转换到另一个状态\(s’\)。我们将这个动作的选择国产称为策略(policy)，每一个policy起始就是一个状态到动作的映射函数\(\pi:S→A\)。给定\(\pi\)也就是给定了\(a=\pi(s)\),也就是说，知道了\(\pi\)就知道了每个状态下一步应该执行的动作。</p>
<h2 id="值函数"><a href="#值函数" class="headerlink" title="值函数"></a>值函数</h2><p>　　上面我们提到增强学习学到的是一个从环境状态到动作的映射（即行为策略），记为策略\(\pi: S→A\)。而增强学习往往又具有延迟回报的特点: 如果在第n步输掉了棋，那么只有状态\(s_n\)和动作\(a_n\)获得了立即回报\(R(s_n,a_n)=-1\)，前面的所有状态立即回报均为0。所以对于之前的任意状态s和动作a，立即回报函数\(R(s,a)\)无法说明策略的好坏。因而需要定义值函数(value function，又叫效用函数)来表明当前状态下策略\(pi\)的长期影响。值函数又称作折算累积回报(discounted cumulative reward)。<br>　　用\(V^{\pi}(s)\)表示在策略\(\pi\)下，状态s的值函数。\(R_i\)表示未来第i步的立即回报，常见的值函数有以下三种：<br>$$a) \ V^{\pi}(s)=E_{\pi}\left[\sum_{i=0}^h R_i| s_0=s \right]$$<br>$$b) \ V^{\pi}(s)= lim_{h→ \infty}E_{\pi}\left[\frac{1}{h}\sum_{i=0}^h R_i| s_0=s \right]$$<br>$$c) \ V^{\pi}(s)=E_{\pi}\left[\sum_{i=0}^{\infty} \gamma^i R_i | s_0=s \right]$$<br>　　其中，<br>a)是采用策略\(\pi\)的情况下未来有限h步的期望立即回报总和；<br>b)是采用策略\(\pi\)的情况下期望的平均回报；<br>c)是值函数最常见的形式，式中\(γ∈[0,1]\)称为折合因子，表明了未来的回报相对于当前回报的重要程度。特别的，\(γ=0\)时，相当于只考虑立即,不考虑长期回报，\(γ=1\)时，将长期回报和立即回报看得同等重要。接下来我们只讨论第三种形式。<br>　　现在将值函数的第三种形式展开，其中\(R_i\)表示未来第i步回报，\(s’\)表示下一步状态，则有：<br>$$V^{\pi}(s)=E_{\pi}[R_0+\gamma R_1 + \gamma^2 R_2 + \gamma^3 R_3 + \ldots|s_0=s,\pi] \\\\<br>=E_{\pi}[R_0+\gamma E[R_1+\gamma R_2 + \gamma^2 R_3 + \ldots]|s_0=s,\pi] \\\\<br>=E_{\pi}[R_0+\gamma V^{\pi}(s’)]$$<br>　　给定策略\(\pi\)和初始状态\(s_0\)，则动作\(a_0=\pi(s_0)\)，\(a_0\)在\(\pi\)给定下是唯一的。但下个时刻将以概率\(p(s’|s_0,a_0)\)转向下个状态\(s’\)，也就是说A→S可能有多种，根据Bellman等式，得到：<br>$$V^{\pi}(s)=R(s)+\gamma \sum_{s’ \in S} P_{s \pi(s)}(s’)V^{\pi}(s’)$$<br>　　\(s’\)表示下一个状态。前面R(s)称为立即回报(immediate reward)，就是R(当前状态)。第二项也可以写作\(E_{s’ \sim P_{s\pi(s)}(s)}[V^{\pi}(s’)]\),是下一状态值函数的期望值，下一状态s’符合\(P_{s\pi(s)}\)分布。<br>　　当状态个数有限时，可以通过上式求出每一个s的Ｖ。如果列出线性方程组的话，也就是|S|个方差，|S|个未知数，直接求解即可。<br>　　我们求V的目的是想寻找一个当前状态s下，最优的行动策略\(\pi\)，定义最优的\(V^{*}\)如下：<br>$$V^{*}(s)=\max_{\pi} V^{\pi} (s)$$<br>　　就是从可选的策略\(\pi\)中挑选一个最优策略(discounted rewards最大)。<br>　　上式的Bellman等式形式如下：<br>$$V^{*}(s)=R(s)+\max_{a \in A} \gamma \sum_{s’ \in S} P_{sa}(s’)V^{*}(s’)$$<br>　　第一项与\(\pi\)无关，所以不变。第二项是一个\(\pi\)就决定了每个状态s的下一步动作a，执行a后，s’按概率分布的回报概率和的期望。可根据下图进行理解。<br><img src="/picture/machine-learning/rl4.png" alt="rl"><br>　　定义了最优的\(V^{*}\),我们再定义最优的策略\(\pi^{*}:S→A\)如下：<br>$$\pi^{*}(s)=arg \max_{a \in A} \sum_{s’ \in S}P_{sa}(s’)V^{*}(s’)$$<br>　　选择最优的\(\pi^{*}\),也就确定了每个状态s的下一步最优动作a。<br>　　根据以上式子，我们可以知道：<br>$$V^{*}(s)=V^{\pi^{*}}(s) \geq V^{\pi}(s)$$<br>　　上式意思就当前状态的最优值函数\(V^{*}\)，是由采用最优执行策略\(\pi^{*}\)的情况下得出的，采用最优执行方案的回报显然要比采用其他的执行策略\(\pi\)要好。<br>　　这里需要注意的是，如果我们能够求得每个s下最优的a，那么从全局来看，\(S→A\),而生成的这个映射是最优映射，称为\(\pi^{*}\)。\(\pi^{*}\)针对全局的s，确定了每一个s的下一个动作a，不会因为初始状态s选取的不同而不同。</p>
<h2 id="值迭代和策略迭代法"><a href="#值迭代和策略迭代法" class="headerlink" title="值迭代和策略迭代法"></a>值迭代和策略迭代法</h2><p>　　上节我们给出了迭代公式和优化目标，这节讨论两种求解有限状态MDP具体策略的有效算法。这里，我们只针对MDP是有限状态、有限动作的情况。</p>
<h3 id="值迭代法"><a href="#值迭代法" class="headerlink" title="值迭代法"></a>值迭代法</h3><p>1）将每个s的V(s)初始化为0<br>2）循环直到收敛{<br>　　对于每一个状态s，对V(s)做更新<br>　　\(V(s):=R(s)+\max_{a \in A} \gamma \sum_{s’}P_{sa}(s’)V(s’)\)<br>}<br>　　内循环的实现由两种策略。<br>　　1. 同步迭代法<br>　　拿初始化后的第一次迭代来说吧，初始状态所有的V(s)都为0。然后对所有的s都计算新的\(V(s)=R(s)+0=R(s)\)。在计算每一个状态时，得到新的V(s)后，先存下来，不立即更新。待所有的s的新值V(s)都计算完毕后，再统一更新。<br>　　2. 异步迭代法<br>　　与同步迭代对应的就是异步迭代了，对每一个状态s，得到新的\(V(s)\)后，不存储，直接更新。这样，第一次迭代后，大部分\(V(s)&gt;R(s)\)。</p>
<p>　　不管使用这两种的哪一种，最终V(s)会收敛到\(V^{*}(s)\)。知道了\(V^{*}\)后，我们再使用公式（3）来求出相应的最优策略\(\pi^{*}\)，当然\(\pi^{*}\)可以在求\(V^{*}\)的过程中求出。</p>
<h3 id="策略迭代法"><a href="#策略迭代法" class="headerlink" title="策略迭代法"></a>策略迭代法</h3><p>　　值迭代法使V值收敛到\(V^{*}\)，而策略迭代法关注\(\pi\),使\(\pi\)收敛到\(\pi^{*}\)。<br>1） 将随机指定一个S到A的映射\(\pi\)。<br>2） 循环知道收敛{<br>　　a) 令\(V:=V^{\pi}\)<br>　　b) 对于每一个状态s，对\(\pi(s)\)做更新。<br>　　　　\(\pi(s):=arg \max_{a \in A} \sum_{s’} P_{sa}(s’)V(s’)\)<br>}<br>　　a)步中的V可以通过之前的Bellman等式求出：<br>$$V^{\pi}(s)=R(s)+\gamma \sum_{s’ \in S} P_{s\pi(s)}(s’)V^{\pi}(s’)$$<br>　　这一步会求得所有状态s的\(V^{\pi}(s)\)。<br>　　b)步实际上就是根据a步的结果挑选出当前状态s下，最优的a，然后对\(\pi(s)\)做更新。<br>　　对于值迭代和策略迭代很难说哪种方法好，哪种不好。对于规模比较小的MDP来说，策略一般能够更快地收敛。但是对于规模很大（状态很多）的MDP来说，值迭代比较容易（不用求线性方程组）。</p>
<h2 id="MDP中的参数估计"><a href="#MDP中的参数估计" class="headerlink" title="MDP中的参数估计"></a>MDP中的参数估计</h2><p>　　在之前讨论的MDP中，我们是已知状态转移概率\(P_{sa}\)和回报函数\(R(s)\)的。但在很多实际问题中，这些参数不能显式得到，我们需要从数据中估计出这些参数（通常S、A和\(\gamma\)是已知的）。<br>　　假设我们已知很多条状态转移路径如下：<br><img src="/picture/machine-learning/rl5.png" alt="rl"><br>　　其中，\(s_i^{(j)}\)是i时刻，第j条转移路径对应的状态，\(a_i^{(j)}\)是\(s_i^{(j)}\)状态时要执行的动作。每个转移路径中状态数是有限的，在实际操作过程中，每个转移链要么进入终结状态，要么达到规定的步数就会终结。<br>　　如果我们获得了很多上面类似的转移链（相当于有了样本），那么我们就可以使用最大似然估计来估计状态转移概率。<br>$$P_{sa}(s’)=\frac{times \ took \ we \ action \ a \ in \ state \ s \ and \ got \ to \ s’}{times \ we \ took \ action \ a \ in \ state \ s}$$<br>　　分子是从s状态执行动作a后到达s’的次数，分母是在状态s时，执行a的次数。两者相除就是在s状态下执行a后，会转移到s’的概率。<br>　　为了避免分母为0的情况，我们需要做平滑。如果分母为0，则令\(P_{sa}(s’)=\frac{1}{|S|}\)，也就是说当样本中没有出现过在s状态下执行a的样例时，我们认为转移概率均分。<br>　　上面这种估计方法是从历史数据中估计，这个公式同样适用于在线更新。比如我们新得到了一些转移路径，那么对上面的公式进行分子分母的修正（加上新得到的count）即可。修正过后，转移概率有所改变，按照改变后的概率，可能出现更多的新的转移路径，这样\(P_{sa}\)会越来越准。<br>　　同样，如果回报函数未知，那么我们认为R(s)为在s状态下已经观测到的回报均值。<br>　　当转移概率和回报函数估计出之后，我们可以使用值迭代或者策略迭代来解决MDP问题。比如，我们将参数估计和值迭代结合起来（在不知道状态转移概率情况下）的流程如下:<br>1、 随机初始化\(\pi\)<br>2、 循环直到收敛 {<br>　　(a) 在样本上统计\(\pi\)中每个状态转移次数，用来更新\(P_{sa}\)和R<br>　　(b) 使用估计到的参数来更新V（使用上节的值迭代方法）<br>　　(c) 根据更新的V来重新得出\(\pi\)<br>}<br>　　在(b)步中我们要做值更新，也是一个循环迭代的过程，在上节中，我们通过将V初始化为0，然后进行迭代来求解V。嵌套到上面的过程后，如果每次初始化V为0，然后迭代更新，就会很慢。一个加快速度的方法是每次将V初始化为上一次大循环中得到的V。也就是说V的初值衔接了上次的结果。<br>　　至此我们了解了马尔可夫决策过程在强化学习中的应用。</p>
<h1 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h1><p><a href="http://open.163.com/special/opencourse/machinelearning.html" target="_blank" rel="external">斯坦福大学机器学习视频教程</a><br><a href="http://www.cnblogs.com/jinxulin/p/3517377.html" target="_blank" rel="external">增强学习（二）—– 马尔可夫决策过程MDP</a><br><a href="http://blog.csdn.net/u012409883/article/details/17091665" target="_blank" rel="external">【机器学习-斯坦福】学习笔记21——增强学习</a></p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;　　强化学习（Reinforcement Learning, RL）又叫做增强学习，是近年来机器学习和智能控制领域的主要方法之一。本文将重点介绍强化学习的概念以及马尔科夫决策过程。&lt;br&gt;
    
    </summary>
    
      <category term="机器学习" scheme="xtf615.com/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="机器学习" scheme="xtf615.com/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="强化学习" scheme="xtf615.com/tags/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>独立成分分析</title>
    <link href="xtf615.com/2017/07/12/ICA/"/>
    <id>xtf615.com/2017/07/12/ICA/</id>
    <published>2017-07-12T07:26:23.000Z</published>
    <updated>2017-07-13T08:46:11.812Z</updated>
    
    <content type="html"><![CDATA[<p>　　前文提到的PCA是一个信息提取的过程，将原始数据进行降维。而本文提到的独立成分分析ICA（Independent Components Analysis）是一个信息解混过程，ICA认为观测信号是若干个统计独立的分量的线性组合。即假设观察到的随机信号x服从模型\(x=As\),其中s为未知源信号，其分量(代表不同信号源)相互独立，A为一未知混合矩阵(As实现不同信号源的线性组合)。ICA的目的是通过且仅通过观察x来估计混合矩阵A以及源信号s。<br><a id="more"></a></p>
<h1 id="引入"><a href="#引入" class="headerlink" title="引入"></a>引入</h1><p>　　让我们从经典的鸡尾酒宴会问题(cocktail party problem)谈起。假设在宴会中有n个人，他们可以同时说话，我们也在房间中的一些角落共放置了n个声音接收器(microphone)用来记录声音。宴会过后，我们从n个麦克风中得到了一组数据\(\{x^{(i)}\left(x_1^{(i)},x_2^{(i)},…,x_n^{(i)}\right),i=1,2,…m\}\),上标i代表采样的时间顺序，每个时刻n个声音组合得到1组样本，并且在每个时刻，每个麦克风都会得到一种n个声音的线性组合，也就是说每组样本包含了n种线性组合，m个时刻共得到了m组采样，并且每一组采样都是n维的。我们的目标是单单从这m组采样数据中分辨出每个人说话的信号。<br>　　将问题细化一下，有n个信号源\(s(s_1,s_2,…,s_n)^T,s \in \mathbb{R}^n\),每一维都是一个人的声音信号，每个人发出的声音信号独立。s是一个矩阵，假设m组样本，则s规格为\(n*m\),每一行代表一个人m个时刻的声音序列，总共有n行，即n个人的声音序列。A是一个未知的混合矩阵(mixing matrix)，用来组合叠加信号s，矩阵计算相当于s进行了线性组合，线性组合的系数由混合矩阵A来决定的。则：<br>$$x=As$$<br>　　x的意义在上文解释过，这里的x不是一个向量，是一个矩阵，其中每个列向量是\(x^{(i)},x^{(i)}=As^{(i)}\),\(x^{(i)}\)列向量是n维的，即n个接收器在i时刻接收到的序列，\(x^{(i)}\)每个分量代表i时刻不同接收器得到的所有n个声音的线性组合。例如第一个分量代表第一个接收器在第i时刻接收到的所有声音的线性组合，第n个分量代表第n个接收器在i时刻接收到的所有声音的线性组合。<br>　　表示成图如下：<br><img src="/picture/machine-learning/ica1.jpg" alt="ica"><br><img src="/picture/machine-learning/ica2.png" alt="ica"><br>　　\(x^{(i)}\)的每个分量都由\(s^{(i)}\)的分量线性表示。A和s都是未知的，x是已知的，我们要想办法根据x来推出s。这个过程也称作盲信号分离。<br>　　令\(W=A^{-1}\)，那么\(s^{(i)}=A^{-1}x^{(i)}=Wx^{(i)}\),则可将W表示成：<br>$$W=\begin{bmatrix}——w_1^T—— \\\ … \\\ ——w_n^T—— \end{bmatrix}$$<br>　　其中，\(w_i \in \mathbb{R}^n\),显然W是\(n*n\)规格的。得到：<br>$$s_j^{(i)}=w_j^{T}x^{(i)}$$</p>
<h2 id="ICA的不确定性"><a href="#ICA的不确定性" class="headerlink" title="ICA的不确定性"></a>ICA的不确定性</h2><p>　　由于w和s都不确定，那么在没有先验知识的情况下，无法同时确定这两个相关参数。比如上面的公式s=wx。当w扩大两倍时，s只需要同时扩大两倍即可，等式仍然满足，因此无法得到唯一的s。同时如果将人的编号打乱，变成另外一个顺序，如上图的蓝色节点的编号变为3,2,1，那么只需要调换A的列向量顺序即可，因此也无法单独确定s。这两种情况称为原信号不确定。<br>　　还有一种ICA不适用的情况，那就是信号不能是高斯分布的。假设只有两个人发出的声音信号符合多元正态分布\(s \sim N(0,I)\)，I是\(2*2\)的单位矩阵，s的概率密度函数以均值0为中心，投影面是椭圆的山峰状，因为\(x=As\),因此x也是高斯分布的，均值为0，协方差为\(E[xx^T]=E[Ass^TA^T]=AA^T\)。<br>　　令R是正交阵(\(RR^T=R^TR=I\)),令\(A’=AR\)，如果将\(A\)替换成\(A’\)。那么\(x’=A’s\)，s的分布仍然是多元高斯分布，则\(x’\)的均值仍然为0，协方差为：<br>$$E[x’(x’)^T]=E[A’ss^T(A’)^T]=E[ARss^T(AR)^T]=ARR^TA^T=AA^T$$<br>　　因此，不管混合矩阵是\(A\)还是\(A’\),x的分布情况是一样的，此时混合矩阵不是唯一的，因此无法确定原信号。</p>
<h1 id="密度函数与线性变换"><a href="#密度函数与线性变换" class="headerlink" title="密度函数与线性变换"></a>密度函数与线性变换</h1><p>　　在讨论ICA算法之前，我们先来回顾一下概率和线性代数里的知识。<br>　　假设我们的随机变量s有概率密度函数\(p_s(s)\)(连续值是概率密度函数，离散值是概率)。为了简单，我们再假设s是实数，有一个随机变量\(x=As\),A和x都是实数。令\(p_x\)是x的概率密度，那么怎么求\(p_x\)呢？<br>　　令\(W=A^{-1}\),首先将式子变幻成\(s=Wx\),然后得到\(P_x(x)=p_s(Ws)\),求解完毕。可惜这种方法是错误的。比如s符合均匀分布的话，即\(s \sim Uniform[0,1]\),那么s的概率密度\(P_s(s)=1\{0 \leq s \leq 1\}\),现在令A=2，即\(x=2s\),也就是说x在[0,2]上均匀分布，则\(p_x(x)=0.5\),因此，按照前面的推导会得到\(p_x(x)=p_s(0.5s)=1\),显然是不对的。正确的公式是：\(p_x=p_s(Wx)|w|\)<br>　　推导方法如下：<br>$$F_X(a)=P(X \leq a)=P(AS \leq a)= p(s \leq Wa)=F_s(Wa) \\\\<br>p_x(a)=F_X’(a)=F_S’(Wa)=p_s(Wa)|W|$$<br>　　更一般地，如果s是向量，A是可逆的方阵，那么上式仍成立。</p>
<h1 id="ICA算法"><a href="#ICA算法" class="headerlink" title="ICA算法"></a>ICA算法</h1><p>　　ICA算法归功于Bell和Sejnowski，这里使用最大似然估计来解释算法，原始的论文中使用的是一个复杂的方法Infomax principal。<br>　　我们假定每个\(s_i\)有概率密度\(p_s\),那么给定时刻原信号的联合分布是：<br>$$p(s)=\prod_{i=1}^n p_s(s_i)$$<br>　　这个公式有一个假设前提：每个人发出的声音信号各自独立。有了p(s),我们可以求得p(x):<br>$$p(x) = p_s(Wx)|W|=|W|\prod_{i=1}^n p_s(w_i^T x_i)$$<br>　　左边是每个采样信号x(n维向量)的概率，右边是每个原信号概率乘积的|W|倍。<br>　　前面提到过，如果没有先验知识，我们无法求得W和s。因此我们需要知道\(p_s(s_i)\)，我们打算选取一个概率密度函数赋给s，但是我们不能选取高斯分布的密度函数。在概率论里面，我们知道密度函数p(x)由累积分布函数（CDF）F(x)求导得到。F(x)要满足两个性质是，单调递增和取值范围在[0,1]。我们发现sigmoid函数很适合，定义域为负无穷到正无穷，值域为0到1，缓慢递增。我们假定s的累积分布函数符合sigmoid函数：<br>$$g(s)=\frac{1}{1+e^{-s}}$$<br>　　求导后：<br>$$P_s(s)=g’(s)=\frac{e^s}{(1+e^s)^2}$$<br>　　这就是s的密度函数，这里s是实数。<br>　　如果我们预先知道s的分布函数，那就不用假设了。但是在缺失的情况下，sigmoid函数能够在大多数问题上取得不错的效果。由于上式中P_s(s)是个对称函数，因此E[s]=0,那么\(E[x]=E[As]=0\),x的均值也为0.<br>　　知道了\(p_s(s)\),就剩下W了。给定采样后的训练样本\(\{x^{(i)}(x_1^{(i)},x_2^{(i)},…,x_n^{(i)},i=1,2…,m\}\)，样本对数似然估计如下，使用前面得到的x的概率密度函数：<br>$$\ell(W)=\sum_{i=1}^m \left(\sum_{j=1}^n log \ g’(w_j^T x^{(i)}) + log |W| \right)$$<br>　　接下来就是对W球到了，这里牵涉一个问题就是对行列式|W|进行求导的方法，属于矩阵微积分。这里先给出结果：<br>$$\nabla_w|W|=|W|(W^{(-1)})^T$$<br>　　最终得到的求导公式如下，\(log g’(s)\)的导数是\(1-2g(s)\)（可以自己验证）：<br>$$W:=W+\alpha \left(\begin{bmatrix}1-2g(w_1^T x^{(i)}) \\\ 1-2g(w_2^T x^{(i)}) \\\ … \\\ 1-2g(w_n^T x^{(i)})\end{bmatrix} {x^{(i)}}^T+(W^T)^{-1} \right)$$<br>　　其中\(\alpha\)是梯度上升速率，人为指定。<br>　　当迭代求出W后，便可得到\(s^{(i)}=Wx^{(i)}\)来还原出原始信号。<br>　　注意：我们计算最大似然估计时，假设了\(x^{(i)}和x^{(j)}\)之间是独立的，然而对于语音信号或者其他具有时间连续依赖特性（如温度），这个假设不成立。但是在数据足够多时，假设独立对效果影响不大，同时如果事先打乱样例，并运行随机梯度上升算法，那么就能够加快收敛速度。<br>　　回顾鸡尾酒宴会问题，s是人发出的信号，是连续值，不同时间点的s不同，每个人发出的信号之间独立(\(s_i和s_j\)之间独立)。s的累积概率分布韩式sigmoid函数，但是所有人发出声音信号都符合这个分布。A(W的逆矩阵)代表了s相对于x的位置变化，x是s和A变化后的结果。</p>
<h1 id="实例"><a href="#实例" class="headerlink" title="实例"></a>实例</h1><p>　　当n=3时，原始信号正弦、余弦、随机信号。如下图所示，也就相当于S矩阵：<br><img src="/picture/machine-learning/ica3.jpg" alt="ica"><br>　　经过随机混合，由6个麦克风录制下来，观察到的x信号如下，相当于X矩阵：<br><img src="/picture/machine-learning/ica4.jpg" alt="ica"><br>　　在使用ICA算法之前，需要对数据进行预处理，可使用PCA和白化。PCA、白化处理后，可以看到6路信号减少为3路，ICA仅需要这3路混合信号即可还原源信号。<br><img src="/picture/machine-learning/ica5.jpg" alt="ica"><br>　　使用ICA算法,进行多步迭代优化，就会按照信号之间独立最大的假设，将信号解混输出。得到原始S信号如下图所示：<br><img src="/picture/machine-learning/ica6.jpg" alt="ica"></p>
<h1 id="PCA和ICA的联系和区别"><a href="#PCA和ICA的联系和区别" class="headerlink" title="PCA和ICA的联系和区别"></a>PCA和ICA的联系和区别</h1><p>　　不管是PCA还是ICA，都不需要你对源信号的分布做具体的假设；如果观察到的信号为高斯，那么源信号也为高斯，此时PCA和ICA等价。下面稍作展开。<br>　　假设你观察到的信号是n维随机变量\(x=(x_1,\ldots,x_n)^T\).主成分分析（PCA）和独立成分分析（ICA）的目的都是找到一个方向，即一个n维向量\(w=(w_1,\ldots,w_n)^T\)使得线性组合\(\sum_{i=1}^nw_ix_i=w^Tx\)的某种特征最大化。</p>
<h2 id="PCA"><a href="#PCA" class="headerlink" title="PCA　　"></a>PCA　　</h2><p>　　PCA认为一个随机信号最有用的信息体包含在方差里。为此我们需要找到一个方向\(\mathbf{w}_1\)，使得随机信号x在该方向上的投影\(w_1^Tx\)的方差最大化。接下来，我们在与\(w_1\)正交的空间里到方向\(w_2\)，使得\(w_2^Tx\)的方差最大，以此类推直到找到所有的n个方向\(w_1,\ldots,w_n\). 用这种方法我们最终可以得到一列不相关的随机变量：\(w_1^Tx,\ldots,w_n^Tx\)。<br>　　如果用矩阵的形式，记\(W=(w_1,\ldots, w_n)\),那么本质上PCA是把原随机信号x变换成了\(y=Wx\),其中y满足,y的各分量不相关以及\(y_1,\ldots,y_n\)的方差递减。<br>　　特别地，当原随机信号x为高斯随机向量的时候，得到的y仍为高斯随机向量，此时它的各个分量不仅仅是线性无关的，它们还是独立的。<br>　　通过PCA，我们可以得到一列不相关的随机变量。至于这些随机变量是不是真的有意义，那必须根据具体情况具体分析。最常见的例子是，如果x的各分量的单位（量纲）不同，那么一般不能直接套用PCA。比如，若x的几个分量分别代表某国GDP, 人口，失业率，政府清廉指数，这些分量的单位全都不同，而且可以自行随意选取：GDP的单位可以是美元或者日元；人口单位可以是人或者千人或者百万人；失业率可以是百分比或者千分比，等等。对同一个对象（如GDP）选用不同的单位将会改变其数值，从而改变PCA的结果；而依赖“单位选择”的结果显然是没有意义的。</p>
<h2 id="ICA"><a href="#ICA" class="headerlink" title="ICA"></a>ICA</h2><p>　　ICA又称盲源分离(Blind source separation, BSS)，它假设观察到的随机信号x服从模型，其中s为未知源信号，其分量相互独立，A为一未知混合矩阵。ICA的目的是通过且仅通过观察x来估计混合矩阵A以及源信号s。大多数ICA的算法需要进行“数据预处理”（data preprocessing）：先用PCA得到y，再把y的各个分量标准化（即让各分量除以自身的标准差）得到z。预处理后得到的z满足下面性质：z的各个分量不相关；z的各个分量的方差都为1。<br>　　有许多不同的ICA算法可以通过z把A和s估计出来。以著名的FastICA算法为例，该算法寻找方向使得随机变量\(w^Tz\)的某种“非高斯性”(non-Gaussianity)的度量最大化。一种常用的非高斯性的度量是四阶矩\(\mathbb{E}[(w^Tx)^4]\)。类似PCA的流程，我们首先找\(w_1\)使得\(\mathbb{E}[(w_1^Tx)^4]\)最大；然后在与\(w_1\)正交的空间里找\(w_2\)，使得\(\mathbb{E}[(w_2^Tx)^4]\)最大，以此类推直到找到所有的\(w_1,w_2…,w_n\). 可以证明，用这种方法得到的\(w_1^T z,…,w_n^T z\)是相互独立的。<br>　　ICA认为一个信号可以被分解成若干个统计独立的分量的线性组合，而后者携带更多的信息。我们可以证明，只要源信号非高斯，那么这种分解是唯一的。若源信号为高斯的话，那么显然可能有无穷多这样的分解。</p>
<h1 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h1><p><a href="http://open.163.com/special/opencourse/machinelearning.html" target="_blank" rel="external">斯坦福大学机器学习视频教程</a><br><a href="https://www.zhihu.com/question/28845451/answer/42537342" target="_blank" rel="external">知乎：独立成分分析 ( ICA ) 与主成分分析 ( PCA ) 的区别在哪里？</a></p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;　　前文提到的PCA是一个信息提取的过程，将原始数据进行降维。而本文提到的独立成分分析ICA（Independent Components Analysis）是一个信息解混过程，ICA认为观测信号是若干个统计独立的分量的线性组合。即假设观察到的随机信号x服从模型\(x=As\),其中s为未知源信号，其分量(代表不同信号源)相互独立，A为一未知混合矩阵(As实现不同信号源的线性组合)。ICA的目的是通过且仅通过观察x来估计混合矩阵A以及源信号s。&lt;br&gt;
    
    </summary>
    
      <category term="机器学习" scheme="xtf615.com/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="机器学习" scheme="xtf615.com/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="独立成分分析" scheme="xtf615.com/tags/%E7%8B%AC%E7%AB%8B%E6%88%90%E5%88%86%E5%88%86%E6%9E%90/"/>
    
  </entry>
  
  <entry>
    <title>因子分析</title>
    <link href="xtf615.com/2017/07/10/Factor-Analysis/"/>
    <id>xtf615.com/2017/07/10/Factor-Analysis/</id>
    <published>2017-07-10T08:41:30.000Z</published>
    <updated>2017-07-11T09:13:28.649Z</updated>
    
    <content type="html"><![CDATA[<p>　　本文主要介绍因子分析模型(Factor Analysis Model)。因子分析模型是对高斯混合模型存在的问题进行解决的一种途径。同时也是属于“空间映射”思想的一种算法。本文将对因子分析模型进行介绍，并使用EM算法进行求解。<br><a id="more"></a></p>
<h1 id="引入"><a href="#引入" class="headerlink" title="引入"></a>引入</h1><p>　　在上一篇笔记中<a href="/2017/04/07/聚类算法/">混合高斯模型</a>，对于混合高斯模型来说，当训练数据样本数据小于样本的维度时，因为协方差矩阵是奇异的，导致不能得到高斯概率密度函数的问题。（\(\Sigma\)出现在分母）<br>　　追本溯源，这个问题可以认为数据信息缺乏的问题，即从训练数据中得不到模型所需要的全部信息。解决方法就是减少模型所需要的信息。本文提到的手段有两个，第一个就是不改变现有模型，但是加强模型的假设，例如可以对协方差矩阵进行限制，使协方差矩阵为对角矩阵，或者进一步假设对角矩阵上的对角线数值都相等，此时只要样本大于1就可以估计出限定的协方差矩阵。第二个手段则是降低模型的复杂度，提出一个需要更少参数的模型，因子分析模型就属于此类方法。本文重点讨论该模型。</p>
<h1 id="边缘和条件高斯分布"><a href="#边缘和条件高斯分布" class="headerlink" title="边缘和条件高斯分布"></a>边缘和条件高斯分布</h1><p>　　在讨论因子分析之前，先看看多元高斯分布中，条件和边缘高斯分布的求法，这个在后面因子分析的EM推导中有用。<br>　　假设x是有两个随机向量组成（可以看作将之前的\(x^{(i)}\)分成了两部分）<br>$$x=\begin{bmatrix}x_1 \\\ x_2 \end{bmatrix}$$<br>　　其中，\(x_1 \in \mathbb{R}^r, x_2 \in \mathbb{R}^s, 则x \in \mathbb{R}^{r+s}\)。假设x服从多元高斯分布\(x \sim N(\mu,\Sigma)\),其中：<br>$$\mu = \begin{bmatrix}\mu_1 \\\ \mu_2 \end{bmatrix}$$<br>$$\Sigma=\begin{bmatrix}\Sigma_{11} \ \Sigma_{12} \\\ \Sigma_{21} \ \Sigma_{22} \end{bmatrix}$$<br>　　其中，\(\mu_1 \in \mathbb{R}^r,\mu_2 \in \mathbb{R}^s,则\Sigma_{11} \in \mathbb{R}^{r*r},\Sigma_{12} \in \mathbb{R}^{r*s}\),由于协方差矩阵是对称的，故\(\Sigma_{12}=\Sigma_{21}^T \)。整体上看，\(x_1,x_2\)联合分布符合多元高斯分布。<br>　　那么只知道联合分布的情况下，如何求\(x_1\)的边缘分布呢？从上面\(\mu,\Sigma\)可以得出：<br>$$E[x_1]=\mu_1, \ Cov(x_1)=E[(x_1-\mu_1)(x_1-\mu_1)^T]=\Sigma_{11}$$<br>　　下面我们验证第二个结果：<br>$$Cov(x)=\Sigma \\\\<br>=\begin{bmatrix}\Sigma_{11} \ \Sigma_{12} \\\ \Sigma_{21} \ \Sigma_{22} \end{bmatrix} \\\\<br>=E[(x-\mu)(x-\mu)^T] \\\\<br>=E\left[\begin{bmatrix}x_1-\mu_1 \\\ x_2-\mu_2 \end{bmatrix} {\begin{bmatrix}x_1-\mu_1 \\\ x_2-\mu_2 \end{bmatrix}}^T \right] \\\\<br>=E \begin{bmatrix} (x_1-\mu_1)(x_1-\mu_1)^T \ (x_1-\mu_1)(x_2-\mu_2)^T \\\ (x_2-\mu_2)(x_1-\mu_1)^T \ (x_2-\mu_2)(x_2-\mu_2)^T \end{bmatrix}<br>$$<br>　　由此可见，多元高斯分布的边缘分布仍然是多元高斯分布。也就是说:<br>$$x_1 \sim N(\mu_1, \Sigma_{11})$$<br>　　上面求得是边缘分布，让我们考虑一下条件分布的问题，也就是\(x_1|x_2\)。根据多元高斯分布的定义：<br>$$x_1|x_2 \sim N(\mu_{1|2},\Sigma_{1|2})$$<br>　　且：<br>$$\mu_{1|2}=\mu_1 + \Sigma_{12} \Sigma_{22}^{-1}(x_2-\mu_2)$$<br>$$\Sigma_{1|2}=\Sigma_{11}-\Sigma_{12}\Sigma_{22}^{-1} \Sigma_{21}$$<br>　　这是接下来计算时需要的公式，这两个公式直接给出。</p>
<h1 id="因子分析模型"><a href="#因子分析模型" class="headerlink" title="因子分析模型"></a>因子分析模型</h1><h2 id="形式化定义"><a href="#形式化定义" class="headerlink" title="形式化定义"></a>形式化定义</h2><p>　　在因子分析模型中，我们假设有如下关于(x,z)的联合分布，其中z是隐含随机变量，且\( z \in \mathbb{R}^k\)<br>$$z \sim N(0,I)$$<br>$$x|z \sim N(\mu+\Lambda z,\Psi)$$<br>　　其中，模型的参数是向量\(\mu \in \mathbb{R}^n\),矩阵\(\Lambda \in \mathbb{R}^{n*k}\)以及对角矩阵\(\Psi \in \mathbb{R}^{n*n}\)。\(k\)的值通常取小于\(n\)。<br>　　因子分析模型<strong>数据产生过程</strong>的假设如下：</p>
<ul>
<li>1) 首先，在一个低维空间内用均值为0，协方差为单位矩阵的多元高斯分布生成m个隐含变量\(z^{(i)}\),\(z^{(i)}\)是k维向量，m是样本数目。</li>
<li>2) 然后使用变换矩阵\(\Lambda\)将z映射到n维空间\(\Lambda z\)。此时因为z的均值为0，映射后的均值仍然为0。</li>
<li>3) 再然后将n维向量\(\Lambda z\)再加上一个均值\(\mu\),对应的意义就是将变换后的z的均值在n维空间上平移。</li>
<li>4）由于真实样例x会有误差，在上述变换的基础上再加上误差\(\epsilon \in N(0,\Psi)\)</li>
<li>5) 最后的结果是认为训练样例生成公式为\(x=\mu+\Lambda z + \epsilon\)</li>
</ul>
<p>　　因此，我们也可以等价地定义因子分析模型如下：<br>$$z \sim N(0,I) \\\ \epsilon \sim N(0,\Psi) \\\ x=\mu+\Lambda z + \epsilon$$<br>　　其中，\(\epsilon和z\)是独立的。</p>
<h2 id="示例"><a href="#示例" class="headerlink" title="示例"></a>示例</h2><p>　　让我们看一个样本生成的例子方便理解因子分析模型。假设：\(z \in \mathbb{R}^1, x \in \mathbb{R}^2\)。z是一维向量，x为二维向量，再假设\(\Lambda=[1 \ 2]^T, \Psi=\begin{bmatrix} 1 \ 0 \\\ 0 \ 2 \end{bmatrix} \mu=[3 \ 1]^T\)<br>　　假设我们有m=5个二维样本点\(x^{(i)}\),两个特征如下：<br><img src="/picture/machine-learning/factor-analysis1.png" alt="factor1"><br>　　按照生成过程的5步。<br>　　１.第一步，我们首先认为在一维空间(这里k=1),存在着按高斯分布\(N(0,I)\)生成m个隐含变量\(z^{(i)}\)。如下：<br><img src="/picture/machine-learning/factor-analysis2.png" alt="factor2"><br>　　2. 然后使用某个\(\Lambda\)将一维的z映射到二维，图下：<br><img src="/picture/machine-learning/factor-analysis3.png" alt="factor3"><br>　　3. 之后加上\(\mu(\mu_1,\mu_2)^T\)，即将所有点的横坐标移动\(\mu_1\),纵坐标移动\(\mu_2\)，将直线移到一个位置，使得直线过点\(\mu\),原始左边轴的原点现在为\(\mu\)(红色点)<br><img src="/picture/machine-learning/factor-analysis4.png" alt="factor4"><br>　　4. 然而，样本点不可能这么规则，在模型上会有一定偏差，因此我们需要将上步生成的店做一些扰动，扰动\(\epsilon \sim N(0,\Psi)\).加入扰动后，得到黑色样本\(x^{(i)}\),如下：<br><img src="/picture/machine-learning/factor-analysis5.png" alt="factor5"><br>　　５.得到最终的训练样本，其中\(z,\epsilon\)均值均为0，因此\(\mu\)是原始样本点的均值。<br><img src="/picture/machine-learning/factor-analysis1.png" alt="factor1"><br>　　为了方便大家理解，在此举一个实际中使用因子分析模型的例子。<br>　　在企业形象或品牌形象的研究中，消费者可以通过一个有24个指标构成的评价体系，评价百货商场24个方面的优劣。但消费者主要关心的是三个方面，即商店的环境、商店的服务和商品的价格。因子分析方法可以通过24个变量，找出反映商店环境、商店服务水平和商店价格的三个潜在因子，对商店进行综合评价。<br>　　<strong>由以上的直观分析，我们知道了因子分析其实就是认为高维样本点实际上是由低维样本点经过高斯分布、线性变换、误差扰动生成的，因此高维数据可以使用低维来表示。</strong></p>
<h1 id="因子分析模型的推导"><a href="#因子分析模型的推导" class="headerlink" title="因子分析模型的推导"></a>因子分析模型的推导</h1><h2 id="似然函数推导"><a href="#似然函数推导" class="headerlink" title="似然函数推导"></a>似然函数推导</h2><p>　　上一节对因子分析模型进行了定义，以及从数据生成的角度对它进行了进一步阐述。本节则介绍上一节中定义的参数在模型中是如何被使用的。具体来讲，就是<strong>该模型对训练数据的似然函数是什么</strong>。<br>　　首先，重新列出模型的定义公式：<br>$$z \sim N(0,I) \\\ \epsilon \sim N(0,\Psi) \\\ x=\mu+\Lambda z + \epsilon$$<br>　　其中，误差\(\epsilon\)和隐含变量\(z\)是相互独立的。<br>　　使用高斯分布的矩阵表示法对模型进行分析。该方法认为z和x符合多元高斯分布，即:<br>$$\begin{bmatrix}z \\\ x \end{bmatrix} \sim N(\mu_{zx},\Sigma)$$<br>　　接下来就是求解\(\mu_{zx},\Sigma\)。<br>　　已知\(E[z]=0,E[\epsilon]=0\),则：<br>$$E[x]=E[\mu+\Lambda z + \epsilon]=\mu$$<br>　　故：<br>$$\mu_{zx}=\begin{bmatrix} \vec{0} \\\ \mu\end{bmatrix}$$<br>　　为了求解\(\Sigma\)，需要计算:<br>$$\Sigma_{zz}=E[(z-E[z])(z-E[z])^T] \\\ \Sigma_{zx}=\Sigma_{xz}^T=E[(z-E[z])(x-E[x])^T] \\\ \Sigma_{xx}=E[(x-E[x])(x-E[x])^T]$$<br>　　根据定义，可知\(\Sigma_{zz}=Cov(z)=I\),另外：<br>$$\Sigma_{zx}=E[(z-E[z])(x-E[x])^T] \\\ =E[z(\mu+\Lambda z + \epsilon - \mu)^T] \\\ =E[zz^T]\Lambda^T+E[z \epsilon^T]=\Lambda^T$$<br>　　上述公式最后一步,\(E[zz^T]=Cov(z)=I\)。并且，\(z,\epsilon\)相互独立，有\(E[z\epsilon^T]=E[z]E[\epsilon^T]=0\)<br>$$\Sigma_{xx}=E[(x-E[x])(x-E[x])^T]=E[(\Lambda z+\epsilon)(\Lambda z + \epsilon)^T]  \\\ =E[\Lambda z z^T \Lambda^T + \epsilon z^T \Lambda^T + \Lambda z \epsilon^T + \epsilon \epsilon^T] \\\ = \Lambda E[z z^T]\Lambda^T + E[\epsilon \epsilon^T]=\Lambda \Lambda^T + \Psi$$<br>　　将上述求解结果放在一起，得到：<br>$$\begin{bmatrix}z \\\ x \end{bmatrix} \sim N(\begin{bmatrix} \vec{0} \\\ \mu \end{bmatrix}, \begin{bmatrix}I \ 　\ \ \Lambda^T \\\ \Lambda \ \  \Lambda \Lambda^T + \Psi \end{bmatrix})$$<br>　　所以，得到ｘ的边际分布为：<br>$$x \sim N(\mu, \Lambda \Lambda^T + \Psi)$$<br>　　因而，对于一个训练集\(\{x^{(i)};i=1,2…,m\}\),我们可以写出参数的似然函数:<br>$$\ell(\mu,\Lambda,\Psi)=log \prod_{i=1}^m \frac{1}{(2\pi)^{n/2}|\Lambda \Lambda^T + \Psi|^{\frac{1}{2}}} * \\\ exp \left(-\frac{1}{2}(x^{(i)}-\mu)(\Lambda \Lambda^T + \Psi)^{-1} (x^{(i)}-\mu)^T \right)$$<br>　　由上式，若是直接最大化似然函数的方法求解参数的话，你会发现很难，因而下一节会介绍使用EM算法求解因子分析的参数。</p>
<h2 id="EM求解参数"><a href="#EM求解参数" class="headerlink" title="EM求解参数"></a>EM求解参数</h2><p>　　因子分析模型的EM求解直接套EM一般化算法中的E-step和M-step公式，对于E-step来说：<br>$$Q_i(z^{(i)})=p(z^{(i)}|x^{(i)};\mu,\Lambda,\Psi)$$<br>　　前面我们已经得到条件分布的期望和方差：<br>$$\mu_{z^{(i)}|x^{(i)}}=\Lambda^T(\Lambda \Lambda^T +\Psi)^{-1} (x^{(i)}-\mu) \\\ \Sigma_{z^{(i)}|x^{(i)}}=I-\Lambda^T (\Lambda \Lambda^T + \Psi)^{-1} \Lambda$$<br>　　代入上面两个公式，可以得到\(Q_i(z^{(i)})\)的概率密度函数了，即：<br>$$Q_i(z^{(i)})=\frac{1}{(2\pi)^{k/2}|\Sigma_{z^{(i)}|x^{(i)}}|^{1/2}}exp \left(-\frac{1}{2}(z^{(i)}-\mu_{z^{(i)}}|x^{(i)})^T \Sigma^{-1}_{z^{(i)}|x^{(i)}}(z^{(i)})-\mu_{z^{(i)}|x^{(i)}}) \right) $$　<br>　　在M-step中，需要最大化如下公式来求取<strong>参数\(\mu,\Lambda,\Psi\)</strong>:<br>$$\sum_{i=1}^m \int_{z^{(i)}} Q_i(z^{(i)}) log \frac{p(x^{(i)},z^{(i)};\mu,\Lambda,\Psi)}{Q_i(z^{(i)})}dz^{(i)} \\\\<br>=\sum_{i=1}^m \int_{z^{(i)}} Q_i(z^{(i)}) [log \ p(x^{(i)}|z^{(i)};\mu,\Lambda,\Psi) +log \ p(z^{(i)}) - log \ Q_i(z^{(i)})] dz^{(i)} \\\\<br>=\sum_{i=1}^m E_{z^{(i)} \sim Q_i}[log \ p(x^{(i)}|z^{(i)};\mu,\Lambda,\Psi) +log \ p(z^{(i)}) - log \ Q_i(z^{(i)})]$$<br>　　具体求解只需要分别对上述式子参数求偏导，令偏导函数为0即可求解。</p>
<h1 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h1><p><a href="http://open.163.com/special/opencourse/machinelearning.html" target="_blank" rel="external">斯坦福大学机器学习视频教程</a></p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;　　本文主要介绍因子分析模型(Factor Analysis Model)。因子分析模型是对高斯混合模型存在的问题进行解决的一种途径。同时也是属于“空间映射”思想的一种算法。本文将对因子分析模型进行介绍，并使用EM算法进行求解。&lt;br&gt;
    
    </summary>
    
      <category term="机器学习" scheme="xtf615.com/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="机器学习" scheme="xtf615.com/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="无监督学习" scheme="xtf615.com/tags/%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="空间映射" scheme="xtf615.com/tags/%E7%A9%BA%E9%97%B4%E6%98%A0%E5%B0%84/"/>
    
  </entry>
  
  <entry>
    <title>主成分分析</title>
    <link href="xtf615.com/2017/07/08/PCA/"/>
    <id>xtf615.com/2017/07/08/PCA/</id>
    <published>2017-07-08T00:38:56.000Z</published>
    <updated>2017-07-12T01:51:28.525Z</updated>
    
    <content type="html"><![CDATA[<p>　　本文主要介绍主成分分析算法(Principal Components Analysis,PCA)。该算法尝试搜寻数据所处的子空间，只需计算特征向量就可以进行降维。本文将尝试解释为什么通过特征向量计算能够实现降维。同时将介绍使用奇异值分解（SVD）方法来实现PCA求解。<br><a id="more"></a></p>
<h1 id="引入"><a href="#引入" class="headerlink" title="引入"></a>引入</h1><p>　　PCA解决了什么样的问题呢? 下面举一个例子来回答。<br>　　设想有一个数据集\(\{x^{(i)};i=1,…,m\}\),其中\(x^{(i)} \in \mathbb{R}^n \)。比如每个\(x\)代表一辆车，\(x\)的属性可能是车的最高速度，每公里耗油量等。如果有这样两个属性，一个以千米为单位的最大速度，一个以英里为单位的最大速度。这两个速度很显然是线性关系，可能因为数字取整等缘故有一点点扰动，但不影响整体线性关系。因此实际上，数据的信息量是n-1维度的。多一维度并不包括更多信息。PCA解决的就是将多余的属性去掉的问题。<br>　　再考虑这样一个例子，直升飞机驾驶员。每个驾驶员都有两个属性，第一个表示驾驶员的技能评估，第二个表示驾驶员对驾驶的兴趣程度。由于驾驶直升机难度较大，所以一般只有对其有很大的兴趣，才能较好的掌握这项技能。因此这两个属性是强相关的。实际上，根据已有的数据，可以将这两个属性使用坐标图进行展示，如下：<br><img src="/picture/machine-learning/pca1.png" alt="pca1"><br>　　由图可知，\(u_1\)展示了数据的相关性，称为主方向；\(u_2\)则反映了主方向之外的噪声，那么如何计算出主方向呢？</p>
<h1 id="预处理"><a href="#预处理" class="headerlink" title="预处理"></a>预处理</h1><p>　　运行PCA算法之前，数据一般需要预处理，预处理步骤如下：<br>1）令\(\mu=\frac{1}{m}\sum_{i=1}^m x^{(i)}\)<br>2) 使用\(x^{(i)}-\mu替换x^{(i)}\)<br>3）令\(\sigma_j^2=\frac{1}{m} \sum_i (x_j^{(i)})^2\)<br>4) 使用\(\frac{x_j^{(i)}}{\sigma_j}替换x_j^{(i)}\)<br>　　步骤1-2将数据的均值变成0，当已知数据的均值为0时，可以省略这两步。步骤3-4将数据的每个维度的方差变为1(此时均值已处理为0，故只需要求平方即可)，从而使得每个维度都在同一尺度都下被衡量，不会造成某些维度因数值较大而影响到。当预先知道数据处于同一尺度下时，可以忽略3-4步，比如图像处理中，已经预知了图像的每个像素都在0-255范围内，因而没有必要再进行归一化了。</p>
<h1 id="PCA算法"><a href="#PCA算法" class="headerlink" title="PCA算法"></a>PCA算法</h1><h2 id="直观理解"><a href="#直观理解" class="headerlink" title="直观理解"></a>直观理解</h2><p>　　如何找到数据的主方向呢？在二维空间下可以这样理解，有一个单位向量u，若从原点出发，这样定义u以后就相当于定义了一条直线。每个数据点在该直线上都有一个投影点，寻找主方向的任务就是寻找一个u使得投影点的方差最大化。<br>　　那么问题来了。问题1，能不能不从原点出发？可以，但那样计算就复杂了，我们归一化时已经将均值变为0，就是为了在寻找方向的时候使向量可以从原点出发，便于计算。问题2，多维空间下，多个主方向时怎么办？ 那就是不止寻找一个单位向量了，找到一个主方向后，将该主方向的方差影响去掉，然后再找主方向。如何去掉前一个主方向的方差影响呢？对于二维数据来说，是将所有数据点在垂直于该主方向的另一个方向上做投影，比如上图，要去掉主方向\(u_1\)的方差影响，需要在\(u_2\)方向上进行投影，多维空间上也可以类推。<br>　　以方差最大化来理解寻找主方向的依据是什么？直观上看，数据初始时会有一个方差，我们把这个方差当作数据包含的信息，我们找主方向的时候尽量使方差在子空间中最大化，从而能保留更多的信息。<br>　　再举一个例子来说明如何寻找主方向。比如下面图中的五个点。<br><img src="/picture/machine-learning/pca2.png" alt="pca2"><br>　　其中一个方向如下图所示:<br><img src="/picture/machine-learning/pca3.png" alt="pca3"><br>　　可以发现，上图中，直线上的黑点即原始数据在直线上的投影，投影的数据仍然保留着较大的方差。<br>　　相反如果取方向如下图所示：<br><img src="/picture/machine-learning/pca4.png" alt="pca4"><br>　　可以发现上图中，投影数据的方差很小。</p>
<h2 id="形式化定义"><a href="#形式化定义" class="headerlink" title="形式化定义"></a>形式化定义</h2><p>　　下面给出求主方向的形式化定义。<br>　　假设给定一个单位向量\(u\)和点\(x\),\(x\)到\(u\)的投影长度为\(x^T u\),即点\(x\)在\(u\)上的投影点，到坐标原点的距离为\(x^T u\).如下图所示:<br><img src="/picture/machine-learning/pca5.png" alt="pca5"><br>　　实际上就是向量内积。<br>　　因此，我们希望最大化投影方差，选择单位向量u使得下式最大化：<br>$$\frac{1}{m}\sum_{i=1}^m ({x^{(i)}}^T u)^2 = \frac{1}{m}\sum_{i=1}^m u^T x^{(i)} {x^{(i)}}^T u \\\\<br>=u^T \left(\frac{1}{m} \sum_{i=1}^m x^{(i)} {x^{(i)}}^T \right)u$$<br>　　上述平方展开可以根据\(X^2=X^T X\)得到，即\(({x^{(i)}}^T u)^2=({x^{(i)}}^T u)^T ({x^{(i)}}^T u)=u^T x^{(i)} {x^{(i)}}^T u\)<br>　　注意到，对于归一化后的数据，其投影点的均值也为0，因而才可以在方差的计算中直接平方。另外，该公式有一个约束条件,即\(||u||_2=1\)。<br>　　首先是协方差矩阵\(\Sigma=\frac{1}{m}\sum_{i=1}^m x^{(i)} {x^{(i)}}^T\)的理解。假设我们有两个特征a和b，有m个样本，则数据集表示为：<br>$$X^T=\begin{bmatrix} —-x^{(1)}—- \\\ —-x^{(2)}—- \\\ … \\\ —-x^{(m)}—- \end{bmatrix}=\begin{bmatrix} a_1 \ b_1 \\\\a_2 \ b_2 \\\ … \\\ a_m \ b_m \end{bmatrix}$$<br>　　则：<br>$$X=\begin{bmatrix}|　\ 　|　\ …　 \ | \\\\　 x^{(1)} \ x^{(2)}\ … \ x^{(m)} \\\\　 |　\ 　|　\ …　\ | \end{bmatrix}=\begin{bmatrix} a_1 \ a_2 \ … \ a_m \\\\b_1 \ b_2 \ … \ b_m \end{bmatrix}$$<br>　　则:<br>　　$$\Sigma=\frac{1}{m}\sum_{i=1}^m x^{(i)} {x^{(i)}}^T=\frac{1}{m}XX^T=\begin{bmatrix}\frac{1}{m} \sum_{i=1}^m a_i^2　\ \frac{1}{m} \sum_{i=1}^m a_i b_i \\\ \frac{1}{m} \sum_{i=1}^m a_i b_i　 \ \frac{1}{m} \sum_{i=1}^m b_i^2 \end{bmatrix}$$<br>　　这个最大化问题的解就是矩阵\(\Sigma=\frac{1}{m}\sum_{i=1}^m x^{(i)} {x^{(i)}}^T\)的特征向量。这是如何得到的呢？如下。<br>　　使用拉格朗日方程求解该最大化问题，则：<br>$$\ell=u^T \left(\frac{1}{m} \sum_{i=1}^m x^{(i)} {x^{(i)}}^T \right)u-\lambda(||u||_2-1)=u^T \Sigma u - \lambda(u^Tu-1)$$<br>　　对u求导：<br>$$\nabla_u \ell=\nabla_u(u^T (\Sigma)-\lambda(u^Tu-1))=\nabla_u u^T\Sigma u-\lambda \nabla_u u^T u \\\\<br>=\nabla_u tr(u^T\Sigma u)-\lambda \nabla_u tr(u^T u)=(\nabla_{u^T}tr(u^T \Sigma u))^T-\lambda(\nabla_{u^T}tr(u^T u))^T \\\\<br>={(\Sigma u)^T}^T-\lambda{u^T}^T=\Sigma u-\lambda u$$<br>令倒数为0，可知u就是\(\Sigma\)特征向量。<br>　　因为\(\Sigma\)是对称矩阵，因而可以得到相互正交的n个特征向量\(U^T=\{u^1,u^2,…,u^n\}\),那么如何达到降维的效果呢？选取最大的k个特征值所对应的特征向量即可。降维后的数据可以用如下式子来表达：<br>$$y^{(i)}=U^T x^{(i)}=\begin{bmatrix}u_1^T x^{(i)} \\\ u_2^T x^{(i)} \\\ … \\\ u_k^T x^{(i)}\end{bmatrix}$$<br>　　注意到，实际上通过特征向量来降维能够保证投影方差最大化，这也是我们的优化目标。</p>
<h1 id="PCA的应用"><a href="#PCA的应用" class="headerlink" title="PCA的应用"></a>PCA的应用</h1><p>　　压缩与可视化，如果将数据由高维降至2维和3维，那么可以使用一些可视化工具进行查看。同时数据的量也减少了。<br>　　预处理与降噪，很多监督算法在处理数据前都对数据进行降维，降维不仅使数据处理更快，还去除了数据中的噪声。是的数据的稀疏性变低，减少了模型假设的复杂度，从而降低了过拟合的概率。<br>　　具体的应用中，比如图片处理，对于一个100*100的图片，其原始特征长度为10000，，使用PCA降维后，大大减少了维度，形成了“特征脸”图片。而且还减少了噪声如光照等影响，使用PCA降维后的数据可以进行图片相似度计算，在图片检索中和人脸检测中都能达到很好的效果。</p>
<h1 id="奇异值分解（SVD）"><a href="#奇异值分解（SVD）" class="headerlink" title="奇异值分解（SVD）"></a>奇异值分解（SVD）</h1><p>　　奇异值分解时PCA的一种实现。前面我们提到PCA的实现手段是通过计算协方差矩阵\(\Sigma=\frac{1}{m} \sum_{i=1}^m x^{(i)} {x^{(i)}}^T\),然后对其特征值与特征向量进行求解。这样做的不好地方在于，协方差矩阵的维度是样本维度×样本维度。比如对于100×100的图片来说，如果以像素值作为特征，那么每张图片的特征维度是10000，则协方差矩阵的维度是10000×10000。在这样的协方差矩阵上求解特征值，耗费的计算量呈平方级增长。利用SVD可以求解出PCA的解，但是无需耗费大计算量，只需要耗费（样本量×样本维度）的计算量。下面介绍SVD:<br>　　SVD的基本公式如下：<br>$$A=UDV^T$$<br>　　即将\(A\)矩阵分解为\(U,D,V^T\)矩阵。其中，\(A \in \mathbb{R}^{m*n}, U \in \mathbb{R}^{m*n}, D \in \mathbb{R}^{n*n}\)，且\(D\)为对角矩阵，D对角线上的每个值都是特征值且已按照大小排好序，\(V^T \in \mathbb{R}^{n*n}\)。其中，U的列向量即是\(AA^T\)的特征向量，V的列向量是\(A^TA\)的特征向量。SVD的原理可以参见【参考】一节的知乎回答。令：<br>$$A=X=\begin{bmatrix}|　\ 　|　\ …　 \ | \\\\　 x^{(1)} \ x^{(2)}\ … \ x^{(m)} \\\\　 |　\ 　|　\ …　\ | \end{bmatrix}$$<br>　　因此计算量为X矩阵，即原始样本矩阵的大小。由前可知，协方差矩阵\(\Sigma=\frac{1}{m}XX^T\),那么U矩阵恰好为PCA的解。将PCA转化为SVD求解问题后，就可以进行加速了。因为SVD的求解有其特定的加速方法。本文不涉及。<br>　　SVD可以理解为PCA的一种求解方法。SVD也可以用于降维，一般情况下，D对角线中的前10%或20%的特征值已占全部特征值之和的90%以上。因而可以对\(UDV^T\)三个矩阵各自进行裁剪，比如将特征由n维降为k维。那么\(U \in \mathbb{R}^{m*k}, D \in \mathbb{R}^{k*k}, V^T \in \mathbb{R}^{k*n}\)即可。<br>　　在SVD的最后，Ng总结出一张表如下：<br><img src="/picture/machine-learning/pca6.png" alt="pca6"><br>　　表格中的内容很好理解，Ng特地强调的是这样的思考方式，寻找算法中的相同点和不同点有利于更好的理解算法。</p>
<h1 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h1><p><a href="http://open.163.com/special/opencourse/machinelearning.html" target="_blank" rel="external">斯坦福大学机器学习视频教程</a><br><a href="https://www.zhihu.com/question/39234760/answer/80323126" target="_blank" rel="external">知乎：为什么PCA可以通过求解协方差矩阵计算，也可以通过分解内积矩阵计算？</a></p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;　　本文主要介绍主成分分析算法(Principal Components Analysis,PCA)。该算法尝试搜寻数据所处的子空间，只需计算特征向量就可以进行降维。本文将尝试解释为什么通过特征向量计算能够实现降维。同时将介绍使用奇异值分解（SVD）方法来实现PCA求解。&lt;br&gt;
    
    </summary>
    
      <category term="机器学习" scheme="xtf615.com/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="机器学习" scheme="xtf615.com/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="空间映射" scheme="xtf615.com/tags/%E7%A9%BA%E9%97%B4%E6%98%A0%E5%B0%84/"/>
    
      <category term="降维" scheme="xtf615.com/tags/%E9%99%8D%E7%BB%B4/"/>
    
      <category term="主成分分析" scheme="xtf615.com/tags/%E4%B8%BB%E6%88%90%E5%88%86%E5%88%86%E6%9E%90/"/>
    
  </entry>
  
  <entry>
    <title>基于ActiveMQ和Redis的分布式车辆实时数据处理</title>
    <link href="xtf615.com/2017/06/23/distributed-data-processing/"/>
    <id>xtf615.com/2017/06/23/distributed-data-processing/</id>
    <published>2017-06-23T06:12:09.000Z</published>
    <updated>2017-06-23T07:26:32.233Z</updated>
    
    <content type="html"><![CDATA[<p>　　本次毕业设计论文的题目是基于ActiveMQ和Redis的分布式车辆实时数据处理。阐述围绕四个方面展开，应用背景、相关技术、解决方案、设计实现。<br><a id="more"></a></p>
<h1 id="应用背景"><a href="#应用背景" class="headerlink" title="应用背景"></a>应用背景</h1><p>　　车联网依赖于物联网和传感器的快速发展。物联网使得车与平台、车与人、人与平台三者的交流沟通成为可能；而传感器技术使得车辆位置、状态等数据的采集成为可能。<br>　　数据是车联网平台的核心，车联网平台的业务依赖于对数据的分析、挖掘和展示。因此车联网的一个重要挑战在于海量数据的存储和处理，这也带来了实时性、负载能力等诸多难题。<br><img src="/picture/machine-learning/distributed3.png" alt="distributed"><br>　　此次分享主要围绕数据处理展开。这也是我的毕设研究内容，基于ActiveMQ和Redis这两个中间件，探讨并构建一套实时性好、响应速度快、负载能力强的车联网分布式车辆实时数据处理解决方案。</p>
<h2 id="难点"><a href="#难点" class="headerlink" title="难点"></a>难点</h2><p>　　车载终端，也就是传感器上报频率非常高。如果按照30s发送一次实时位置，假设我们支持3万辆车的接入能力，以最大负载来算，所有的车辆全部在线，那么1分钟内产生的实时位置数据为6万条，1小时产生的数据为360万条，1天按10小时算的话，也会产生3600万条数据。实际上，产生的数据远不止车辆实时位置，还包括表征车辆状态的CAN数据等，可见数据量是很惊人的。<br>　　另外，由于我们的云服务平台地图模块是基于Web端设计的，平台的载体是浏览器客户端，浏览器的性能瓶颈使得我们不可能一次性同时将上万级别的数据发送至地图模块处理，因此必须进行数据过滤、处理，达到流量控制的目的。<br>　　全车监控的难点在于车辆数以及产生的位置数据众多，很难实现对所有车进行实时车辆位置的定位。一旦车辆众多，既可能造成数据传输到客户端过程中出现卡死现象，也可能在对车辆进行渲染展示时造成百度地图卡死现象。上图采取聚合操作，聚合会将邻近的车辆聚合在一起，并在聚合中心上显示车辆数，一个聚合中心内可能会有上千辆车。一旦有大量新数据产生，位置移动渲染过程以及聚合计算过程将会给浏览器和地图带来很大的负载压力。假如采取轮询的方式轮询这上万辆车，进行实时位置获取，那么势必更会造成客户端性能下降。如下图所示：<br><img src="/picture/machine-learning/distributed1.png" alt="distributed"><br>　　而单车监控的难点在于实时性的控制。如下图所示，会放大到最细粒度图层，并实时绘制轨迹。用户可能同时对多辆车进行实时监控。实时监控的难点在于数据的及时性。轮询的方式首先及时性不够，会受到轮询时间间隔的影响，同时仍然会加大客户端的负载，造成用户体验下降。<br><img src="/picture/machine-learning/distributed2.png" alt="distributed"><br>　　因此针对大数据量导致的客户端负载难点，本文需要考虑对大数据量进行多次的过滤和处理，根据一定的策略实现流量控制的目的。针对实时性要求，本文需要从多方面进行设计，既包括数据采集架构来加快数据采集的及时性；又包括读写吞吐量的提升来加快数据快速处理；同时还包括数据流转的方式设计，抛弃传统的轮询方式，构建消息驱动的方式来监听数据；最后需要设计复杂的地图监控的逻辑，包括车辆移动、聚合、轨迹绘制、全车监控和实时监控切换逻辑等。<br>　　因此，这里的核心问题在于，海量数据实时性处理需求与地图和浏览器客户端性能瓶颈之间的矛盾。本文的目标不仅是能够支持大数据量的处理、存储，还需要实现实时性、地图模块稳定性、平台运行流畅、用户体验好等目标。</p>
<h2 id="核心组件"><a href="#核心组件" class="headerlink" title="核心组件"></a>核心组件</h2><p>　　为了解决这样的难题，我们的核心组件或需求应该至少包括如下几个部分，<br>　　首先需要一个高效、响应速度快、高可用的数据采集架构，能够应付车载终端源源不断采集的数据。其次需要实现海量的数据存储，一边在不断采集数据，一边就需要保存这些数据。保存数据的目的在于进行离线分析，例如历史轨迹回放，驾驶行为分析等。再进一步，如果要进行在线分析的话，就需要实时性数据处理架构，例如车辆实时监控，远程诊断。中间两个步骤分别对应于离线分析和在线分析需求。处理完之后的数据，需要传输到表现层进行渲染，因此需要一个数据推送便捷、高效的架构，尤其是针对在线分析的功能。最后一步就是Web端渲染展示，例如基于百度地图实现车辆实时监控功能。<br><img src="/picture/machine-learning/distributed4.png" alt="distributed"></p>
<h1 id="核心技术"><a href="#核心技术" class="headerlink" title="核心技术"></a>核心技术</h1><p>　　这里涉及两种技术。</p>
<h2 id="ActiveMQ"><a href="#ActiveMQ" class="headerlink" title="ActiveMQ"></a>ActiveMQ</h2><p>　　ActiveMQ是Apache出品，最流行，能力最强劲的开源消息总线，有两种消息模型，P2P点对点模式和发布订阅模式。<br>　　MQ核心应用场景包括，异步处理、应用解耦、流量削峰、消息通讯。</p>
<ul>
<li>其中异步处理能够提升系统的响应速度，处在业务处理前端的生产者服务器在处理完业务请求后，将数据写入消息队列，不需要等待消费者服务器处理就可以直接返回，响应延迟减少。</li>
<li>应用解耦能够提高系统的高可用性，即使消费者服务器发生故障，数据也可以在消息队列服务器中存储堆积，生产者服务器可以继续处理业务请求，系统整体表现无障碍，消费者服务器恢复正常后，继续处理消息队列中的数据。</li>
<li>流量削峰能够消除并发访问高峰，它是将突然增加的访问请求数据放入消息队列中，等待消费者服务器依次处理，不会对整个系统负载造成太大压力。</li>
<li>而消息通讯则经常用在广播、通知、聊天等业务中，对于新增的用户，不需要额外增加一个接口来通知，只需要使用发布订阅模型就能轻松扩展。</li>
</ul>
<p>　　在车联网项目中，使用ActiveMQ搭建消息队列服务器，通过异步的方式处理车载终端采集的数据，大幅度提高车载终端的响应能力，随着接入终端数的不断增加，可通过横向扩展，快速提升吞吐量。</p>
<h2 id="Redis"><a href="#Redis" class="headerlink" title="Redis"></a>Redis</h2><p>　　Redis是一个开源、内存存储的数据结构服务器，可用作数据库，高速缓存、消息队列代理等。</p>
<ul>
<li>在易用性方面，Redis支持包括字符串，Hash表，链表，集合，有序结合在内的多种数据结构。</li>
<li>读写吞吐量方面，Redis基于内存存储，支持主从模式减少单机器并发量，支持分布式读写分离模型，提供基于管理批量读写。</li>
<li>高可用方面，Redis支持分布式集群部署、Redis主从复制、Sentinel哨兵支持单点故障转移。</li>
<li>最后，拥有众多的优秀客户端，例如Spring-data-redis、jedis等。</li>
</ul>
<p>　　在车联网项目中，将使用Redis构建分布式缓存集群。Redis提供了业务数据的内存存储和批量读写功能，可提高关键数据的快速读写。并提供主从热备、主从热切换功能，保证数据的高可靠性。</p>
<h1 id="解决方案"><a href="#解决方案" class="headerlink" title="解决方案"></a>解决方案</h1><p>　　该解决方案会围绕此前介绍的核心架构展开。</p>
<h2 id="数据采集架构"><a href="#数据采集架构" class="headerlink" title="数据采集架构"></a>数据采集架构</h2><p>　　前置机承担数据采集、初步校验解析、下发控制指令等工作，并将数据转发至消息队列服务器。<br>　　而消息队列服务器通过异步处理可提高前置机的响应速度和数据采集能力，随着接入终端数的不断增加，可通过横向扩展，快速的提升吞吐量，因为松耦合的软件体系结构，所以不同的业务服务器只需要采用订阅/发布的方式获取相关数据，并与数据层进行交互。<br><img src="/picture/machine-learning/distributed5.png" alt="distributed"></p>
<h2 id="数据存储架构"><a href="#数据存储架构" class="headerlink" title="数据存储架构"></a>数据存储架构</h2><p>　　结合不同的数据特点，采用不同的方式进行存储，如MySql、mongoDB、redis，并且结合不同的使用场景进行存储设计。<br>　　如图MYSQL可用于存储用户数据、车辆数据等平台中比较固定的一些业务数据。Redis缓存使用读写分离模型，适用于少量写，大量读的数据，加快业务的处理。MongoDB采用分表分片模式，用于存储海量的数据，将一些大表根据需要进行拆分，例如车辆状态数据就可能有上百/上千个属性。<br>　　因为异构数据库管理的复杂度较高，所以封装统一的访问层，从外部看，业务层仅连接一个单一的数据源。也就是统一数据访问模块。<br><img src="/picture/machine-learning/distributed6.png" alt="distributed"></p>
<h2 id="数据处理架构"><a href="#数据处理架构" class="headerlink" title="数据处理架构"></a>数据处理架构</h2><p>　　传统的方案是基于存储+轮询。构建DBP数据处理模块监听接收MQ中的数据，存储到非关系型数据库MongoDB，WEB端地图模块再通过轮询的方式读取MongoDB中的数据，再进行渲染处理。<br>　　该方案存在着诸多弊端。首先，实时性方面，存储到数据库中，再从数据库中取数据，本身步骤是繁琐多余的，比直接将数据传送到地图模块进行处理慢的多，因此会造成实时性的降低；其次，在性能方面，由于是采用轮询机制，并且需要监控每一辆在线的车，正常情况下是需要为每辆车都进行轮询查看有没有最新消息，因此会受到车辆数的影响，随着车辆的增加，性能越来越低。另外，性能还受到网络的影响，大量的数据库连接请求会降低数据库的吞吐能力。最后，稳定性方面，地图模块的稳定性会受到轮询方式的影响，由于JS是单线程的，随着车辆数的增加，轮询势必会影响地图模块的正常操作，造成用户体验下降。<br>　　本文的方案是基于缓存+消息中间件+websocket技术，能够兼顾负载能力、实时性、稳定性、流畅度等多项目标。<br>　　核心部分包括：</p>
<ul>
<li>基于Redis构建分布式缓存集群能够加快数据处理。</li>
<li>基于ActiveMQ构建数据处理模块，以消息驱动的方式对数据进行监听处理。</li>
<li>基于Websocket、百度地图实现实时监控，Websocket能够实现数据的主动推送，不需要前端进行轮询。</li>
</ul>
<h3 id="物理结构"><a href="#物理结构" class="headerlink" title="物理结构"></a>物理结构</h3><p>　　前置机模块采集车辆的位置、终端告警、CAN数据、行为数据、状态等数据，发送到消息服务器集群的全局队列MQ。这部分对应于前文提到的数据采集架构。<br>　　后台数据处理机部分的数据处理模块以异步的方式监听该队列中的数据，接收下来进行数据解析、过滤、清洗后，调用前文提到的数据存储架构中的统一数据访问模块存储到MongoDB、MYSQL或Redis当中。<br>　　对于在线分析功能，也就是实时监控的位置、告警、CAN状态数据，数据处理模块处理完后，会进行二次转发至ActiveMQ消息服务器的Location、Alarm、CAN主题。Web服务器会以发布订阅的方式监听获取主题里的数据，进行二次处理后，通过Websocket主动将数据推送至地图模块，进而进行监控或渲染。<br>　　我们可以看到这里面数据监听和处理有两处地方，<br>　　第一处是数据处理机中的数据处理模块，该模块承担着数据消费处理的核心任务，必须保证效率，所做的事情很单纯，接收解析数据后，保存到数据库当中。<br>　　第二处是Web服务器的监听模块，这部分主要用于数据的在线分析功能，也就是车辆实时监控、告警、CAN数据等，这里面主要是对数据进行二次处理。<br>　　为什么不直接在数据处理模块进行数据二次处理，并将这些数据使用WEBSOCKET推送到地图模块呢？原因在于效率，如果让数据处理模块承担这么多工作，这部分的消费就会很慢，这是数据处理的第一道门，这部分的效率慢的话，会直接导致后续业务应用的障碍。基于单一职责原则，这部分只负责数据的二次转发，数据二次处理放到WEB服务器进行处理。<br><img src="/picture/machine-learning/distributed7.png" alt="distributed"></p>
<h3 id="数据处理流程图"><a href="#数据处理流程图" class="headerlink" title="数据处理流程图"></a>数据处理流程图</h3><p>　　车载终端采集车辆数据第一次转发至MQ，数据处理模块进行第一次流量控制。<br>　　第一次流量控制主要工作是，会排除那些不存在在线监控用户的车辆数据，也就是说该车辆数据不存在监控它的在线用户，那就没必要发送到地图了。<br>　　如何判断某个数据是否存在在线用户监控它呢？这部分就需要Redis来提供大量快速的读操作，用户登录的时候会在车辆和在线用户归属关系的缓存中记录。排除后的数据只剩下存在在线用户监控的车辆数据，数据处理模块会将这部分剩余的数据二次转发到消息服务器中另外的Topic中。WEB应用模块接收下来后会进行第二次流量控制。<br>　　为了介绍第二次流量控制，这里首先介绍一下两种监控场景，全车监控和实时监控。<br>全车监控在地图级别比较大的时候，邻近的车辆会聚合起来。聚合物上面数字代表这里面聚合的车辆数目。地图放大的时候，车辆会显示出来。实时监控会实时绘制车辆的轨迹图。由于全车监控和单车监控粒度不同，全车监控会因为车辆数众多，导致地图聚合操作，而单车监控会放大到最细粒度图层，并实时绘制轨迹，故二者在粒度、精度、及时性方面要求不同。为了区分这两种方式的监控，在ActiveMQ中专门设置了两种主题，MonitorLocation对应全车监控，RealLocation对应单车实时监控。同样，用户在打开实时监控某辆车时，会在实时监控缓存中记录，因此数据处理模块在进行二次转发的时候，能够将这两种数据区分开来，发送到不同的主题当中。<br>　　在进行第二次流量控制的时候，针对全车监控，根据用户目前的地图级别、用户所处的视野、车辆位移来进行流量控制。首先是用户的视野，比如用户目前视野在福建，其他省份看不清，此时其他省份车辆位置就不需要发送过来进行监控。其次是地图级别，地图级别越大，地图图层越细，车辆移动的效果也就更明显，反之，如果地图级别缩小至全国级别，那么车辆移动很不明显，基本会处在聚合物当中，只有当位移比较大的时候，地图上才会有效果。因此需要根据地图级别以及车辆的位移来进行流量控制。整体策略是，只发送处在用户视野当中，并且位移能够明显被感知的情况。<br>　　而针对实时监控，我们不进行过滤和流量控制，只要存在监控该车的在线用户，则立即发送至前端地图模块进行实时轨迹绘制。<br>　　这样的区分能够完美的支持全车监控和单车监控场景。<br>　　紧接着，设计数据推送模块。对于处理完的数据，通过websocket和stomp.js并集成Spring，实现主动推送至前端地图模块的目的。抛弃了传统的轮询机制，能够大幅提高系统负载能力和性能，满足实时性要求。<br>　　最后，需要基于百度地图实现车辆实时监控，实现整个监控流程，包括车辆位置移动、车辆轨迹绘制、轨迹点保存、车辆聚合、全车监控和单车监控以及相互间的切换逻辑，其中聚合部分，对百度地图中的聚合操作进行优化。<br><img src="/picture/machine-learning/distributed8.png" alt="distributed"></p>
<h1 id="实现细节"><a href="#实现细节" class="headerlink" title="实现细节"></a>实现细节</h1><p>　　接着介绍一下实现细节。</p>
<h2 id="基于Redis构建分布式缓存"><a href="#基于Redis构建分布式缓存" class="headerlink" title="基于Redis构建分布式缓存"></a>基于Redis构建分布式缓存</h2><p>　　首先是基于Redis构建分布式缓存。从缓存集群、高可用架构、缓存API封装、缓存数据结构设计入手。</p>
<h3 id="缓存集群"><a href="#缓存集群" class="headerlink" title="缓存集群"></a>缓存集群</h3><p>　　首先是缓存集群，Redis提供了主从热备机制，主服务器的数据同步到从服务器，具体而言，采用一主多从的方式，主从之间进行数据同步，主节点接收到写操作后，直接返回成功，然后在后台用异步方式把数据同步到从节点上。主从另一个目的是进行读写分离，这是当单机读写压力过高的一种通用型解决方案。其主机的角色只提供写操作或少量的读，把多余读请求通过负载均衡算法分流到单个或多个slave服务器上。<br>　　Redis支持主从热切换。通过Sentinel哨兵组件实时监控主服务器状态并负责选举主服务器。当发现主服务器异常时根据一定的算法重新选举主服务器，并将问题服务器从可用列表中去除，原来的主节点恢复后以slave的身份重新加入。由于主节点IP发生变化，故在主节点地址发生变化后要及时通知到客户端，客户端收到新地址后，使用新地址继续发送新请求。<br><img src="/picture/machine-learning/distributed9.png" alt="distributed"></p>
<h3 id="高可用方案"><a href="#高可用方案" class="headerlink" title="高可用方案"></a>高可用方案</h3><p>　　接着是高可用方案设计。本文高可用方案包括: 构建Redis主从模式集群和Sentinel哨兵集群。使用1个主节点，3个从节点。主节点提供读写操作，从节点只提供读操作。因为数据处理模块是数据采集模块，存在着大量的读写操作，故基于本地化策略，减少网络传输消耗的时间，选择主节点Master安装在dbp模块，其余3个从节点，前期可以安装在单台机器的3个端口，后期测试稳定后，可以将其中一个安装在web端模块所在的服务器，基于本地化策略提供大量读操作，其余两个从节点单独使用两台服务器安装。另外，还需要配置3个哨兵，主节点dbp安装1个哨兵，另外3台从服务器选其中两台各安装一个，构建一个Sentinel集群，作为HA高可用方案，防止主节点单点失败。<br><img src="/picture/machine-learning/distributed10.png" alt="distributed"></p>
<h3 id="缓存API的封装"><a href="#缓存API的封装" class="headerlink" title="缓存API的封装"></a>缓存API的封装</h3><p>　　我们的目标是针对Redis缓存，定义一个通用的、轻量级的客户端访问框架，能够提供给业务层不同业务进行调用。<br>　　选择Redis的Java版本客户端Jedis进行API封装，Jedis提供了丰富的Redis底层数据结构操作方法并且Jedis能够配置连接池，也能够使用sentinel机制，通过连接sentinel集群支持HA高可用方案。<br>　　为了进一步简化API的封装以及集成Spring应用，我们选择组件spring data redis作为更上层的接口封装工具。并和jedis配套使用，spring-data-redis使用jredis作为底层连接Redis集群的工具。简化了大量复杂的数据操作方法。<br>　　更高层次的抽象，需要我们自己来定义。本文针对Redis不同的数据结构分别定制了RedisStringUtil，RedisSetUtil，RedisMapUtil，RedisListUtil四种工具类的封装，能够实现单条或批量操作。并支持自定义序列化方法。<br>　　这是部分API工具。RedisBaseUtil定义一些通用的操作。<br><img src="/picture/machine-learning/distributed11.png" alt="distributed"></p>
<h3 id="缓存数据结构"><a href="#缓存数据结构" class="headerlink" title="缓存数据结构"></a>缓存数据结构</h3><p>　　缓存数据结构强调一点。对于那些频繁读写某个类的部分字段的缓存。推荐使用hash结构。例如终端快照，<tss:终端号,属性,属性值>，可支持单独获取和修改某个或某些属性。如果大家听说过memcache的话，会发现在memcahche如果要实现这点，必须要把整个对象取出来，反序列化后，获取某个字段，再修改对象字段，序列化后存到缓存中，效率极其低下。Redis只需要一步，单独修改该字段即可。</tss:终端号,属性,属性值></p>
<h2 id="基于ActiveMQ构建实时数据处理模块"><a href="#基于ActiveMQ构建实时数据处理模块" class="headerlink" title="基于ActiveMQ构建实时数据处理模块"></a>基于ActiveMQ构建实时数据处理模块</h2><p>　　该部分是针对二次转发的主题中的数据进行接收处理，包括消息驱动实现数据监听，数据处理类图设计，全车监控流量控制。</p>
<h3 id="消息驱动"><a href="#消息驱动" class="headerlink" title="消息驱动"></a>消息驱动</h3><p>　　消息驱动主要是介绍Spring和MQ，Websocket集成。整体流程如图所示。具体配置代码也很简单。<br><img src="/picture/machine-learning/distributed12.png" alt="distributed"></p>
<h2 id="数据处理类图设计"><a href="#数据处理类图设计" class="headerlink" title="数据处理类图设计"></a>数据处理类图设计</h2><p>　　数据处理部分主要介绍使用设计模式来设计类之间的关系。为了实现低耦合、高内聚的设计，我们将数据接收、数据处理、数据发送逻辑独立出来。<br>　　其中最后一步数据发送逻辑是由上文中提到的spring-websocket组件SimpMessageSendingOperations来充当，只需要将其注入到数据处理类中，就可以直接调用方法convertAndSendToUser发送到前端进行渲染展示。<br>　　数据接收方面，我们使用MessageListenerAdapter来作为数据监听器代理类，然后委托给我们自定义的不同监听器类来执行，不同的监听器监听不同的主题，在这里，我们使用VehicleTrackListener来监听全车监控队列MonitorLocation，使用RealLocationListener来监听单车实时监控队列RealLocation，使用CanListener来监听CAN数据队列，AlarmListener来监听Alarm告警数据队列。监听器只做一件最简单的事情，就是接收数据。后续扩展非常方便，自定义监听器就可以。<br>　　数据处理方面，我们定义数据处理抽象父类MessageDispatcher，以及不同的子类，VehicleTrackDispatcher对应于全车监控数据处理，RealLocationDispatcher对应于单车监控数据处理，CanDispathcer对应于CAN数据处理，AlarmDispatcher对应于告警数据处理。<br>其中，MessageDispatcher父类定义了数据处理模板方法sendMessage，sendMessage定义了整个数据处理框架，首先获取所有的在线用户列表，然后遍历每条数据，获取有权限查看该车辆数据的所有在线用户，如果存在这样的在线用户，并且isDispatcher方法返回True，则进行发送。<br>　　isDispatcher是抽象方法，用于判断是否需要发送该数据，不同子类进行重载，根据不同数据各自的处理细节进行定制，例如全车监控需要根据用户视野和地图级别进行监控，单车监控则不进行拦截，默认返回True即可。<br>　　这部分扩展也非常方便，假如需要处理新的类型的数据。定义新的子类继承父类，如果处理逻辑类似，那么就可以共用sendMessage逻辑。重载抽象方法isDispatcher方法即可。如果不同逻辑的话，直接覆盖父类的sendMessage方法，自定义即可。<br><img src="/picture/machine-learning/distributed13.jpg" alt="distributed"></p>
<h3 id="全车监控流量控制"><a href="#全车监控流量控制" class="headerlink" title="全车监控流量控制"></a>全车监控流量控制</h3><p>　　接下来是全车流量控制，其中重点部分是视野和位移逻辑。全车监控流量控制的方法是综合考虑用户所在的地图视野、地图级别、地图聚合功能、前后两次车辆位移。首先是判断车辆位置是否处在用户视野范围，进一步判断移动位移是否足够大。<br><img src="/picture/machine-learning/distributed14.jpg" alt="distributed"></p>
<h2 id="基于Websocket-百度地图实现车辆监控"><a href="#基于Websocket-百度地图实现车辆监控" class="headerlink" title="基于Websocket/百度地图实现车辆监控"></a>基于Websocket/百度地图实现车辆监控</h2><h3 id="websocket"><a href="#websocket" class="headerlink" title="websocket"></a>websocket</h3><p>　　使用Websocket推送数据到前端，前端使用Stomp.js、Socket.js实现数据接收。这里面是使用Stomp协议进行数据传输的，Spring很容易进行配置，参考下官方文档。</p>
<h3 id="初次加载性能优化"><a href="#初次加载性能优化" class="headerlink" title="初次加载性能优化"></a>初次加载性能优化</h3><p>　　接着介绍下初次加载性能优化，我们的需求是在用户登录时，显示其视野内，有权限查看的所有车辆的最后一次位置。由于当时开发时MongoDB不支持这样复杂的批量查询，至于现在支不支持，没有查过。如果要查询的话，只能一条一条查询，性能很差。全车监控在实时性方面，只要用户在线的时候，视野内的、位移明显的最新数据能够实时展示即可，而对于初次登录的最后一次视野范围内车辆位置的精度要求不是非常高。<br>　　我们的解决方案是借助MYSQL批量查询功能，使用Quartz定时任务，在后台每30分钟将MongoDB中所有车辆的最后一次位置保存到MySQL终端快照表中，这样终端快照表中的数据就是最新的位置数据。初次加载地图时，我们只需要将车辆用户归属关系的表和终端快照表做一次连接，加上视野范围的限制条件，就能一次性批量查询出用户有权限查看的、最后一次视野范围内的车辆位置。经过这样的设计，能够实现秒级的响应性能。</p>
<h3 id="地图聚合优化"><a href="#地图聚合优化" class="headerlink" title="地图聚合优化"></a>地图聚合优化</h3><p>　　聚合功能是将邻近的车辆图标聚合在一起，减少地图上的图标数量，从而减轻地图操作，最终减轻负载的一种功能。<br>　　百度地图提供了相应的聚合功能，但是在我们实际测试过程中，发现百度地图聚合功能性能极差，在最新版的谷歌浏览器中测试，5000个点左右，对地图进行放大缩小操作，响应时间在3s左右，10000点响应时间在7s左右，30000点响应时间在20s左右，性能表现极差。我们平台目前拥有3万辆车，我们的目标是在3万辆车同时展示的最大负载情况下，依然能够实现秒级以内的性能体验，也就是说用户对地图的基础操作，应该在1s左右就能够得到响应，这样才能提高用户的体验。为了实现这样的性能提升，我们基于百度聚合功能进行性能优化。</p>
<h1 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h1><p>　　本文重点研究基于ActiveMQ和Redis构建分布式车辆实时数据处理环境，并实现车联网平台的核心业务，实时监控模块。<br>　　车联网平台大数据实时处理是一种很棘手的挑战，本文中提到的实时数据处理方法是众多应对方法中的一种。我认为本课题可以通过尝试更多方法对大数据实时处理方法进行深入探讨和研究具体方法包括：</p>
<ul>
<li>基于Storm实现实时流数据处理。Storm是一个在线实时、分布式以及具备高容错的计算系统。随着车联网平台数据进一步增加，我们可以通过搭建Storm集群，基于分布式算法来对车辆数据进行实时处理分析。</li>
<li>基于Spark构建大数据实时处理环境。Spark实现了内存级别的分布式处理模式，使用户无需关注复杂的内部工作机制，无需具备丰富的分布式系统知识及开发经验，即可实现大规模分布式系统的部署与大数据的并行处理。<br>具体改进上，可以将本文中提到的，基于web服务器监听方式的数据处理方法单独隔离出来，使用storm或spark分布式集群来处理，这样能够减轻web服务器的压力。</li>
</ul>
<p>　　总之，希望通过本文的研究，能够提供一种车联网实时数据处理的解决方案。</p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;　　本次毕业设计论文的题目是基于ActiveMQ和Redis的分布式车辆实时数据处理。阐述围绕四个方面展开，应用背景、相关技术、解决方案、设计实现。&lt;br&gt;
    
    </summary>
    
      <category term="大数据" scheme="xtf615.com/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
    
      <category term="大数据" scheme="xtf615.com/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
      <category term="分布式" scheme="xtf615.com/tags/%E5%88%86%E5%B8%83%E5%BC%8F/"/>
    
      <category term="redis" scheme="xtf615.com/tags/redis/"/>
    
      <category term="ActiveMQ" scheme="xtf615.com/tags/ActiveMQ/"/>
    
      <category term="车联网" scheme="xtf615.com/tags/%E8%BD%A6%E8%81%94%E7%BD%91/"/>
    
  </entry>
  
  <entry>
    <title>基于股评的情感分析和股市投资策略研究</title>
    <link href="xtf615.com/2017/05/14/stock-sentiment-analysis/"/>
    <id>xtf615.com/2017/05/14/stock-sentiment-analysis/</id>
    <published>2017-05-14T13:26:39.000Z</published>
    <updated>2017-05-15T02:06:49.881Z</updated>
    
    <content type="html"><![CDATA[<p>　　本次双学位论文的题目是基于股评的情感分析和投资策略研究。阐述围绕四个方面展开，研究背景和内容、构建情感分析模型、构建时间序列预测模型以及总结展望。<br><a id="more"></a></p>
<h1 id="研究背景"><a href="#研究背景" class="headerlink" title="研究背景"></a>研究背景</h1><p>　　首先是研究背景。<br>　　本文的研究是基于股票市场，股市是一国经济的晴雨表，然而股市受政策、新闻、舆论的影响非常大，容易波动剧烈。因此对股市进行研究很有必要。<br>　　其次随着互联网新媒体的发展，人们越来越倾向于通过互联网平台来交流信息。实时股评中包含丰富的金融信息，体现投资者的情绪变化。因此对股市的研究可以考虑从股评入手进行挖掘分析。<br>　　最后新理论和技术的兴起。行为金融学使得对于股评的挖掘有了理论基础。文本挖掘、机器学习、时间序列模型等技术兴起使得股评挖掘成为了可能。<br><img src="/picture/machine-learning/double-degree2.png" alt="double-degree"></p>
<h1 id="研究内容"><a href="#研究内容" class="headerlink" title="研究内容"></a>研究内容</h1><p>　　因此本文的研究内容是，<strong>对股评进行情感分析并构建情感指标，结合股价建立时间序列模型，对股票走势提供一定的预测能力</strong>。这里面主要包含两方面的工作：</p>
<ul>
<li>第一，构建情感分析分类模型，实现对股票评论情感倾向的快速判断。</li>
<li>第二，构建时间序列预测模型，对股市走势提供一定的预测能力。<br><img src="/picture/machine-learning/double-degree3.png" alt="double-degree"><h1 id="构建情感分析模型"><a href="#构建情感分析模型" class="headerlink" title="构建情感分析模型"></a>构建情感分析模型</h1><h2 id="数据获取"><a href="#数据获取" class="headerlink" title="数据获取"></a>数据获取</h2>　　首先是数据的获取。对于股评数据，选择东方财富网的“上证指数吧”提取股评数据。为此我专门使用Python语言设计了一款爬虫程序，爬取了3554页评论，时间跨度从2016-08-25到2017-3-15，共包含283905条股票评论。如图是其中几条股评以及设计的爬虫程序目录。<br><img src="/picture/machine-learning/double-degree4.png" alt="double-degree"><br><img src="/picture/machine-learning/double-degree5.png" alt="double-degree"><br>　　对于股票行情数据，使用开源财经接口包TuShare获取数据，采集的数据包括：日期、开盘价、最高价、收盘价、最低价、成交量、价格变动、涨跌幅等数据，实现对股票数据采集、清洗加工到存储的过程。<br><img src="/picture/machine-learning/double-degree6.png" alt="double-degree"><h2 id="构建流程"><a href="#构建流程" class="headerlink" title="构建流程"></a>构建流程</h2>　　接着是情感分析模型的构建流程。<strong>目标是构建一套情感分析和机器学习模型，挖掘股评中的情绪，实现对股票评论情感倾向的快速判断</strong>。<br>　　具体流程包括使用有监督机器学习分类方法、使用向量空间模型来进行文本表示、使用中文分词对句子进行切分，使用卡方统计量作为特征选择的指标、使用文本挖掘算法进行模型训练。<br><img src="/picture/machine-learning/double-degree7.png" alt="double-degree"><br>　　在构建过程中，首先我人工标注了5000条左右的看涨看跌数据，然后将数据集进行划分，3份作为训练，1份作为测试。最后进行模型训练以及使用多种指标在测试集上进行评估。如准确率、召回率等。如图，这是特征选择时得到的有用的特征项。可以看到“跑”这个词很大程度上反映了看跌股评。<br><img src="/picture/machine-learning/double-degree8.png" alt="double-degree"><br>　　下图是不同模型得到的指标。可以看到基于多项式的贝叶斯估计分类器在各项指标上综合表现最好，准确率得到了90%。我们选择该分类器来对28万多条的股评进行看涨看跌倾向判断。<br><img src="/picture/machine-learning/double-degree9.png" alt="double-degree"><h1 id="构建时间序列预测模型"><a href="#构建时间序列预测模型" class="headerlink" title="构建时间序列预测模型"></a>构建时间序列预测模型</h1><h2 id="指标选择"><a href="#指标选择" class="headerlink" title="指标选择"></a>指标选择</h2>　　首先是指标的选择。对于情感指标，本文选择看涨指数。计算得到以日为单位的情感指标时间序列数据。如下公式所示：<br>$$BI=ln(\frac{1+M^{bull}}{1+M^{bear}})$$<br>　　对于股票指标，选择收盘价和涨跌幅进行研究。得到以日为单位的收盘价和涨跌幅时间序列数据。<br><img src="/picture/machine-learning/double-degree10.png" alt="double-degree"><h2 id="基于股票价格的股票预测模型"><a href="#基于股票价格的股票预测模型" class="headerlink" title="基于股票价格的股票预测模型"></a>基于股票价格的股票预测模型</h2>　　首先是基于股票价格的股票预测模型。这里面先不考虑情感指标，单纯的基于股票价格。具体的构建步骤包括：<br>　　平稳性检验。使用ADF单位根检验。如下图是原始的收盘价时间序列，明显有个趋势，检验结果p值大于显著性水平也表明序列不平稳。<br><img src="/picture/machine-learning/double-degree11.png" alt="double-degree"><br><img src="/picture/machine-learning/double-degree12.png" alt="double-degree"><br>　　进行一阶差分后，进行ADF检验，发现序列已经平稳了。<br><img src="/picture/machine-learning/double-degree13.png" alt="double-degree"><br><img src="/picture/machine-learning/double-degree14.png" alt="double-degree"><br>　　接着是参数的选择。ARMA(p,q)有p,q两个参数。根据自相关图拖尾特征得到q=1，根据偏自相关图的拖尾特征得到p=2。进一步使用AIC准则进行参数选择，AIC越小表明模型越好，使用AIC准则同样得到q=1,p=2。<br><img src="/picture/machine-learning/double-degree15.png" alt="double-degree"><br>　　最后使用该模型进行预测。如图是原始数据和预测数据绘制的图。<br><img src="/picture/machine-learning/double-degree16.png" alt="double-degree"><br>　　根据均方根误差指标来检验：<br>$$RMSE=\sqrt{\frac{\sum_{i=1}^n(Y_{obs,i}-Y_{model,i})^2}{n}}$$<br>　　发现均方根误差为19.4138，偏大一点，不够理想。<h2 id="相关性分析"><a href="#相关性分析" class="headerlink" title="相关性分析"></a>相关性分析</h2>　　我们考虑基于情感看涨指数序列进行改进。<br>　　首先进行看涨指数序列和过去股票涨跌幅序列相关性分析。如下图是二者绘制在一起的图，可以看出来趋势挺一致的。<br><img src="/picture/machine-learning/double-degree17.png" alt="double-degree"><br>　　进一步使用pearson相关系数分析：<br>$$\rho_{X,Y}=corr(X,Y)=\frac{cov(X,Y)}{\sigma_X \sigma_Y}=\frac{E[(X-\mu_X)(Y-\mu_Y)]}{\sigma_X \sigma_Y}$$<br>　　得到相关系数为0.677，属于较强相关，因此看涨指数一定程度上反映了股价涨跌幅趋势。<br>　　注意：这里的相关性分析的主体是看涨指数序列与过去股票涨跌幅。看涨指数不受未来股票价格走势的影响。这个是下文构建带外生变量的ARIMA模型的前提。<h2 id="基于外生变量的ARIMA模型"><a href="#基于外生变量的ARIMA模型" class="headerlink" title="基于外生变量的ARIMA模型"></a>基于外生变量的ARIMA模型</h2>　　进一步我们构建基于外生变量的ARIMA模型。外生变量是指在经济机制中受外部因素影响的变量，可以影响内部变量。股评情感指数可以看作是很多外生变量的综合反映，反映了宏观经济、公司基本面信息、政策、重大事件等诸多股票价格变动的外在因素。<br>　　因此本文构建以收盘价为内生变量，股评看涨指数为外生变量的ARIMA模型，如图是拟合结果，均方差误差已经缩小至5.7。<br><img src="/picture/machine-learning/double-degree18.png" alt="double-degree"><br>　　进一步输出模型的AIC值，足够小表明模型稳定。<br><img src="/picture/machine-learning/double-degree19.png" alt="double-degree"><br>　　最后对模型进行系数检验，系数标准差很小且z值检验接近于0，可认为显著性水平高，为下面进一步预测奠定基础。<br><img src="/picture/machine-learning/double-degree20.png" alt="double-degree"><br>　　最后我们使用外生变量的ARIMA模型进行未来股价的预测及投资策略研究。本文主要研究短期预测。首先是静态预测，我们使用模型对未来n天数据进行预测，得到该表，可以看到3-15预测很准，3-16号预测数值相差较大，3-17趋势预测错误。<br><img src="/picture/machine-learning/double-degree21.png" alt="double-degree"><br>　　上面我们发现对第二天预测的结果表现不错，我们使用滚动预测，通过添加最新的数据预测第二天的数据，得到该表。发现趋势全部预测正确，数值方面存在一点偏差。<br><img src="/picture/machine-learning/double-degree22.png" alt="double-degree"><br>　　最后总结下投资策略选择，如图是预测的走势和实际走势图，很一致。<br><img src="/picture/machine-learning/double-degree23.png" alt="double-degree"><br>　　将其和历史数据一起绘制，从整体来看，更加吻合。<br><img src="/picture/machine-learning/double-degree24.png" alt="double-degree"><br>　　因此既可以根据看涨指数和涨跌幅的强相关性，能够大致得到股票收盘价格的整体趋势变化情况。也可以结合股票历史收盘价格序列以及看涨指数，使用带外生变量的ARIMA模型，来对股票走势进行预测。</li>
</ul>
<h1 id="总结展望"><a href="#总结展望" class="headerlink" title="总结展望"></a>总结展望</h1><p>　　最后总结展望一下。本文结合股票价格和情感指标，构建时间序列预测模型，对股市短期投资策略提供一定的参考。后续的工作：</p>
<ul>
<li>可以使用算法融合思想提高情感分析的精度。</li>
<li>推广研究对象，不局限于上证指数，实现对单一股票的分析。</li>
<li>也可以推广研究指标，不局限于收盘价格和看涨指数，可以对开盘价、转手率等进行研究或构建新的情感指标。</li>
<li>最后，也可以不局限于短期预测，探索长期预测的模型和方法。<br><img src="/picture/machine-learning/double-degree25.png" alt="double-degree"></li>
</ul>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;　　本次双学位论文的题目是基于股评的情感分析和投资策略研究。阐述围绕四个方面展开，研究背景和内容、构建情感分析模型、构建时间序列预测模型以及总结展望。&lt;br&gt;
    
    </summary>
    
      <category term="金融学" scheme="xtf615.com/categories/%E9%87%91%E8%9E%8D%E5%AD%A6/"/>
    
    
      <category term="时间序列" scheme="xtf615.com/tags/%E6%97%B6%E9%97%B4%E5%BA%8F%E5%88%97/"/>
    
      <category term="机器学习" scheme="xtf615.com/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="金融" scheme="xtf615.com/tags/%E9%87%91%E8%9E%8D/"/>
    
      <category term="情感分析" scheme="xtf615.com/tags/%E6%83%85%E6%84%9F%E5%88%86%E6%9E%90/"/>
    
  </entry>
  
  <entry>
    <title>K-means和混合高斯模型</title>
    <link href="xtf615.com/2017/04/07/%E8%81%9A%E7%B1%BB%E7%AE%97%E6%B3%95/"/>
    <id>xtf615.com/2017/04/07/聚类算法/</id>
    <published>2017-04-07T12:14:27.000Z</published>
    <updated>2017-07-10T08:40:54.653Z</updated>
    
    <content type="html"><![CDATA[<p>　　本文主要的内容包括：无监督学习中的K均值(k-means)聚类算法、混合高斯分布模型(Mixture of Gaussians, MoG)、求解MoG模型的期望最大化(EM)算法，以及EM一般化形式。<br><a id="more"></a></p>
<h1 id="k-means算法"><a href="#k-means算法" class="headerlink" title="k-means算法"></a>k-means算法</h1><p>　　在聚类问题中，给定一组数据\(\{x^{(1)},…,x^{(m)}\}\)，\(x^{(i)} \in \mathbb{R}^n\)，但是未给标签\(y^{(i)}\)。因此这是个无监督学习问题，需要聚类算法去发掘数据中的隐藏结构。</p>
<h2 id="算法"><a href="#算法" class="headerlink" title="算法"></a>算法</h2><p>　　k-means算法的具体流程如下：</p>
<ul>
<li>1）随机初始化k个聚类中心\(\mu_1,\mu_2,…,\mu_k \in \mathbb{R}^n\)</li>
<li>2）为每个样本数据选择聚类中心，即将其类别标号设为距离其最近的聚类中心的标号：<br>  $$c^{(i)}:=arg \min_j ||x^{(i)}-\mu_j||^2$$</li>
<li>3）更新聚类中心，即更新为属于该聚类中心的所有样本的平均值：<br>  $$\mu_j=\frac{\sum_{i=1}^m I\{c^{(i)}=j\}x^{(i)}}{\sum_{i=1}^m I\{c^{(i)}=j\}}$$</li>
<li>4）重复2、3步骤，直到聚类中心不变或变化低于阈值为止。</li>
</ul>
<p>　　在上述问题中，k是k-means算法的参数，是聚类中心的数量。\(\mu_j\)代表目前对某个聚类中心的猜测。为了初始化聚类中心，我们可以随机选取k个训练样本作为聚类中心初始值。下图是k=2时的一个聚类算法演示过程：<br><img src="/picture/machine-learning/cluster1.png" alt="cluster"></p>
<h2 id="优化函数"><a href="#优化函数" class="headerlink" title="优化函数"></a>优化函数</h2><p>　　聚类算法能够保证收敛吗？我们定义k-means的优化函数为：<br>$$J(c,\mu)=\sum_{i=1}^m ||x^{(i)}-\mu_{c^{(i)}}||^2$$<br>　　\(J\)衡量了每个样本距离其中心的距离平方和。可以将k-means算法看作是目标函数J的坐标下降(coordinate descent，在SVM中SMO算法中介绍过)过程。在第2步中，我们保持聚类中心不变，将样本类别设为距离最近的中心的类别，此时对于修改了类别中心的样本，其距离的平方会变小，即\(\sum_{修改类别的样本}||x-\mu||^2\)的值变小，而没有修改类别的样本J不变，从而整体变小。在第三步中，我们保持样本类别不变，更新了聚类中心点的值，这样使得对每个类别而言，其目标函数项会变小，即\(\sum_{属于某类的样本}||x-\mu||^2\)变小，从而整体变小。因此\(J\)会不断减小，从而保证收敛，通常这也意味着\(c、\mu\)也会收敛。理论上，不同的聚类中心可能会导致相同的收敛值J，称作震荡。但在实际中很少发生。<br>　　上述目标函数\(J\)是非凸的(non-convex)，因此坐标下降法不能保证收敛到全局最优值，容易陷入局部最优值。一个较为简单的解决方法是随机初始化多次，以最优的聚类结果(即J最小)为最终结果。<br>　　在聚类结束后，如果一个中心没有得到任何样本，那么需要去除这个中心点或者重新初始化。<br>　　聚类算法可用于离群点检测。比如飞机零件评测、信用卡消费行为异常监控等。</p>
<h1 id="混合高斯分布"><a href="#混合高斯分布" class="headerlink" title="混合高斯分布"></a>混合高斯分布</h1><p>　　混合高斯分布(MoG)也是一种无监督学习算法，常用于聚类。当聚类问题中各个类别的尺寸不同、聚类间有相关关系的时候，往往使用MoG更合适。对一个样本来说，MoG得到的是其属于各个类的概率(通过计算后验概率得到)，而不是完全的属于某个类，这种聚类方法被称作软聚类。一般来说，任意形状的概率分布都可以用多个高斯分布函数取近似，因而MoG的应用比较广泛。</p>
<h2 id="形式化表述"><a href="#形式化表述" class="headerlink" title="形式化表述"></a>形式化表述</h2><p>　　在MoG问题中，<strong>数据属于哪个分布可以看成是一个隐含变量</strong>\(z\)。与k-means的硬指定不同，我们首先认为\(z^{(i)}\)满足一定的概率分布，并使用联合概率分布来进行建模，即:\(p(x^{(i)},z^{(i)})=p(x^{(i)}|z^{(i)})p(z^{(i)})\)。其中，\(z^{(i)} \sim Multinomial(\phi)\)即z服从多项式分布(\(\phi_j \geq 0, \sum_{j=1}^k \phi_j=1,\phi_j=p(z^{(i)}=j)\))。\(x^{(i)}|z^{(i)} \sim \mathcal{N}(\mu_j,\Sigma_j)\),即在给定z的条件下，x服从高斯分布。令\(k\)为\(z^{(i)}\)取值范围的数量。<strong>MoG模型假设每个\(x^{(i)}\)的产生有两个步骤，首先从k个类别中按多项式分布随机选择一个\(z^{(i)}\)，然后在给定\(z^{(i)}\)条件下，从k个高斯分布中选择使得联合概率最大的高斯分布，并从该分布中生成数据\(x^{(i)}\)</strong>。<br>　　(注意：学习一个模型的关键在于理解其<strong>数据产生</strong>的假设。后面学习因子分析模型时，也要重点关注其<strong>数据产生</strong>的假设(低维空间映射到高维空间,再增加噪声)，这是上手的突破口。)<br>　　因此我们模型的参数是:\(\phi,\mu,\Sigma\)，为了估计这些参数，我们写出似然函数：<br>$$\mathcal{l}(\phi,\mu,\Sigma)=\sum_{i=1}^m log \ p(x^{(i)};\phi,\mu,\Sigma) \\\ = \sum_{i=1}^m log \ \sum_{z^{(i)}=1}^k p(x^{(i)}|z^{(i)};\mu,\Sigma)p(z^{(i)};\phi)$$<br>　　由于\(z^{(i)}\)是未知的，如果对上述求导并设为0来求解问题，会很难求解出最大似然估计值。<br>　　随机变量\(z^{(i)}\)指明了每个样本x^{(i)}到底是从哪个高斯分布生成的。如果\(z^{(i)}\)已知，则极大似然估计就变得很容易，重写为：<br>$$\mathcal{l}(\phi,\mu,\Sigma) = \sum_{i=1}^m log \ \sum_{z^{(i)}=1}^k p(x^{(i)}|z^{(i)};\mu,\Sigma)+log \ p(z^{(i)};\phi)$$<br>　　求导得到极大似然估计结果为：<br>$$\phi_j=\frac{1}{m}\sum_{i=1}^m I\{z^{i}=j\}\\\\<br>\mu_j=\frac{\sum_{i=1}^m I\{z^{i}=j\}x^{(i)}}{\sum_{i=1}^m I\{z^{(i)}=j\}}\\\\<br>\Sigma_j=\frac{\sum_{i=1}^m I\{z^{(i)}=j\}(x^{(i)}-\mu_j)(x^{(i)}-\mu_j)^T}{\sum_{i=1}^m I\{z^{(i)}=j\}}<br>$$<br>　　实际上，如果\(z^{(i)}\)的值已知，那么极大似然估计和之前生成算法中的GDA很类似，这里的\(z^{(i)}\)就相当于生成算法的类标签。所不同的是，GDA里的y是伯努利分布，而这里的z是多项式分布，并且每个样例有不同的协方差矩阵，而GDA中认为只有1个。<br>　　然而在我们的问题中，\(z^{(i)}\)是未知的，该如何解决？</p>
<h1 id="EM算法和混合高斯模型"><a href="#EM算法和混合高斯模型" class="headerlink" title="EM算法和混合高斯模型"></a>EM算法和混合高斯模型</h1><p>　　最大期望算法是一种迭代算法，主要有两个步骤。在我们的问题中，第一步E-step，尝试猜测\(z^{(i)}\)的值；第二步M-step,基于猜测，更新模型参数的值。<br>　　循环下面步骤，直到收敛：{<br>　　　　E：对于每个i和j,计算(即对每个样本i，计算由第j个高斯分布生成的概率，每个高斯分布代表一种类别，也就是z的分布):<br>$$w_j^{(i)}:= p(z^{(i)}=j|x^{(i)};\phi,\mu,\Sigma)=\frac{p(x^{(i)}|z^{(i)}=j;\mu,\Sigma)p(z^{(i)}=j;\phi)}{\sum_{l=1}^k p(x^{(i)}|z^{(i)}=l;\mu,\Sigma)p(z^{(i)}=l;\phi)}$$<br>　　　　M: 更新参数：<br>$$\phi_j:=\frac{1}{m}\sum_{i=1}^m w_j^{(i)}$$<br>$$\mu_j := \frac{\sum_{i=1}^m w_j^{(i)}x^{(i)}}{\sum_{i=1}^m w_j^{(i)}}$$<br>$$\Sigma_j := \frac{\sum_{i=1}^m w_j^{(i)}(x^{(i)}-\mu_j)(x^{(i)}-\mu_j)^T}{\sum_{i=1}^m w_j^{(i)}}$$</p>
<p>　　在E步中，我们将其他参数\(\Phi,\mu,\Sigma\)看作常量，计算\(z^{(i)}\)的后验概率，也就是估计隐含类别变量。其中，\(p(x^{(i)}|z^{(i)}=j;\mu,\Sigma)\)根据高斯密度函数得到，\(p(z^{(i)}=j;\phi)\)根据多项式分布得到。因此\(w_j^{(i)}\)代表隐含类变量\(z^{(i)}\)的软估计。<br>　　在M步中，估计好后，利用上面的公式重新计算其他参数，\(\phi_j\)是多项式分布的参数，决定了样本属于第j个高斯分布的概率。因为每个样本都会计算属于不同高斯分布生成的概率，所以可根据每个样本属于第j个高斯分布的概率来求平均得到。\(\mu_j,\Sigma\)是高斯分布的参数。<br>　　计算好后发现最大化似然估计时，\(w_j^{(i)}\)的值又不对了，需要重新计算，周而复始，直至收敛。<br>　　ＥＭ算法同样会陷入局部最优化，因此需要考虑使用不同的参数进行初始化。
　　</p>
<h1 id="一般化EM算法"><a href="#一般化EM算法" class="headerlink" title="一般化EM算法"></a>一般化EM算法</h1><p>　　上述EM算法是对于混合高斯模型的一个例子。到目前为止，我们还没有定量地给出EM的收敛性证明，以及一般化EM的推导过程。下面重点介绍这些内容。</p>
<h2 id="Jensen不等式"><a href="#Jensen不等式" class="headerlink" title="Jensen不等式"></a>Jensen不等式</h2><p>　　若f为凸函数，即\(f’’(x) \geq 0\)。注意，并不要求f一定可导，但若存在二阶导数，则必须恒大于等于0。再令X为随机变量，则存在不等式:<br>$$f(E[X]) \leq E[f(x)]$$<br>　　进一步，若二阶导数恒大于0，则不等式等号成立当且仅当x=E[x],即x是固定值。<br>　　若二阶导数的不等号方向逆转，则不等式的不等号方向逆转。<br><img src="/picture/machine-learning/cluster2.png" alt="cluster2"></p>
<h2 id="EM算法一般化形式"><a href="#EM算法一般化形式" class="headerlink" title="EM算法一般化形式"></a>EM算法一般化形式</h2><p>　　假设有一个训练集\(\{x^{(1)},x^{(2)},…,x^{(m)}\}\),由m个独立的样本构成，我们的目标是拟合包含隐变量的模型\(p(x,z)\),似然函数如下：<br>$$\ell (\theta)=\sum_{i=1}^m log p(x;\theta) \\\\<br>= \sum_{i=1}^m log \sum_{z^{(i)}} p(x,z;\theta)$$<br>　　直接对上式求导来求似然函数估计会非常困难。注意，这里的\(z^{(i)}\)是隐变量，并且和上面一样，如果\(z^{(i)}\)已知，那么似然估计很容易。但无监督算法中\(z^{(i)}\)未知。<br>　　在这种情况下，EM算法给出了最大似然估计的一种有效的求法。直接最大化\(\ell\)很困难。相反，我们通过构造\(\ell\)的下界(E-step)，并且最优化下界(M-step)来解决。<br>　　对每一个样本i，令\(Q_i\)为关于隐含变量z的分布,是一种概率(\(\sum_i Q_i(z)=1, Q_i(z) \geq 0\))<br>$$\sum_i log p(x^{(i)};\theta)=\sum_i log \sum_{z^{(i)}} p(x^{(i)},z^{(i)};\theta)\\\\<br>=\sum_{i} log \sum_{z^{(i)}} Q_i(z^{(i)})\frac{p(x^{(i)},z^{(i)};\theta)}{Q_i(z^{(i)})}\\\\<br>= \sum_{i} log E\left[\frac{p(x^{(i)},z^{(i)};\theta)}{Q_i(z^{(i)})}\right] \\\\<br>\geq  \sum_{i}  E\left[log \frac{p(x^{(i)},z^{(i)};\theta)}{Q_i(z^{(i)})}\right] \\\\<br>= \sum_{i}\sum_{z^{(i)}} Q_i(z^{(i)}) log \frac{p(x^{(i)},z^{(i)};\theta)}{Q_i(z^{(i)})}$$<br>　　上面推导根据Jensen不等式，log函数二阶导函数小于0，因此是非凸的，故不等号逆转。<br>　　因此有：<br>$$\ell(\theta) \geq \sum_{i}\sum_{z^{(i)}} Q_i(z^{(i)}) log \frac{p(x^{(i)},z^{(i)};\theta)}{Q_i(z^{(i)})}=lowbound(\theta)$$<br>　　现在，对任意分布\(Q_i\),\(lowbound(\theta)\)给出了\(\ell(\theta)\)的下界。\(Q_i\)的选择有很多种，我们该选择哪种呢? 假设目前已经求出了\(\theta\)的参数，我们肯定希望在\(\theta\)处使得下界更紧，最好能够使得不等式取等号。后面我们会证明，随着EM的迭代，\(\ell\)会稳步增加，逼近等号成立。<br>　　为了使得对于特定的\(\theta\)下界更紧，我们需要使得Jensen不等式取到等号。即\(X=E[X]\)时取到等号。当X为常数时，能够保证该条件成立。故令：<br>$$\frac{p(x^{(i)},z^{(i)};\theta)}{Q_i(z^{(i)};\theta)}=c$$<br>　　通过选择合适的\(Q_i(z^{(i)})\)，能够使得c不受\(z^{(i)}\)的影响。可以选择\(Q_i(z^{(i)})\)满足下式：<br>$$Q_i(z^{(i)}) \propto p(x^{(i)},z^{(i)};\theta)$$<br>　　又因为，\(\sum_{z^{(i)}} Q_i(z^{(i)})=1\),以及\(Q_i(z^{(i)})=\frac{p(x^{(i)},z^{(i)};\theta)}{c}\)<br>　　两边求和，<br>$$\sum_{z^{(i)}} Q_i(z^{(i)})=\sum_{z^{(i)}} \frac{p(x^{(i)},z^{(i)};\theta)}{c}=1$$<br>故，$$\sum_{z^{(i)}} p(x^{(i)},z^{(i)};\theta)=c$$<br>则：$$Q_i(z^{(i)})=\frac{p(x^{(i)},z^{(i)};\theta)}{c}=\frac{p(x^{(i)},z^{(i)};\theta)}{\sum_{z^{(i)}} p(x^{(i)},z^{(i)};\theta)}\\\\<br>=\frac{p(x^{(i)},z^{(i)};\theta)}{p(x^{(i)};\theta)} \\\\<br>=p(z^{(i)} | x^{(i)};\theta)$$<br>　　因此，只要令\(Q_i\)为给定\(\theta\)以及观察值x下，\(z^{(i)}\)的后验概率分布即可。<br>　　因此EM算法迭代过程如下：<br>　　E-step:对每一个样本i,令：<br>$$Q_i(z^{(i)}):=p(z^{(i)}|x^{(i)};\theta)$$<br>　　M-step,令：<br>　　$$\theta:=arg \max_\theta \sum_i \sum_{z^{(i)}} Q_i(z^{(i)}) log \frac{p(x^{(i)},z^{(i)};\theta)}{Q_i(z^{(i)})}$$<br>　　如何保证算法的收敛性？假设\(\theta^{(t)}\)和\(\theta^{(t+1)}\)是连续两次EM算法迭代求得的参数值。我们可以证明：\(\ell(\theta^{(t+1)}) \geq \ell(\theta^{(t)})\),这意味着随着迭代次数增加，最大似然值也在稳步增加。为了得到这样的结果，这里的关键在于Q的选择。假设EM算法初始参数值为\(\theta^{(t)}\),选择Q_i^{(t)}(z^{(i)}):=p(z^{(i)}|x^{(i)};\theta^{(t)}).前面我们知道，这样的选择能够保证Jensen不等式等号成立，即使得\(\ell\)的下界最紧，根据前面，我们有：<br>$$\ell(\theta^{(t)})=\sum_i \sum_{z^{(i)}} Q_i^{(t)}(z^{(i)}) log \frac{p(x^{(i)},z^{(i)};\theta^{t})}{Q_i^{(t)}(z^{(i)})}$$<br>进而：<br>$$\ell(\theta^{(t+1)}) \geq \sum_i \sum_{z^{(i)}} Q_i^{(t)}(z^{(i)}) log \frac{p(x^{(i)},z^{(i)};\theta^{t+1})}{Q_i^{(t)}(z^{(i)})} \\\\<br>\geq \sum_i \sum_{z^{(i)}} Q_i^{(t)}(z^{(i)}) log \frac{p(x^{(i)},z^{(i)};\theta^{t})}{Q_i^{(t)}(z^{(i)})} \\\\<br>=\ell(\theta^{(t)})$$<br>第一个式子是根据前面的Jensen不等式：<br>$$\ell(\theta) \geq \sum_i \sum_{z^{(i)}} Q_i(z^{(i)}) log \frac{p(x^{(i)},z^{(i)};\theta)}{Q_i(z^{(i)})}$$<br>当\(Q_i=Q_i^{(t)}\)时，取到等号，此时\(\theta=\theta^{(t+1)}\)。<br>第二个式子通过极大似然估计得到\(\theta^{(t+1)}\)值，也就是M-step：<br>$$arg \max_\theta \sum_i \sum_{z^{(i)}} Q_i(z^{(i)}) log \frac{p(x^{(i)},z^{(i)};\theta)}{Q_i(z^{(i)})}$$<br>　　为了便于理解，这里以一幅图来对EM算法进行总结。<br><img src="/picture/machine-learning/cluster3.png" alt="cluster3"><br>　　上述所展现的内容就是前面所述的主要思想，存在一个我们不能直接进行求导的似然函数，给定初始参数，我们找到在初始参数下紧挨着似然函数的下界函数，在下界上求极值来更新参数。然后以更新后的参数为初始值再次进行操作，这就是EM进行参数估计的方法。<br>　　当然似然函数不一定是图4中那样只有一个极值点，因而EM算法也有可能只求出局部极值。当然，可以像K-means那样多次选择初始参数进行求解，然后取最优的参数。<br>　　在EM的一般化形式中，可以将目标函数看作是：<br>$$J(Q,\theta)=\sum_{i=1}^m \sum_{z^{(i)}} Q_i(z^{(i)})log \frac{p(x^{(i)},z^{(i)};\theta)}{Q_i(z^{(i)})}$$<br>　　这样，EM算法就可以看作是对目标函数的坐标上升过程。在E-step中，\(\theta\)不变，调整Q使函数变大；在M-step中，Q不变，调整\(\theta\)使目标函数变大。</p>
<h1 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h1><p><a href="http://open.163.com/special/opencourse/machinelearning.html" target="_blank" rel="external">斯坦福大学机器学习视频教程</a></p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;　　本文主要的内容包括：无监督学习中的K均值(k-means)聚类算法、混合高斯分布模型(Mixture of Gaussians, MoG)、求解MoG模型的期望最大化(EM)算法，以及EM一般化形式。&lt;br&gt;
    
    </summary>
    
      <category term="机器学习" scheme="xtf615.com/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="机器学习" scheme="xtf615.com/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="无监督学习" scheme="xtf615.com/tags/%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="k-means" scheme="xtf615.com/tags/k-means/"/>
    
      <category term="混合高斯分布" scheme="xtf615.com/tags/%E6%B7%B7%E5%90%88%E9%AB%98%E6%96%AF%E5%88%86%E5%B8%83/"/>
    
      <category term="期望最大化算法" scheme="xtf615.com/tags/%E6%9C%9F%E6%9C%9B%E6%9C%80%E5%A4%A7%E5%8C%96%E7%AE%97%E6%B3%95/"/>
    
  </entry>
  
  <entry>
    <title>Advice for applying Machine Learning(2)</title>
    <link href="xtf615.com/2017/04/03/practice-ml-advice/"/>
    <id>xtf615.com/2017/04/03/practice-ml-advice/</id>
    <published>2017-04-03T07:21:13.000Z</published>
    <updated>2017-04-04T09:25:50.068Z</updated>
    
    <content type="html"><![CDATA[<p>　　本文对<a href="/2017/04/01/ml-advice/">Advice for applying Machine Learning</a>一文中提到的算法诊断等理论方法进行实践，使用Python工具，具体包括数据的可视化(data visualizing)、模型选择(choosing a machine learning method suitable for the problem at hand)、过拟合和欠拟合识别和处理(identifying and dealing with over and underfitting)、大数据集处理（dealing with large datasets）以及不同代价函数(pros and cons of different loss functions)优缺点等。<br><a id="more"></a></p>
<h1 id="数据可视化"><a href="#数据可视化" class="headerlink" title="数据可视化"></a>数据可视化</h1><h2 id="数据集获取"><a href="#数据集获取" class="headerlink" title="数据集获取"></a>数据集获取</h2><p>　　使用\(sklearn\)自带的\(make\_classification\)方法获取数据。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">from</span> sklearn.datasets <span class="keyword">import</span> make_classification</div><div class="line"><span class="keyword">from</span> pandas <span class="keyword">import</span> DataFrame</div><div class="line">X, y = make_classification(<span class="number">1000</span>, n_features=<span class="number">20</span>, n_informative=<span class="number">2</span>, </div><div class="line">                           n_redundant=<span class="number">2</span>, n_classes=<span class="number">2</span>, random_state=<span class="number">0</span>)</div><div class="line">columns = map(<span class="keyword">lambda</span> i:<span class="string">"col_"</span>+ str(i),range(<span class="number">20</span>)) + [<span class="string">"class"</span>]</div><div class="line">df = DataFrame(np.hstack((X, y[:, <span class="keyword">None</span>])), columns=columns)</div></pre></td></tr></table></figure>
<p>　　我们对二分类问题进行讨论，选取了1000个样本，20个特征。下表是部分数据：<br><img src="/picture/machine-learning/practice-advice1.jpg" alt="practice"><br>　　显然尽管维度很少，直接看这个数据很难得到关于问题的任何有用信息。我们通过可视化数据来发现规律。</p>
<h2 id="可视化"><a href="#可视化" class="headerlink" title="可视化"></a>可视化</h2><p>　　我们使用\(Seaborn\)开源库来进行可视化。<br>　　第一步我们使用pairplot方法来绘制任意两个维度和类别的关系，我们使用前100个数据，5个维度特征来进行绘图。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">_ = sns.pairplot(df[:<span class="number">100</span>], vars=[<span class="string">"col_8"</span>, <span class="string">"col_11"</span>, <span class="string">"col_12"</span>, <span class="string">"col_14"</span>, <span class="string">"col_19"</span>], hue=<span class="string">"class"</span>, size=<span class="number">1.5</span>)</div></pre></td></tr></table></figure></p>
<p><img src="/picture/machine-learning/practice-advice2.png" alt="practice"><br>　　上图25幅图，是5个维度特征两两组合的结果。对角线的柱状图反映了同一个维度不同类别之间取值的差异，从图中可以看出特征11和特征14取值在不同类别间差异显著。再观察散点图，散点图反映了任意两个维度组合特征和类别的关系，我们可以根据是否线性可分或者是否存在明显的相关来判断组合特征在类别判断中是否起到作用。如图特征11和特征14的散点图，我们发现基本上是线性可分的，而特征12和特征19则存在明显的反相关。对于相关性强的特征我们必须舍弃其一，对于和类别相关性强的特征必须保留。<br>　　我们继续观察特征与特征之间以及特征与类别之间的相关性：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">plt.figure(figsize=(<span class="number">12</span>, <span class="number">10</span>))</div><div class="line">plt.xticks(rotation=<span class="number">90</span>)</div><div class="line">_ = sns.heatmap(df.corr()) <span class="comment">#df.corr()是求相关系数函数</span></div></pre></td></tr></table></figure></p>
<p><img src="/picture/machine-learning/practice-advice3.png" alt="practice"><br>　　如上图，我们使用热力图来绘制不同特征之间以及特征与类别之间的相关性。首先看最后一行，反映了类别和不同特征之间的关系。可以看到，特征11和类别关系最密切，即特征11在类别判断中能起到很重要的作用。特征14、12次之。再看特征12和特征19，我们发现存在着明显的反相关，特征11和特征14正相关性也很强。因此存在一些冗余的特征。因为我们很多模型是假设在给定类别的情况下，特征取值之间是独立的，比如朴素贝叶斯。而剩余的其他特征大部分是噪声，既和其他特征不相关，也和类别不相关。</p>
<h1 id="模型初步选择"><a href="#模型初步选择" class="headerlink" title="模型初步选择"></a>模型初步选择</h1><p>　　一旦我们对数据进行可视化完，就可以快速使用模型来进行粗糙的学习(回顾前文提到的bulid-and-fixed方法)。由于机器学习模型多样，有的时候很难决定先用哪一种方法，根据一些总结的经验，我们使用如下图谱入手：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">from</span> IPython.display <span class="keyword">import</span> Image</div><div class="line">Image(filename=<span class="string">'machine-learning-method.png'</span>, width=<span class="number">800</span>, height=<span class="number">600</span>)</div></pre></td></tr></table></figure></p>
<p><img src="/picture/machine-learning/machine-learning-method.png" alt="practice"><br>　　因为我们有1000个样本，并且是有监督分类问题，根据图谱推荐使用\(LinearSVC\)，我们首先使用线性核函数的SVM来尝试建模。回顾一下\(SVM\)的目标函数：<br>$$\min_{\gamma,w,b} \frac{1}{2}{||w||}^2+C \sum_{i=1}^m \zeta_i \\\ 使得, y^{(i)}(w^T x^{(i)} + b) \geq 1-\zeta_i, 　　i=1,…,m \\\ \zeta_i \geq 0,　　i=1,…,m$$<br>　　上式使用的是L2-regularized,L1-loss(\(C \sum_{i=1}^m \zeta_i\))(具体含义参加<a href="/2017/03/28/SVM支持向量机/#软间隔分类器">SVM支持向量机-软间隔分类器一节</a>)。因此penalty=’l2’,loss=’hinge’,即：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line"><span class="comment">#http://scikit-learn.org/stable/modules/generated/sklearn.svm.LinearSVC.html</span></div><div class="line"><span class="keyword">from</span> sklearn.svm <span class="keyword">import</span> LinearSVC</div><div class="line"><span class="comment"># 二者间距较大，存在过拟合的嫌疑，即训练集拟合的很好，分数很高。但是测试集分数很低</span></div><div class="line">plot_learning_curve(LinearSVC(C=<span class="number">10.0</span>,penalty=<span class="string">'l2'</span>,loss=<span class="string">'hinge'</span>), <span class="string">"LinearSVC(C=10.0,penalty='l2',loss='hinge')"</span>,</div><div class="line">                    X, y, ylim=(<span class="number">0.8</span>, <span class="number">1.01</span>),</div><div class="line">                    train_sizes=np.linspace(<span class="number">.05</span>, <span class="number">0.2</span>, <span class="number">5</span>),baseline=<span class="number">0.9</span>)</div></pre></td></tr></table></figure></p>
<p><img src="/picture/machine-learning/practice-advice4.png" alt="practice"><br>　　上式是学习曲线，对应我们之前提到的诊断方法中的方差/误差分析图。我们在下一小节介绍该图的细节。我们现在先关注上图，我们只使用了20%(np.linspace第二个参数)，即200个数据进行训练测试。由图中可以看出，训练分数和泛化分数二者间距较大，并且训练分数处在一个很高的水准，根据之前介绍的偏差方差分析，我们可以得出，上述存在过拟合(over-fitting)的问题。注意，该学习曲线和之前偏差方差分析图存在区别：<br><img src="/picture/machine-learning/advice1.jpg" alt="advice"></p>
<p>　　区别在于，之前使用的是误差，这里使用的是得分。因此测试集和训练集分数曲线相对位置调换，训练集分数曲线在上，测试集分数曲线在下。随着样本的增多，误差曲线下降，这里分数曲线则是上升。但是相同点在于，过拟合图对应的学习曲线，训练分数(误差)和泛化分数(误差)二者间距较大，且训练分数(误差)处在一个高水准。</p>
<h2 id="学习曲线"><a href="#学习曲线" class="headerlink" title="学习曲线"></a>学习曲线</h2><p>　　这里我们先介绍下学习曲线绘制方法。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># http://scikit-learn.org/stable/modules/learning_curve.html#learning-curves</span></div><div class="line"><span class="keyword">from</span> sklearn.learning_curve <span class="keyword">import</span> learning_curve</div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">plot_learning_curve</span><span class="params">(estimator, title, X, y, ylim=None, cv=None,</span></span></div><div class="line">                        train_sizes=np.linspace<span class="params">(<span class="number">.1</span>, <span class="number">1.0</span>, <span class="number">5</span>)</span>,baseline=None):</div><div class="line">    <span class="string">"""</span></div><div class="line">    Generate a simple plot of the test and traning learning curve.</div><div class="line"></div><div class="line">    Parameters</div><div class="line">    ----------</div><div class="line">    estimator : object type that implements the "fit" and "predict" methods</div><div class="line">        An object of that type which is cloned for each validation.</div><div class="line"></div><div class="line">    title : string</div><div class="line">        Title for the chart.</div><div class="line"></div><div class="line">    X : array-like, shape (n_samples, n_features)</div><div class="line">        Training vector, where n_samples is the number of samples and</div><div class="line">        n_features is the number of features.</div><div class="line"></div><div class="line">    y : array-like, shape (n_samples) or (n_samples, n_features), optional</div><div class="line">        Target relative to X for classification or regression;</div><div class="line">        None for unsupervised learning.</div><div class="line"></div><div class="line">    ylim : tuple, shape (ymin, ymax), optional</div><div class="line">        Defines minimum and maximum yvalues plotted.</div><div class="line"></div><div class="line">    cv : integer, cross-validation generator, optional</div><div class="line">        If an integer is passed, it is the number of folds (defaults to 3).</div><div class="line">        Specific cross-validation objects can be passed, see</div><div class="line">        sklearn.cross_validation module for the list of possible objects</div><div class="line">    """</div><div class="line">    </div><div class="line">    plt.figure()</div><div class="line">    train_sizes, train_scores, test_scores = learning_curve(</div><div class="line">        estimator, X, y, cv=<span class="number">5</span>, n_jobs=<span class="number">1</span>, train_sizes=train_sizes)</div><div class="line">    <span class="keyword">print</span> train_sizes</div><div class="line">    <span class="keyword">print</span> <span class="string">'-------------'</span></div><div class="line">    <span class="keyword">print</span> train_scores</div><div class="line">    train_scores_mean = np.mean(train_scores, axis=<span class="number">1</span>)</div><div class="line">    train_scores_std = np.std(train_scores, axis=<span class="number">1</span>)</div><div class="line">    test_scores_mean = np.mean(test_scores, axis=<span class="number">1</span>)</div><div class="line">    test_scores_std = np.std(test_scores, axis=<span class="number">1</span>)</div><div class="line"></div><div class="line">    plt.fill_between(train_sizes, train_scores_mean - train_scores_std,</div><div class="line">                     train_scores_mean + train_scores_std, alpha=<span class="number">0.1</span>,</div><div class="line">                     color=<span class="string">"r"</span>)</div><div class="line">    plt.fill_between(train_sizes, test_scores_mean - test_scores_std,</div><div class="line">                     test_scores_mean + test_scores_std, alpha=<span class="number">0.1</span>, color=<span class="string">"g"</span>)</div><div class="line">    plt.plot(train_sizes, train_scores_mean, <span class="string">'o-'</span>, color=<span class="string">"r"</span>,</div><div class="line">             label=<span class="string">"Training score"</span>)</div><div class="line">    plt.plot(train_sizes, test_scores_mean, <span class="string">'o-'</span>, color=<span class="string">"b"</span>,</div><div class="line">             label=<span class="string">"Cross-validation score"</span>)</div><div class="line"></div><div class="line">    <span class="keyword">if</span> baseline:</div><div class="line">        plt.axhline(y=baseline,color=<span class="string">'red'</span>,linewidth=<span class="number">5</span>,label=<span class="string">'Desired Performance'</span>) <span class="comment">#baseline</span></div><div class="line">    plt.xlabel(<span class="string">"Training examples"</span>)</div><div class="line">    plt.ylabel(<span class="string">"Score"</span>)</div><div class="line">    plt.legend(loc=<span class="string">"best"</span>)</div><div class="line">    plt.grid(<span class="string">"on"</span>) </div><div class="line">    <span class="keyword">if</span> ylim:</div><div class="line">        plt.ylim(ylim)</div><div class="line">    plt.title(title)</div></pre></td></tr></table></figure></p>
<p>　　简要解释下几个重要点。首先是参数，estimator代表模型，title标题，X是样本数据集，y是标签集，ylim是学习曲线y轴的取值范围(min,max)，cv是交叉验证折数，train_sizes=np.linspace(.1, 1.0, 5)代表划分训练集，np.linspace(.1, 1.0, 5)返回的结果[ 0.1  ,  0.325,  0.55 ,  0.775,  1.   ]，即等间隔划分数据集，第一个参数是起始，第二个参数是终点，最后一个参数是划分份数。因为学习曲线的x轴代表样本的数量，即画出指标在训练集和验证集上样本数量变化的情况。我们不可能对每个样本量取值(从1一直递增到1000)都进行绘图，即不能画出平滑的曲线，而是取一些关键的点进行训练绘图，上述得到的train_sizes就是每次训练的样本占总样本的比例的数组。<br>　　接着是重要的一些代码。train_sizes, train_scores, test_scores = learning_curve(estimator, X, y, cv=5, n_jobs=1, train_sizes=train_sizes)返回的train_sizes是根据传入的train_sizes比例数组计算的实际训练样本数量数组。train_scores是训练集的得分，是一个二维数组，第一维等于train_sizes数组大小,即每次训练的分数，第二维等于交叉验证份数cv,即每次交叉验证的得分数组。test_scores是测试集的得分。因此可以取平均进行绘图，plt.fill_between方法是图中阴影的部分。</p>
<h1 id="过拟合处理"><a href="#过拟合处理" class="headerlink" title="过拟合处理"></a>过拟合处理</h1><p>　　有许多方法可以解决过拟合问题。</p>
<h2 id="增加样本数量"><a href="#增加样本数量" class="headerlink" title="增加样本数量"></a>增加样本数量</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">plot_learning_curve(LinearSVC(C=<span class="number">10.0</span>,penalty=<span class="string">'l2'</span>,loss=<span class="string">'hinge'</span>), <span class="string">"LinearSVC(C=10.0,penalty='l2',loss='hinge')"</span>,</div><div class="line">                    X, y, ylim=(<span class="number">0.8</span>, <span class="number">1.01</span>),</div><div class="line">                    train_sizes=np.linspace(<span class="number">.1</span>, <span class="number">1.0</span>, <span class="number">5</span>), baseline=<span class="number">0.9</span>)</div></pre></td></tr></table></figure>
<p><img src="/picture/machine-learning/practice-advice5.png" alt="practice"><br>　　这里修改linspace第二个参数为1，使用全部样本进行训练。我们发现泛化分数随着样本的增多不断增大，并且泛化分数和训练分数的间距不断缩小。但是高偏差的时候间距也是小的，我们继续进一步判断，发现训练分数和泛化分数都处在一个较高的水准，高于期望分数，而高偏差时，训练分数和泛化分数都比较低，低于理想分数。因此此时不存在过拟合或欠拟合的问题。</p>
<h2 id="减少特征"><a href="#减少特征" class="headerlink" title="减少特征"></a>减少特征</h2><p>　　根据前面的可视化分析，我们发现特征11和14和类别关联紧密，因此可以考虑先手动选择这两种特征进行训练。同样只在20%的样本上进行训练：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">plot_learning_curve(LinearSVC(C=<span class="number">10.0</span>,penalty=<span class="string">'l2'</span>,loss=<span class="string">'hinge'</span>), <span class="string">"LinearSVC(C=10.0,penalty='l2',loss='hinge') Features: 11&amp;14"</span>,</div><div class="line">                    df[[<span class="string">"col_11"</span>, <span class="string">"col_14"</span>]], y, ylim=(<span class="number">0.8</span>, <span class="number">1.0</span>),</div><div class="line">                    train_sizes=np.linspace(<span class="number">.05</span>, <span class="number">0.2</span>, <span class="number">5</span>),baseline=<span class="number">0.9</span>)</div></pre></td></tr></table></figure></p>
<p><img src="/picture/machine-learning/practice-advice6.png" alt="practice">　<br>　　和最早的那幅过拟合图相比，这里的结果已经好很多，基本上解决了过拟合的问题。但是这里的特征选择方法有点作弊嫌疑，首先是因为手动选择的，其次是因为我们是在1000个样本上进行选择的，而我们最终却只使用200个样本进行训练绘图。下面进行特征自动选择：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">from</span> sklearn.pipeline <span class="keyword">import</span> Pipeline</div><div class="line"><span class="keyword">from</span> sklearn.feature_selection <span class="keyword">import</span> SelectKBest, f_classif</div><div class="line"><span class="comment"># SelectKBest(f_classif, k=2) will select the k=2 best features according to their Anova F-value</span></div><div class="line">plot_learning_curve(Pipeline([(<span class="string">"fs"</span>, SelectKBest(f_classif, k=<span class="number">2</span>)), <span class="comment"># select two features</span></div><div class="line">                               (<span class="string">"svc"</span>, LinearSVC(C=<span class="number">10.0</span>,penalty=<span class="string">'l2'</span>,loss=<span class="string">'hinge'</span>))]),</div><div class="line">                    <span class="string">"SelectKBest(f_classif, k=2) + LinearSVC(C=10.0,penalty='l2',loss='hinge')"</span>,</div><div class="line">                    X, y, ylim=(<span class="number">0.8</span>, <span class="number">1.0</span>),</div><div class="line">                    train_sizes=np.linspace(<span class="number">.05</span>, <span class="number">0.2</span>, <span class="number">5</span>),baseline=<span class="number">0.9</span>)</div></pre></td></tr></table></figure></p>
<p><img src="/picture/machine-learning/practice-advice7.png" alt="practice">　<br>　　上述使用\(SelectKBest\)选择2个特征，我们发现在这个数据集上特征选择表现很好。注意，这种特征选择方法只是减少模型复杂度的一种方法。其他方法还包括，减少线性回归中多项式的阶数，减少神经网络中隐藏层的数量和节点数，增加高斯核函数的bandwidth(\(\sigma\)),或减小\(\gamma\)等(参考<a href="http://blog.csdn.net/wusecaiyun/article/details/49681431?locationNum=4" target="_blank" rel="external">SVM C和gamma参数理解</a>)。</p>
<h2 id="修改目标函数正则化项"><a href="#修改目标函数正则化项" class="headerlink" title="修改目标函数正则化项"></a>修改目标函数正则化项</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line"><span class="comment">#C表征了对离群点的重视程度，越大越重视，越大越容易过拟合。</span></div><div class="line"><span class="comment">#减小C可以一定程度上解决过拟合</span></div><div class="line">plot_learning_curve(LinearSVC(C=<span class="number">0.1</span>,penalty=<span class="string">'l2'</span>,loss=<span class="string">'hinge'</span>), <span class="string">"LinearSVC(C=0.1,penalty='l2',loss='hinge')"</span>, </div><div class="line">                    X, y, ylim=(<span class="number">0.8</span>, <span class="number">1.01</span>),</div><div class="line">                    train_sizes=np.linspace(<span class="number">.05</span>, <span class="number">0.2</span>, <span class="number">5</span>),baseline=<span class="number">0.9</span>)</div></pre></td></tr></table></figure>
<p><img src="/picture/machine-learning/practice-advice8.png" alt="practice">　<br>　　惩罚因子\(C\)决定了你有多重视离群点带来的损失，显然当所有离群点的松弛变量(\(\zeta\))的和一定时，你定的C越大，对目标函数的损失也越大，此时就暗示着你非常不愿意放弃这些离群点，最极端的情况是你把C定为无限大，这样只要稍有一个点离群，目标函数的值马上变成无限大，马上让问题变成无解，这就退化成了硬间隔问题，即C越大，你越希望在训练数据上少犯错误，而实际上这是不可能/没有意义的，于是就造成过拟合。<br>　　因此这里减少\(C\)能够一定程度上减少过拟合。<br>　　我们可以使用网格搜索来寻找最佳C。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line"><span class="comment">#使用网格搜索</span></div><div class="line"><span class="keyword">from</span> sklearn.grid_search <span class="keyword">import</span> GridSearchCV</div><div class="line">est = GridSearchCV(LinearSVC(penalty=<span class="string">'l2'</span>,loss=<span class="string">'hinge'</span>), </div><div class="line">                   param_grid=&#123;<span class="string">"C"</span>: [<span class="number">0.0001</span>,<span class="number">0.001</span>, <span class="number">0.01</span>, <span class="number">0.1</span>, <span class="number">1.0</span>, <span class="number">10.0</span>]&#125;)</div><div class="line">plot_learning_curve(est, <span class="string">"LinearSVC(C=AUTO)"</span>, </div><div class="line">                    X, y, ylim=(<span class="number">0.8</span>, <span class="number">1.0</span>),</div><div class="line">                    train_sizes=np.linspace(<span class="number">.05</span>, <span class="number">0.2</span>, <span class="number">5</span>),baseline=<span class="number">0.9</span>)</div><div class="line"><span class="keyword">print</span> <span class="string">"Chosen parameter on 100 datapoints: %s"</span> % est.fit(X[:<span class="number">100</span>], y[:<span class="number">100</span>]).best_params_</div></pre></td></tr></table></figure></p>
<p>输出结果：<strong>Chosen parameter on 100 datapoints: {‘C’: 0.01}</strong><br><img src="/picture/machine-learning/practice-advice9.png" alt="practice">　<br>　　特征选择看起来比修改正则化系数来的好。还有一种正则化方法，将LinearSVC的penalty设置为L1,官方文档解释为<strong>The ‘l1’ leads to coef_ vectors that are sparse</strong>,即L1可以导致稀疏参数矩阵，参数为0的特征不起作用，则相当于隐含的特征选择。不过注意,LinearSVC不支持L1-regularized和L1-loss,L1-regularized对应penalty=’l1’,L1-loss对应loss=’hinge’。可参考<a href="https://www.quora.com/Support-Vector-Machines/Liblinear-does-not-support-L1-regularized-L1-loss-hinge-loss-support-vector-classification-Why" target="_blank" rel="external">Liblinear does not support L1-regularized L1-loss ( hinge loss ) support vector classification. Why?</a>因此需要把loss改成’squared_hinge’。另外，此时不能用对偶问题来解决。故dual=False。<br><img src="/picture/machine-learning/practice-advice10.png" alt="practice">　<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">plot_learning_curve(LinearSVC(C=<span class="number">0.1</span>, penalty=<span class="string">'l1'</span>, loss=<span class="string">'squared_hinge'</span>,dual=<span class="keyword">False</span>), </div><div class="line">                    <span class="string">"LinearSVC(C=0.1, penalty='l1')"</span>, </div><div class="line">                    X, y, ylim=(<span class="number">0.8</span>, <span class="number">1.0</span>),</div><div class="line">                    train_sizes=np.linspace(<span class="number">.05</span>, <span class="number">0.2</span>, <span class="number">5</span>),baseline=<span class="number">0.9</span>)</div></pre></td></tr></table></figure></p>
<p><img src="/picture/machine-learning/practice-advice11.png" alt="practice">　<br>　　结果看起来不错。<br>　　学习到的参数如下：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">est = LinearSVC(C=<span class="number">0.1</span>, penalty=<span class="string">'l1'</span>, loss=<span class="string">'squared_hinge'</span>,dual=<span class="keyword">False</span>)</div><div class="line">est.fit(X[:<span class="number">150</span>], y[:<span class="number">150</span>])  <span class="comment"># fit on 150 datapoints</span></div><div class="line"><span class="keyword">print</span> <span class="string">"Coefficients learned: %s"</span> % est.coef_</div><div class="line"><span class="keyword">print</span> <span class="string">"Non-zero coefficients: %s"</span> % np.nonzero(est.coef_)[<span class="number">1</span>]</div></pre></td></tr></table></figure></p>
<p><img src="/picture/machine-learning/practice-advice12.png" alt="practice">　<br>　　可以看到特征11的权重最大，即最重要。</p>
<h1 id="欠拟合处理"><a href="#欠拟合处理" class="headerlink" title="欠拟合处理"></a>欠拟合处理</h1><p>　　之前使用的数据集分类结果都比较理想，我们尝试使用另一个二分类数据集。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">from</span> sklearn.datasets <span class="keyword">import</span> make_circles</div><div class="line">X, y = make_circles(n_samples=<span class="number">1000</span>, random_state=<span class="number">2</span>)<span class="comment">#只有2个特征</span></div><div class="line">plot_learning_curve(LinearSVC(C=<span class="number">0.25</span>), <span class="string">"LinearSVC(C=0.25)"</span>, </div><div class="line">                    X, y, ylim=(<span class="number">0.4</span>, <span class="number">1.0</span>),</div><div class="line">                    train_sizes=np.linspace(<span class="number">.1</span>, <span class="number">1.0</span>, <span class="number">5</span>))<span class="comment">#效果非常差</span></div></pre></td></tr></table></figure></p>
<p><img src="/picture/machine-learning/practice-advice13.png" alt="practice"><br>　　由上图可以看出，训练分数和泛化分数差距很小，并且训练分数明显低于期望分数。根据之前的方差/偏差分析可知，这里存在着明显的偏差，即欠拟合问题。<br>　　我们首先对数据进行可视化观察：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># 环形数据，外圈的数据是一种类别，内圈的数据是一种类别</span></div><div class="line">columns = map(<span class="keyword">lambda</span> i:<span class="string">"col_"</span>+ str(i),range(<span class="number">2</span>)) + [<span class="string">"class"</span>]</div><div class="line">df = DataFrame(np.hstack((X, y[:, <span class="keyword">None</span>])), </div><div class="line">               columns = columns)</div><div class="line">_ = sns.pairplot(df, vars=[<span class="string">"col_0"</span>, <span class="string">"col_1"</span>], hue=<span class="string">"class"</span>, size=<span class="number">3.5</span>)</div></pre></td></tr></table></figure></p>
<p><img src="/picture/machine-learning/practice-advice14.png" alt="practice"><br>　　根据上图，该数据集是环形数据，外圈的点代表一种类别，内圈的点代表另一种类别。显然上述数据是线性不可分的，使用再多数据或者减少特征都没用，我们的模型是错误的，需要进行欠拟合处理。</p>
<h2 id="增加或使用更好的特征"><a href="#增加或使用更好的特征" class="headerlink" title="增加或使用更好的特征"></a>增加或使用更好的特征</h2><p>　　我们尝试增加特征，根据散点图，显然不同类别距离原点的距离不同，我们可以增加到原点的距离这一特征。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line"><span class="comment">#解决欠拟合方法1：增加特征</span></div><div class="line"><span class="comment"># X[:, [0]]**2 + X[:, [1]]**2)计算的是离原点的距离</span></div><div class="line">X_orginal_distance = X[:, [<span class="number">0</span>]]**<span class="number">2</span> + X[:, [<span class="number">1</span>]]**<span class="number">2</span><span class="comment">#X[:, [0]]将得到的列数据变成二维的形式，[[  8.93841424e-01],[ -7.63891636e-01]...]</span></div><div class="line">df[<span class="string">'col_3'</span>] = X_orginal_distance </div><div class="line"><span class="comment">#可以看到完全线性可分</span></div><div class="line">_ = sns.pairplot(df, vars=[<span class="string">"col_0"</span>, <span class="string">"col_1"</span>,<span class="string">"col_3"</span>], hue=<span class="string">"class"</span>, size=<span class="number">3.5</span>)</div></pre></td></tr></table></figure></p>
<p><img src="/picture/machine-learning/practice-advice15.png" alt="practice"><br>　　由最后一幅图，我们发现根据col_3新特征，就能将类别完全线性分隔开，因此col_3特征在区分类别上能起决定性作用。不妨看看热力图：<br><img src="/picture/machine-learning/practice-advice16.png" alt="practice"><br>　　根据热力图，我们发现col_3和类别存在着非常强的负相关性。使用新增完的特征集进行预测：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">X_extra = np.hstack((X,X[:,[<span class="number">0</span>]]**<span class="number">2</span>+X[:,[<span class="number">1</span>]]**<span class="number">2</span>))</div><div class="line">plot_learning_curve(LinearSVC(C=<span class="number">10</span>,penalty=<span class="string">'l2'</span>,loss=<span class="string">'hinge'</span>), <span class="string">"LinearSVC(C=10,penalty='l2',loss='hinge')"</span>, </div><div class="line">                    X_extra, y, ylim=(<span class="number">0</span>, <span class="number">1.01</span>),</div><div class="line">                    train_sizes=np.linspace(<span class="number">.1</span>, <span class="number">1.0</span>, <span class="number">5</span>),baseline=<span class="number">0.9</span>)</div></pre></td></tr></table></figure></p>
<p><img src="/picture/machine-learning/practice-advice17.png" alt="practice"><br>　　根据结果，完全分开了样本。我们可以进一步思考，是否可以让模型进行自动生成新特征？</p>
<h2 id="使用更复杂的模型"><a href="#使用更复杂的模型" class="headerlink" title="使用更复杂的模型"></a>使用更复杂的模型</h2><p>　　<strong>使用复杂的模型，相当于更换了目标函数</strong>。根据上面数据集非线性可分的特点，我们可尝试非线性分类器，使用RBF核的SVM进行分类。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">from</span> sklearn.svm <span class="keyword">import</span> SVC</div><div class="line"><span class="comment"># note: we use the original X without the extra feature</span></div><div class="line"><span class="comment"># 使用RBF核，最小间隔gamma设为1.</span></div><div class="line">plot_learning_curve(SVC(C=<span class="number">10</span>, kernel=<span class="string">"rbf"</span>, gamma=<span class="number">1.0</span>),</div><div class="line">                    <span class="string">"SVC(C=10, kernel='rbf', gamma=1.0)"</span>,</div><div class="line">                    X, y, ylim=(<span class="number">0.5</span>, <span class="number">1.1</span>), </div><div class="line">                    train_sizes=np.linspace(<span class="number">.1</span>, <span class="number">1.0</span>, <span class="number">5</span>),baseline=<span class="number">0.9</span>)</div></pre></td></tr></table></figure></p>
<p><img src="/picture/machine-learning/practice-advice18.png" alt="practice"><br>　　注意上述建模使用的是原始数据集X，而没有用新的特征。可以发现结果很理想，RBF核会将特征映射到高维空间，因此得到的非线性模型效果很好。</p>
<h1 id="大数据集和高维特征处理"><a href="#大数据集和高维特征处理" class="headerlink" title="大数据集和高维特征处理"></a>大数据集和高维特征处理</h1><h2 id="SGDClassfier增量学习"><a href="#SGDClassfier增量学习" class="headerlink" title="SGDClassfier增量学习"></a>SGDClassfier增量学习</h2><p>　　如果数据集增大，特征增多，那么上述SVM运行会变慢很多。根据之前的图谱推荐，此时可以使用\(SGDClassifier\)，该分类器也是一个线性模型,但是使用随机梯度下降法(stochastic gradient descent),\(SGDClassifier\)对特征缩放很敏感，因此可以考虑标准化数据集，使特征均值为0，方差为1.<br>　　\(SGDClassifier\)允许增量学习，会在线学习，在数据量很大的时候很有用。此时不适合采用交叉验证，我们采取progressive validation方法，即将数据集等分成块，每次在前一块训练，在后一块验证，并且使用增量学习，后面块的学习是在前面块学习的基础上继续学习的。　<br>　　首先生成数据，20万+200特征+10个类别。　<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">X, y = make_classification(<span class="number">200000</span>, n_features=<span class="number">200</span>, n_informative=<span class="number">25</span>, </div><div class="line">                           n_redundant=<span class="number">0</span>, n_classes=<span class="number">10</span>, class_sep=<span class="number">2</span>,</div><div class="line">                           random_state=<span class="number">0</span>)</div></pre></td></tr></table></figure></p>
<p>　　建模和验证：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span> SGDClassifier</div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">sgd_score</span><span class="params">(X,y)</span>:</span></div><div class="line">    est = SGDClassifier(penalty=<span class="string">"l2"</span>, alpha=<span class="number">0.001</span>)</div><div class="line">    progressive_validation_score = []</div><div class="line">    train_score = []</div><div class="line">    <span class="keyword">for</span> datapoint <span class="keyword">in</span> range(<span class="number">0</span>, <span class="number">199000</span>, <span class="number">1000</span>):</div><div class="line">        X_batch = X[datapoint:datapoint+<span class="number">1000</span>]</div><div class="line">        y_batch = y[datapoint:datapoint+<span class="number">1000</span>]</div><div class="line">        <span class="keyword">if</span> datapoint &gt; <span class="number">0</span>:</div><div class="line">            progressive_validation_score.append(est.score(X_batch, y_batch))</div><div class="line">        est.partial_fit(X_batch, y_batch, classes=range(<span class="number">10</span>)) <span class="comment">#增量学习或称为在线学习</span></div><div class="line">        <span class="keyword">if</span> datapoint &gt; <span class="number">0</span>:</div><div class="line">            train_score.append(est.score(X_batch, y_batch))</div><div class="line">            </div><div class="line">    plt.plot(train_score, label=<span class="string">"train score"</span>,color=<span class="string">'blue'</span>)</div><div class="line">    plt.plot(progressive_validation_score, label=<span class="string">"progressive validation score"</span>,color=<span class="string">'red'</span>)</div><div class="line">    plt.xlabel(<span class="string">"Mini-batch"</span>)</div><div class="line">    plt.ylabel(<span class="string">"Score"</span>)</div><div class="line">    plt.axhline(y=<span class="number">0.8</span>,color=<span class="string">'red'</span>,linewidth=<span class="number">5</span>,label=<span class="string">'Desired Performance'</span>) <span class="comment">#baseline</span></div><div class="line">    plt.legend(loc=<span class="string">'best'</span>)</div><div class="line">sgd_score(X,y)</div></pre></td></tr></table></figure></p>
<p><img src="/picture/machine-learning/practice-advice19.png" alt="practice"><br>　　上图表明，在50次mini-batches滞后，分数提高就很少了，因此我们可以提前停止训练。由于训练分数和泛化分数差距很小，其训练分数较低，因此可能存在欠拟合的可能。<br>然而SGDClassifier不支持核技巧，根据图谱可以使用kernel approximation。<br>　　The advantage of using approximate explicit feature maps compared to the kernel trick, which makes use of feature maps implicitly, is that explicit mappings can be better suited for online learning and can significantly reduce the cost of learning with very large datasets. The combination of kernel map approximations with SGDClassifier can make non-linear learning on large datasets possible.<br>　　相较于核函数隐示的映射，kernel approximation使用显示的映射方法，这对在线学习非常重要，可以减少超大数据集的学习代价。使用SGDClassifier配合kernel approximation可以在大数据集上实现非线性学习的目的。</p>
<h2 id="手写体数字识别"><a href="#手写体数字识别" class="headerlink" title="手写体数字识别"></a>手写体数字识别</h2><p>　　现在尝试对手写体数字问题进行建模。</p>
<h3 id="可视化-1"><a href="#可视化-1" class="headerlink" title="可视化"></a>可视化</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># http://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_digits.html#sklearn.datasets.load_digits</span></div><div class="line"><span class="keyword">from</span> sklearn.datasets <span class="keyword">import</span> load_digits</div><div class="line">digits = load_digits(n_class=<span class="number">6</span>)</div><div class="line">X = digits.data</div><div class="line">y = digits.target</div><div class="line">n_samples, n_features = X.shape</div><div class="line"><span class="keyword">print</span> <span class="string">"Dataset consist of %d samples with %d features each"</span> % (n_samples, n_features)</div><div class="line"></div><div class="line"><span class="comment"># Plot images of the digits</span></div><div class="line">n_img_per_row = <span class="number">20</span> <span class="comment">#最大为32，即展示1024个样本</span></div><div class="line">img = np.zeros((<span class="number">10</span> * n_img_per_row, <span class="number">10</span> * n_img_per_row)) <span class="comment"># 200*200规格的像素矩阵</span></div><div class="line"></div><div class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(n_img_per_row):</div><div class="line">    ix = <span class="number">10</span> * i + <span class="number">1</span> <span class="comment">#空1个像素点</span></div><div class="line">    <span class="keyword">for</span> j <span class="keyword">in</span> range(n_img_per_row):</div><div class="line">        iy = <span class="number">10</span> * j + <span class="number">1</span></div><div class="line">        img[ix:ix + <span class="number">8</span>, iy:iy + <span class="number">8</span>] = X[i * n_img_per_row + j].reshape((<span class="number">8</span>, <span class="number">8</span>))<span class="comment">#1行64个特征是通过8*8展平的,存入分块矩阵</span></div><div class="line"></div><div class="line">plt.imshow(img, cmap=plt.cm.binary)</div><div class="line">plt.xticks([])</div><div class="line">plt.yticks([])</div><div class="line">_ = plt.title(<span class="string">'A selection from the 8*8=64-dimensional digits dataset'</span>)</div></pre></td></tr></table></figure>
<p><img src="/picture/machine-learning/practice-advice20.png" alt="practice"><br>　　手写体数字的64维特征就是一个8*8数字图片每个像素点平铺开来的。因此我们可以通过上面代码进行重建图片。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">print</span> digits.images.shape <span class="comment">#三维数组(1083L, 8L, 8L)，1083个样本</span></div><div class="line"><span class="keyword">print</span> img.shape <span class="comment">#二维数组(200L,200L),每个样本占8*8小分块矩阵。每8行20个样本，一共可以放400个样本。</span></div><div class="line"><span class="comment">#可以扩大该二维数组，例如(320L,320L), 每个样本占8*8小分块矩阵， 每8行展示32个样本，最大可以展示1024个样本。即32*32</span></div><div class="line"><span class="comment"># digits.images[0] == img[1:9,1:9]</span></div><div class="line"><span class="comment"># digits.images[1] == img[1:9,11:19]</span></div><div class="line">plt.matshow(digits.images[<span class="number">1</span>],cmap=plt.cm.gray)  <span class="comment">#第二个样本为数字1</span></div><div class="line">plt.matshow(img[<span class="number">1</span>:<span class="number">9</span>,<span class="number">11</span>:<span class="number">19</span>],cmap=plt.cm.gray)  <span class="comment">#第二个样本数字1</span></div></pre></td></tr></table></figure></p>
<p><img src="/picture/machine-learning/practice-advice21.png" alt="practice"><br>　　上述代码展示一个数字的结果，可以发现digits.images<a href="/2017/04/01/ml-advice/">1</a>和img[1:9,11:19]都是代表第二个样本，我们可以从图中看出第二个样本数字是1。<br>　　进一步可视化：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># Helper function based on </span></div><div class="line"><span class="comment"># http://scikit-learn.org/stable/auto_examples/manifold/plot_lle_digits.html#example-manifold-plot-lle-digits-py</span></div><div class="line"><span class="comment"># 我们之前已经讨论过手写数字的数据，每个手写的阿拉伯数字被表达为一个8*8的像素矩阵，</span></div><div class="line"><span class="comment"># 我们曾经使用每个像素点，也就是64个特征，使用logistic和knn的方法（分类器）去根据训练集判别测试集中的数字。</span></div><div class="line"><span class="comment"># 在这种做法中，我们使用了尚未被降维的数据。其实我们还可以使用降维后的数据来训练分类器。</span></div><div class="line"><span class="comment"># 现在，就让我们看一下对这个数据集采取各种方式降维的效果。</span></div><div class="line"><span class="keyword">from</span> matplotlib <span class="keyword">import</span> offsetbox</div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">plot_embedding</span><span class="params">(X, title=None)</span>:</span></div><div class="line">    x_min, x_max = np.min(X, <span class="number">0</span>), np.max(X, <span class="number">0</span>)</div><div class="line">    X = (X - x_min) / (x_max - x_min)</div><div class="line"></div><div class="line">    plt.figure(figsize=(<span class="number">10</span>, <span class="number">10</span>))</div><div class="line">    ax = plt.subplot(<span class="number">111</span>)</div><div class="line">    </div><div class="line">    <span class="comment"># 绘制每个样本这两个维度的值以及实际的数字</span></div><div class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(X.shape[<span class="number">0</span>]):</div><div class="line">        plt.text(X[i, <span class="number">0</span>], X[i, <span class="number">1</span>], str(digits.target[i]),</div><div class="line">                 color=plt.cm.Set1(y[i] / <span class="number">10.</span>),</div><div class="line">                 fontdict=&#123;<span class="string">'weight'</span>: <span class="string">'bold'</span>, <span class="string">'size'</span>: <span class="number">12</span>&#125;)</div><div class="line"></div><div class="line">    <span class="keyword">if</span> hasattr(offsetbox, <span class="string">'AnnotationBbox'</span>):</div><div class="line">        <span class="comment"># only print thumbnails with matplotlib &gt; 1.0</span></div><div class="line">        shown_images = np.array([[<span class="number">1.</span>, <span class="number">1.</span>]])  <span class="comment"># 定义一个标准点</span></div><div class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(digits.data.shape[<span class="number">0</span>]):<span class="comment">#样本数</span></div><div class="line">            dist = np.sum((X[i] - shown_images) ** <span class="number">2</span>,axis=<span class="number">1</span>)<span class="comment">#计算要展示的点和目前所有的点的距离，</span></div><div class="line">            <span class="comment">#axis=1代表横着加，即每个样本x^2+y^2; 得到该样本和所有的点的距离的数组;axis=0，按列加，就变成了把每个样本的x^2全加起来，y^2全部加起来。</span></div><div class="line">            <span class="keyword">if</span> np.min(dist) &lt; <span class="number">4e-3</span>: <span class="comment">#选择最近的距离</span></div><div class="line">                <span class="keyword">continue</span> <span class="comment"># don't show points that are too close</span></div><div class="line">            shown_images = np.r_[shown_images, [X[i]]] <span class="comment"># 纵向合并</span></div><div class="line">            imagebox = offsetbox.AnnotationBbox(</div><div class="line">                offsetbox.OffsetImage(digits.images[i], cmap=plt.cm.gray_r),</div><div class="line">                X[i])<span class="comment">#X[i]代表每个样本的两个维度的值，即横轴和纵轴的值，即两个维度决定的位置画出灰度图</span></div><div class="line">            ax.add_artist(imagebox)</div><div class="line">    plt.xticks([]), plt.yticks([])</div><div class="line">    <span class="keyword">if</span> title <span class="keyword">is</span> <span class="keyword">not</span> <span class="keyword">None</span>:</div><div class="line">        plt.title(title)</div></pre></td></tr></table></figure></p>
<h3 id="降维"><a href="#降维" class="headerlink" title="降维"></a>降维</h3><p><strong>随机降维</strong><br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line"><span class="comment">#降维——随机投影</span></div><div class="line"><span class="comment">#把64维数据随机地投影到二维上</span></div><div class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> (manifold, datasets, decomposition, ensemble,</div><div class="line">                     discriminant_analysis, random_projection)</div><div class="line">rp = random_projection.SparseRandomProjection(n_components=<span class="number">2</span>, random_state=<span class="number">42</span>)<span class="comment">#随机投影到两个维度</span></div><div class="line">stime = time.time()</div><div class="line">X_projected = rp.fit_transform(X)</div><div class="line">plot_embedding(X_projected, <span class="string">"Random Projection of the digits (time: %.3fs)"</span> % (time.time() - stime))</div></pre></td></tr></table></figure></p>
<p><img src="/picture/machine-learning/practice-advice22.png" alt="practice"></p>
<p><strong>PCA降维</strong><br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># PCA降维</span></div><div class="line"><span class="comment"># linear线性降维</span></div><div class="line"><span class="comment"># TruncatedSVD是pca的一种方式，不需要计算协方差矩阵，适用于稀疏矩阵</span></div><div class="line"><span class="comment"># PCA for dense data or TruncatedSVD for sparse data</span></div><div class="line"><span class="comment">#implemented using a TruncatedSVD which does not require constructing the covariance matrix</span></div><div class="line"><span class="comment"># LSA的基本思想就是，将document从稀疏的高维Vocabulary空间映射到一个低维的向量空间，我们称之为隐含语义空间(Latent Semantic Space).</span></div><div class="line">X_pca = decomposition.TruncatedSVD(n_components=<span class="number">2</span>).fit_transform(X)</div><div class="line">stime = time.time()</div><div class="line">plot_embedding(X_pca,<span class="string">"Principal Components projection of the digits (time: %.3fs)"</span> % (time.time() - stime))</div></pre></td></tr></table></figure></p>
<p><img src="/picture/machine-learning/practice-advice23.png" alt="practice"></p>
<p><strong>LDA线性变换</strong><br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line">print(<span class="string">"Computing Linear Discriminant Analysis projection"</span>)</div><div class="line">X2 = X.copy()</div><div class="line">X2.flat[::X.shape[<span class="number">1</span>] + <span class="number">1</span>] += <span class="number">0.01</span>  <span class="comment"># Make X invertible</span></div><div class="line">stime = time.time()</div><div class="line">X_lda = discriminant_analysis.LinearDiscriminantAnalysis(n_components=<span class="number">2</span>).fit_transform(X2, y)</div><div class="line">plot_embedding(X_lda,</div><div class="line">               <span class="string">"Linear Discriminant projection of the digits (time %.2fs)"</span> %</div><div class="line">               (time.time() - stime))</div></pre></td></tr></table></figure></p>
<p><img src="/picture/machine-learning/practice-advice24.png" alt="practice"></p>
<p><strong>t-SNE非线性变换</strong><br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line"><span class="comment">#http://scikit-learn.org/stable/modules/generated/sklearn.manifold.TSNE.html#sklearn.manifold.TSNE</span></div><div class="line"><span class="comment">#非线性的变换</span></div><div class="line"><span class="comment">#最小化KL距离，Kullback-Leibler </span></div><div class="line">tsne = manifold.TSNE(n_components=<span class="number">2</span>, init=<span class="string">'pca'</span>, random_state=<span class="number">0</span>)</div><div class="line">stime = time.time()</div><div class="line">X_tsne = tsne.fit_transform(X)</div><div class="line">plot_embedding(X_tsne,</div><div class="line">               <span class="string">"t-SNE embedding of the digits (time: %.3fs)"</span> % (time.time() - stime))</div></pre></td></tr></table></figure></p>
<p><img src="/picture/machine-learning/practice-advice25.png" alt="practice"><br>　　可以发现，在该数据集上，非线性变换的结果比线性变换的结果更理想。</p>
<h1 id="目标函数选择"><a href="#目标函数选择" class="headerlink" title="目标函数选择"></a>目标函数选择</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># adapted from http://scikit-learn.org/stable/auto_examples/linear_model/plot_sgd_loss_functions.html</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">modified_huber_loss</span><span class="params">(y_true, y_pred)</span>:</span></div><div class="line">    z = y_pred * y_true</div><div class="line">    loss = <span class="number">-4</span> * z</div><div class="line">    loss[z &gt;= <span class="number">-1</span>] = (<span class="number">1</span> - z[z &gt;= <span class="number">-1</span>]) ** <span class="number">2</span></div><div class="line">    loss[z &gt;= <span class="number">1.</span>] = <span class="number">0</span></div><div class="line">    <span class="keyword">return</span> loss</div><div class="line">xmin, xmax = <span class="number">-4</span>, <span class="number">4</span></div><div class="line">xx = np.linspace(xmin, xmax, <span class="number">100</span>)</div><div class="line">lw = <span class="number">2</span></div><div class="line">plt.plot([xmin, <span class="number">0</span>, <span class="number">0</span>, xmax], [<span class="number">1</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>], color=<span class="string">'gold'</span>, lw=lw,</div><div class="line">         label=<span class="string">"Zero-one loss"</span>)</div><div class="line">plt.plot(xx, np.where(xx &lt; <span class="number">1</span>, <span class="number">1</span> - xx, <span class="number">0</span>), color=<span class="string">'teal'</span>, lw=lw,</div><div class="line">         label=<span class="string">"Hinge loss"</span>)</div><div class="line">plt.plot(xx, -np.minimum(xx, <span class="number">0</span>), color=<span class="string">'yellowgreen'</span>, lw=lw,</div><div class="line">         label=<span class="string">"Perceptron loss"</span>)</div><div class="line">plt.plot(xx, np.log2(<span class="number">1</span> + np.exp(-xx)), color=<span class="string">'cornflowerblue'</span>, lw=lw,</div><div class="line">         label=<span class="string">"Log loss"</span>)</div><div class="line">plt.plot(xx, np.where(xx &lt; <span class="number">1</span>, <span class="number">1</span> - xx, <span class="number">0</span>) ** <span class="number">2</span>, color=<span class="string">'orange'</span>, lw=lw,</div><div class="line">         label=<span class="string">"Squared hinge loss"</span>)</div><div class="line">plt.plot(xx, np.exp(-xx), color=<span class="string">'red'</span>,lw=lw,linestyle=<span class="string">'--'</span>,</div><div class="line">         label=<span class="string">"Exponential loss"</span>)</div><div class="line">plt.plot(xx, modified_huber_loss(xx, <span class="number">1</span>), color=<span class="string">'darkorchid'</span>, lw=lw,</div><div class="line">         linestyle=<span class="string">'--'</span>, label=<span class="string">"Modified Huber loss"</span>)</div><div class="line">plt.ylim((<span class="number">0</span>, <span class="number">8</span>))</div><div class="line">plt.legend(loc=<span class="string">"upper right"</span>)</div><div class="line">plt.xlabel(<span class="string">r"Decision function $f(x)$"</span>)</div><div class="line">plt.ylabel(<span class="string">"$L(y, f(x))$"</span>)</div><div class="line">plt.show()</div></pre></td></tr></table></figure>
<p><img src="/picture/machine-learning/practice-advice26.png" alt="practice"><br>　　不同的代价函数有不同的优点：</p>
<ul>
<li>0-1 loss:在分类问题中使用。这是ERM用的代价函数，然而是非凸的，因此必须使用其他代价函数来近似替代。</li>
<li>hinge loss:在SVM中使用，体现最大间隔思想，不容易受离群点影响，有很好的鲁棒性，然后不能提供较好的概率解释。</li>
<li>log loss:在逻辑回归使用，能提供较好的概率解释，然而容易受离群点影响。</li>
<li>Exponential loss: 指数代价，在Boost中使用，容易受离群点影响，在AdaBoost中能够实现简单有效的算法。</li>
<li>perceptron loss:在感知机算法中使用。类似hinge loss，左移了一下。不同于hinge loss,percptron loss不对离超平面近的点进行惩罚。</li>
<li>squared hinge loss: 对hinge loss进行改进，平方损失。</li>
<li>modified huber loss: 对squared hinge loss进一步改进，是一种平滑损失，能够容忍离群点的影响。</li>
</ul>
<h1 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h1><p><a href="https://see.stanford.edu/materials/aimlcs229/ML-advice.pdf" target="_blank" rel="external">斯坦福机器学习：Advice for applying Machine Learning</a><br><a href="https://jmetzen.github.io/2015-01-29/ml_advice.html" target="_blank" rel="external">Advice for applying Machine Learning</a></p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;　　本文对&lt;a href=&quot;/2017/04/01/ml-advice/&quot;&gt;Advice for applying Machine Learning&lt;/a&gt;一文中提到的算法诊断等理论方法进行实践，使用Python工具，具体包括数据的可视化(data visualizing)、模型选择(choosing a machine learning method suitable for the problem at hand)、过拟合和欠拟合识别和处理(identifying and dealing with over and underfitting)、大数据集处理（dealing with large datasets）以及不同代价函数(pros and cons of different loss functions)优缺点等。&lt;br&gt;
    
    </summary>
    
      <category term="机器学习" scheme="xtf615.com/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="机器学习" scheme="xtf615.com/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="算法诊断" scheme="xtf615.com/tags/%E7%AE%97%E6%B3%95%E8%AF%8A%E6%96%AD/"/>
    
      <category term="偏差方差分析" scheme="xtf615.com/tags/%E5%81%8F%E5%B7%AE%E6%96%B9%E5%B7%AE%E5%88%86%E6%9E%90/"/>
    
      <category term="学习曲线" scheme="xtf615.com/tags/%E5%AD%A6%E4%B9%A0%E6%9B%B2%E7%BA%BF/"/>
    
      <category term="目标函数" scheme="xtf615.com/tags/%E7%9B%AE%E6%A0%87%E5%87%BD%E6%95%B0/"/>
    
  </entry>
  
  <entry>
    <title>Advice for applying Machine Learning</title>
    <link href="xtf615.com/2017/04/01/ml-advice/"/>
    <id>xtf615.com/2017/04/01/ml-advice/</id>
    <published>2017-04-01T09:24:05.000Z</published>
    <updated>2017-04-02T01:53:07.123Z</updated>
    
    <content type="html"><![CDATA[<p>　　本文对Ng提到的关于机器学习应用实践上的建议进行整理总结。主要内容包括：诊断(Diagnostics for debugging learning algorithms)、误差分析(error enalysis)、销蚀分析(ablative analysis)、过早优化(premature optimization)。并结合“自动驾驶直升机”应用探讨如何对存在问题进行诊断。<br><a id="more"></a></p>
<h1 id="ML算法诊断"><a href="#ML算法诊断" class="headerlink" title="ML算法诊断"></a>ML算法诊断</h1><p>　　考虑前面”生成算法”一文中讨论的垃圾邮件分类问题。前面我们使用50000+词语的词典作为特征向量集，本文将从中挑选100个词作为特征集。并使用”Learning Theory(2)”中提到的贝叶斯逻辑回归进行分类。代价函数如下，多了正则化项：<br>$$\max_{\theta} \sum_{i=1}^m log p(y^{(i)}|x^{(i)},\theta)-\lambda ||\theta||^2$$<br>　　使用梯度上升方法来进行极大似然估计，注意因为我们要最大化上述式子，故正则化项前为减号。(之前我们的目标是最小化，因为之前第一项对数似然取负号,故要加上正则化项,这里相当于对之前整个式子取负号,将最小化转成最大化,相应的变为减正则化项)。<br>　　目前研究状况是测试误差达到了20%, 这个结果显然不能接受，我们下一步要怎么做呢？<br>　　可能的方法包括：</p>
<ul>
<li><strong>尝试获取更多的训练数据</strong>(Try getting more training examples)</li>
<li><strong>尝试减少特征</strong>(Try a smaller set of features)</li>
<li><strong>尝试增加特征</strong>(Try a larger set of features)</li>
<li><strong>尝试选择更好特征</strong>(Try changing the features)</li>
<li><strong>尝试对梯度下降方法多迭代几次</strong>(Run gradient descent for more iterations)</li>
<li><strong>尝试使用牛顿法</strong>，牛顿法收敛更快(Try Newton’s method)</li>
<li><strong>修改正则化系数</strong>\(\lambda\)(Use a different value for \(\lambda\))</li>
<li><strong>尝试其他算法</strong>，如SVM算法(Try using an SVM)<br>　　可能的方法数以百计，我们只列出这么多。对于上述8种改进方法，如果一一尝试的话非常耗时。比如第一条，增加数据一般会达到比较好的效果，但对于一些应用来说，收集数据是很困难耗时的事情，如果你花了三个月的时间来收集数据，但最后发现增加数据并没有使算法效果编号，那就悲催了！<br>　　更好的解决方法是，使用诊断法来发现问题所在。<h2 id="偏差-方差分析"><a href="#偏差-方差分析" class="headerlink" title="偏差/方差分析"></a>偏差/方差分析</h2>　　第一个方法是判断问题是出在高方差还是高偏差。一般来说，高方差反映了过拟合问题，即训练误差很小但泛化误差却很大。而高偏差反映了模型本身存在问题或特征太少等问题，此时训练误差和泛化误差都很大。<h3 id="高方差诊断"><a href="#高方差诊断" class="headerlink" title="高方差诊断"></a>高方差诊断</h3><img src="/picture/machine-learning/advice1.jpg" alt="advice"><br>　　上图是高方差情况下，训练误差和泛化误差随样本数量变化的情况。该图有几个重要特征：</li>
<li><strong>泛化误差随样本m增大而降低</strong>。<br>  Test error still decreasing as m increases,并且不断逼近期望误差，意味着增加数据可以起到作用。</li>
<li><strong>训练误差和泛化误差之间差距很大</strong>。<br>  Large gap between training and test error，训练误差一直比期望误差理想，意味着可能存在过拟合。</li>
</ul>
<h3 id="高偏差诊断"><a href="#高偏差诊断" class="headerlink" title="高偏差诊断"></a>高偏差诊断</h3><p><img src="/picture/machine-learning/advice2.jpg" alt="advice"><br>　　上图是高偏差的情况下，训练误差和泛化误差随样本数量变化的情况。该图有几个重要特征：</p>
<ul>
<li><strong>训练误差很大</strong>。<br>  Even training error is unacceptably high, 比期望的误差还大。</li>
<li><strong>训练误差和泛化误差之间差距很小</strong>。<br>  Small gap between training and test error.</li>
</ul>
<p>　　二者最明显的区别就在于，<strong>训练误差和泛化误差间的差距大小</strong>以及<strong>训练误差和理想误差的差距大小</strong>。<br>　　前者体现在，<strong>\(m\)较大</strong>的时候，高方差对应的训练误差和泛化误差的差距较大，高偏差对应的训练误差和泛化误差的差距较小。<br>　　后者体现在，在<strong>\(m\)较小</strong>的时候，高方差的训练误差比理想误差还小,或者说维持在一个较优的水平；而高偏差的训练误差一般比理想误差大，维持在一个较差的水平。<br>　　上述图片也称作学习曲线。根据上面的分析，我们可以对前面提到的8条解决思路前4条进行分类。</p>
<ul>
<li>尝试获取更多的训练数据。———— <strong>解决高方差</strong></li>
<li>尝试减少特征。————<strong>解决高方差</strong></li>
<li>尝试增加特征。————<strong>解决高偏差</strong></li>
<li>尝试选择更好特征。———— <strong>解决高偏差</strong></li>
</ul>
<h2 id="收敛与目标函数分析"><a href="#收敛与目标函数分析" class="headerlink" title="收敛与目标函数分析"></a>收敛与目标函数分析</h2><p>　　考虑下面的情景，仍然是针对垃圾邮件分类问题：</p>
<ul>
<li>使用贝叶斯逻辑回归(BLR)可以达到垃圾邮件上的2%错误率，正常邮件上2%的错误率。<br>  正常邮件上的如此高的错误率是无法接受的。</li>
<li>使用SVM模型可以达到垃圾邮件10%的错误率，正常邮件0.01%的错误率。<br>  可以接受。</li>
<li>显然这个场景下SVM表现更好。但是因为计算效率上的考虑，你想使用逻辑回归，该如何解决？<br>　　<br>对于这种情况，有如下两种分析。</li>
<li><strong>算法是否收敛</strong>？(Is the algorithm(gradient descent for BLR) converging?)<br>　　算法收敛程度和训练优化算法以及迭代次数关系很大。训练优化算法包括梯度下降、牛顿法、SMO等</li>
<li><strong>目标优化函数是否合适</strong>？(Are you optimizing the right function?)<br>　　目标函数是否正确包括目标函数里的参数设置，例如BLR中正则化项系数\(\lambda\)，SVM对偶问题中的惩罚因子\(C\)。另外，不同模型使用的目标函数不同，例如SVM使用对偶问题目标函数，因此目标函数还和模型相关。<br>　　这两种情况都有可能造成上面的SVM优于BLR的问题。如果BLR没有收敛,SVM比BLR收敛的更好，所以SVM效果更好。如果模型的目标函数没有找对，也可能造成上述问题。<br>　　对于函数是否收敛来说，我们可以画出迭代次数与目标函数值的趋势图，但是这样的趋势图在通常情况下很难分辨出目标函数是否稳定的趋势，因为在训练的后期目标函数的每步优化往往都只提高一点。<br><img src="/picture/machine-learning/advice3.jpg" alt="advice"><br>　　上图很难看出是否收敛，目标函数似乎还在增长.(注意本文使用的是最大化目标函数)<br>　　这里介绍一种更直观的方法。<br>　　首先我们关注的是分类的准确率，并且对于正常邮件应当给予更高的权重。<br>$$a(\theta)=\max_{\theta} \sum_{i} w^{(i)} I\{h_\theta(x^{(i)})=y^{(i)}\}$$<br>　　\(w^{(i)}\)代表权重，在正常邮件中权值更大。<br>　　我们令\(\theta_{SVM}\)为SVM模型学习到的参数结果，\(\theta_{BLR}\)为Bayesian logistic regression学习到的参数结果。<br>　　显然根据我们前面的假设,SVM效果比BLR好，则:<br>$$a((\theta_{SVM}) &gt; a(\theta_{BLR})$$<br>　　再来观察目标函数值对比。<br>　　BLR尝试最大化如下目标函数:<br>$$J(\theta) = \sum_{i=1}^m log p(y^{(i)}|x^{(i)},\theta)-\lambda ||\theta||^2$$<br>　　SVM尝试最小化如下的原始目标函数为:<br>$$\min_{w,b} \frac{1}{2}||w||^2 + C\sum_{i=1}^m \zeta \\\\<br>使得， y^{(i)} (w^T  x^{(i)}-b) \geq 1- \zeta<br>$$<br>　　转成对偶问题后，变成最大化对偶问题目标函数：<br>$$\max_{\alpha} W(\alpha) = \sum_{i=1}^m \alpha_i - \frac{1}{2}\sum_{i,j=1}^m y^{(i)} y^{(j)} \alpha_i \alpha_j ＜x^{(i)},x^{(j)}＞ \\\ 使得， \alpha_i \geq 0,　　i=1,…,m \\\ \sum_{i=1}^m \alpha_i y^{(i)} = 0$$</li>
</ul>
<p>　　我们将比较\(J(\theta_{SVM})和J(\theta_{BLR})\)</p>
<ul>
<li>情况1<br>$$a(\theta_{SVM}) &gt; a(\theta_{BLR})且J(\theta_{SVM})&gt;J(\theta_{BLR})$$<br>我们注意到，BLR尝试最大化目标函数，SVM尝试最大化对偶目标函数。\(J(\theta_{SVM})&gt;J(\theta_{BLR})\)表明SVM收敛得比BLR好。因此问题可能出在优化算法，应该改进训练优化算法,例如使用牛顿法替换梯度下降，使之收敛更快。</li>
<li>情况2<br>$$a(\theta_{SVM}) &gt; a(\theta_{BLR})且J(\theta_{SVM}) \leq J(\theta_{BLR})$$<br>这种情况表明，BLR收敛没问题，SVM收敛性能比BLR更差，结果却更优。意味着目标函数\(J(\theta)\)存在问题，其不能真实反应人们在该问题上的需要(后面无人机诊断实例就能明白)，应该改进目标函数。要么进行调参，要么考虑换模型，也就间接地更改了目标函数。</li>
</ul>
<p>　　因此，我们可以对解决思路后四条进行分类。</p>
<ul>
<li>尝试对梯度下降方法多迭代几次。————<strong>解决优化算法(optimization algorithm)</strong></li>
<li>尝试使用牛顿法。———— <strong>解决优化算法(optimization algorithm)</strong></li>
<li>修改目标函数参数(\(\lambda,C\)等)。————<strong>解决目标函数(optimization objective)</strong></li>
<li>尝试其他算法(SVM算法),相当于修改目标函数。————<strong>解决目标函数(optimization objective)</strong></li>
</ul>
<h2 id="诊断实例"><a href="#诊断实例" class="headerlink" title="诊断实例"></a>诊断实例</h2><p>　　Ng和他的一些学生正在做一个自动驾驶直升机飞行的项目。本文以此为例子来分析Ng之前在遇到问题时是如何入手解决的。<br><img src="/picture/machine-learning/advice4.png" alt="advice"><br>　　首先对于一个能够自动驾驶直升机的程序来说，要经过如下步骤：</p>
<ul>
<li>建立一个精确的模拟器(Build a simulator of helicopter)</li>
<li>选择一个代价函数。(Choose a cost function)<br>$$eg:　　J(\theta)=||x-x_{desired}||^2$$</li>
<li>使用强化学习算法在模拟器中飞行来对代价函数进行最小化优化。(Run reinforcement learning algorithm to fly helicoper in simulation)<br>$$得到输出参数:\theta_{RL}= arg \min_{\theta}{J(\theta)}$$<br>　　假设已经做了如上步骤。但是得到的控制参数\(\theta_{RL}\)驾驶的实际效果比真人驾驶的效果差很多。此时该如何入手解决呢？<br>　　可能的措施包括：</li>
<li><strong>提升模拟器性能</strong>。(使其更符合真实环境)</li>
<li><strong>修改代价函数J</strong>(使最优化代价函数能够反映出飞行表现良好)</li>
<li><strong>修改RL算法</strong>(优化算法，使代价函数更好得收敛)<br>　　我们假设在理想情况下，即上述步骤都很完美得完成了。</li>
<li>直升机模拟器是精确的。(能够很好模拟真实环境)</li>
<li>最小化代价函数意味着能够正确的自动驾驶。(该代价函数能够反映实际需求:完美飞行)</li>
<li>RL算法在模拟器环境下正确得最小化了代价函数。(算法没问题：能求解问题)<br>　　如果上述条件都满足，那么毫无疑问，直升机会在实际环境中飞行得很好。<br>　　我们使用如下的诊断方法：</li>
<li>如果算法求出的控制参数\(\theta_{RL}\)在模拟器中飞行得很好，但是在实际环境中却表现很差。问题很可能出在模拟器上。比如，模拟器无法真实模拟出现实环境的气压、风向等因素。</li>
<li>让真实的人来驾驶直升飞机，得到参数\(\theta_{human}\)，如果\(J(\theta_{RL})&gt;J(\theta_{human})\),即人操作得到的代价函数更小，也就说明算法收敛不够，那么需要考虑改进优化算法，使算法更加收敛，可以考虑修改强化模型的优化算法或改成其他模型。</li>
<li>如果\(J(\theta_{RL})&lt;J(\theta_{human})\),则问题出在代价函数的设计上，代价函数已经很好得收敛了，结果却不好。即该代价函数最小化并不能代表飞行好。(Minimizing it doesn’t correspond to good autonomous flight)此时需要对代价函数重新设计。<br>　　因此我们需要为算法设计诊断方法来发现问题所在。即使你的算法表现良好，你也可以运用诊断方法来更加深入理解整个运行机制。这有助于理解问题的本质，有助于得到一个直观的感觉，什么样的改进能起作用，什么样的改进不起作用。同时运行诊断方法以及后面谈到的误差分析方法能够更好得对问题和观点进行阐述，从而写出更具有研究性的论文。<h1 id="误差分析"><a href="#误差分析" class="headerlink" title="误差分析"></a>误差分析</h1>　　误差分析用来判读误差的来源。<br>　　实际过程中，一个系统可能由多个部件组成，比如一个基于人脸的性别识别系统，可能由如下几个部件组成，按流程处理的先后顺序如下：<br><img src="/picture/machine-learning/advice5.jpg" alt="advice"><br>　　所谓的误差分析，做法就是对每个部件用基准值代替算法的输出，然后观察最后结果的变化，可能的误差分析结果如下：<br><img src="/picture/machine-learning/advice6.jpg" alt="advice"><br>　　如上图，系统整体的性能是85%准确率。我们使用一个已经对背景进行消除的照片进行建模(相当于拍了一张不带背景的照片，即不需要我们模型进行预处理)，此时得到的准确率是85.1%，显然只有0.1%的提升; 我们再在此基础上进行处理，固定其他因素不变，直接将人脸所在位置告诉模型(不需要模型进行人脸位置的检测),然后进行建模(注意此时是在第一步的基础上的，即用不带背景的照片进行人脸检测),发现性能为91%，相较于85.1%提升了5.9%。同理对于器官检测，我们可以告诉模型眼睛所在位置，得到95%的准确率，提升4%等等。如果最终将所有的处理都用基准值代替，即所有的步骤人工告知模型，那么最后Logistic得到的结果是100%.<br>　　经过这样的分析，我们可以得出，对于较多的部分是人脸识别和眼睛识别。当然，器官识别中不同的基准值代替顺序可能会有不同的分析结果，所以可以调整顺序不断进行比较，找到问题的瓶颈所在。<h2 id="销蚀分析"><a href="#销蚀分析" class="headerlink" title="销蚀分析"></a>销蚀分析</h2>　　销蚀分析与误差分析不同，误差分析师一步步用基准值代替算法输出，比较的是系统当前性能与最高性能的差别。而销蚀分析考虑的是系统性能和底线性能(baseline)的差异。<br>　　比如，对于拉简邮件分类器来说，先构建一个初始分类器，然后考虑一些比较高级的特征，比如邮件的语法风格，主机信息，标题等。先将所有特征全部加入分类器，然后逐个剔除，观察性能的下降幅度，将那些性能下降少或者反而导致性能提升的特征去除。<br><img src="/picture/machine-learning/advice7.jpg" alt="advice"><br>　　如上图，显然Email text parser features最重要。</li>
</ul>
<h1 id="如何开始ML问题的解决"><a href="#如何开始ML问题的解决" class="headerlink" title="如何开始ML问题的解决"></a>如何开始ML问题的解决</h1><h2 id="途径1：精心设计法-Careful-design"><a href="#途径1：精心设计法-Careful-design" class="headerlink" title="途径1：精心设计法(Careful design)"></a>途径1：精心设计法(Careful design)</h2><p>　　对问题进行深入分析，提取正确的特征，收集正确的数据，设计正确的算法架构，最终实现它。好处在于能一次性能到可扩展的算法，甚至可能会创造一些新的算法。有点类似瀑布流开发模式。</p>
<h2 id="途径2：构建修改法-Build-and-fix"><a href="#途径2：构建修改法-Build-and-fix" class="headerlink" title="途径2：构建修改法(Build-and-fix)"></a>途径2：构建修改法(Build-and-fix)</h2><p>　　使用一些粗糙但是快速的方法进行初步构建。然后使用诊断法和误差分析法来分析问题所在，并修正误差。好处在于可以较快的构建应用，快速占领市场。因为互联网上的成功产品，很多往往不是做的最好的，而是最早占领市场的。</p>
<h2 id="过早优化-Premature-statistical-optimiztion"><a href="#过早优化-Premature-statistical-optimiztion" class="headerlink" title="过早优化(Premature statistical optimiztion)"></a>过早优化(Premature statistical optimiztion)</h2><p>　　通常情况下，是很难发现系统哪个部分是比较容易或者比较难构建的，也很难发现系统哪部分需要花更多时间进行开发。也因此上述途径1的精心设计方法很容易陷入过早优化问题，即对系统的某个部分过早的进行精心设计，却不知这部分设计是否是真的对系统性能有很大影响。<br><img src="/picture/machine-learning/advice8.jpg" alt="advice"><br>　　The only way to find out what needs work is to implement something quickly,and find out what parts break。即通过快速入手一些工作来发现哪部分是需要多花时间，哪部分不需要多花时间。一个较好的建议是先对数据进行分析，比如为什么数据中这些属性是负的，当找出数据中的规律或者数据中的错误时，往往会发现系统性能差不是需要更复杂的算法，而是更强大的预处理。</p>
<h2 id="过度理论-over-theorizing"><a href="#过度理论-over-theorizing" class="headerlink" title="过度理论(over-theorizing)"></a>过度理论(over-theorizing)</h2><p>　　在做研究时，要把注意力集中在关键问题上，不要轻易的相信某些理论对算法有用而花大量的时间在那些理论上。比如如果要检测出图片中的物体，可能有人会说三维微分几何对这个问题有用，但是在你确认这个确实有用前，不要浪费精力在这个上面。<br><img src="/picture/machine-learning/advice9.jpg" alt="advice"></p>
<h1 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h1><ul>
<li>花在设计诊断方法上的时间是值得的。</li>
<li>需要根据自己的直接和经验来来设计正确的诊断方法</li>
<li>误差分析和销蚀分析用于可以提供对问题的深入理解</li>
<li>两种途径入手机器学习问题：精心设计法(容易陷入过早优化)、构建修改法。</li>
</ul>
<h1 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h1><p><a href="http://open.163.com/special/opencourse/machinelearning.html" target="_blank" rel="external">斯坦福大学机器学习视频教程</a></p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;　　本文对Ng提到的关于机器学习应用实践上的建议进行整理总结。主要内容包括：诊断(Diagnostics for debugging learning algorithms)、误差分析(error enalysis)、销蚀分析(ablative analysis)、过早优化(premature optimization)。并结合“自动驾驶直升机”应用探讨如何对存在问题进行诊断。&lt;br&gt;
    
    </summary>
    
      <category term="机器学习" scheme="xtf615.com/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="机器学习" scheme="xtf615.com/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="算法诊断" scheme="xtf615.com/tags/%E7%AE%97%E6%B3%95%E8%AF%8A%E6%96%AD/"/>
    
      <category term="偏差方差分析" scheme="xtf615.com/tags/%E5%81%8F%E5%B7%AE%E6%96%B9%E5%B7%AE%E5%88%86%E6%9E%90/"/>
    
      <category term="学习曲线" scheme="xtf615.com/tags/%E5%AD%A6%E4%B9%A0%E6%9B%B2%E7%BA%BF/"/>
    
      <category term="误差分析" scheme="xtf615.com/tags/%E8%AF%AF%E5%B7%AE%E5%88%86%E6%9E%90/"/>
    
      <category term="销蚀分析" scheme="xtf615.com/tags/%E9%94%80%E8%9A%80%E5%88%86%E6%9E%90/"/>
    
  </entry>
  
  <entry>
    <title>Learning Theory</title>
    <link href="xtf615.com/2017/03/29/Learning-Theory/"/>
    <id>xtf615.com/2017/03/29/Learning-Theory/</id>
    <published>2017-03-29T04:59:14.000Z</published>
    <updated>2017-03-30T02:11:25.295Z</updated>
    
    <content type="html"><![CDATA[<p>　　本文与前面不同，主要内容不是算法，而是机器学习的另一部分内容——学习理论。主要包括偏差/方差(Bias/Variance)、经验风险最小化(Empirical Risk Minimization,ERM)、联合界(Union bound)、一致收敛(Uniform Convergence)。</p>
<h1 id="偏差-方差权衡"><a href="#偏差-方差权衡" class="headerlink" title="偏差/方差权衡"></a>偏差/方差权衡</h1><p>　　回顾一下，当我们讨论线性回归时，我们尝试过使用简单的线性方法，如\(y=\theta_0 + \theta_1 x\),也尝试使用更复杂的模型，如多项式\(y=\theta_0+\theta_1 x+…+\theta_5 x^5\)。观察下图：<br><img src="/picture/machine-learning/theory1.jpg" alt="theory"><br>　　以上述回归问题为例，机器学习的目标是从训练集中得到一个模型，使之能对测试集进行分类。这里训练集和测试集都是分布\(\mathcal{D}\)的样本。机器学习的关注点在于模型在测试集上的分类效果，这也称作泛化能力(generalization ability)。如上图，最左边的图用一个线性模型进行拟合，显然即使拥有很多的训练集，该模型在测试集上进行预测的话，仍然存在着很大的误差，这种情况称为欠拟合，对应着高偏差。对于最右边的图，用一个高阶(五阶)去拟合，从数据中得到的模型结构很可能碰巧是该训练集特有的，即尽管五次多项式模型对训练集的拟合不错，但并非是一个好的模型，因为对于训练集以外的数据，该模型不一定能很好得进行预测，即泛化能力不够好，因此仍然存在着很大的泛化误差，这种情况称作过拟合，对应高方差。<br>　　在机器学习中，对偏差和方差的权衡是学习理论重点解决的问题。如果我们的模型太过于简单，只有少量的参数要学习，那么就可能存在高偏差(large bias but small variance)。如果我们的模型太过于复杂，拥有大量的参数，那么就可能存在高方差(large variance but small bias)。在上面的例子中，训练一个二次型的模型(对应中间那幅图)，比训练一个过于简单的一次型模型或过于复杂的五次型模型都好。　　<br><a id="more"></a></p>
<h1 id="经验风险最小化"><a href="#经验风险最小化" class="headerlink" title="经验风险最小化"></a>经验风险最小化</h1><p>　　在这部分我们开始对学习理论进行研究。这部分的研究可以磨练我们的直觉，并学到在不同情况下，更好得应用学习算法的经验法则。我们也会尝试回答一些问题，例如是否可以将偏差/方差的权衡进行形式化表述?这有助于后面对于特征选择方法的学习，也有助于为训练数据选择合适的多项式拟合阶数。其次，在机器学习中，我们对泛化误差格外关注，然而我们的模型是对训练集进行拟合的，训练集拟合表现对泛化误差有什么影响？是否可以将训练集的误差和泛化误差联系在一起。最后一个问题，证明学习算法表现良好是否需要一些条件作为基础？</p>
<h2 id="引理"><a href="#引理" class="headerlink" title="引理"></a>引理</h2><p>　　首先讨论两条引理。<br>　　<strong>联合界定理(The union bound)</strong>:令\(A_1,A_2,…,A_k\)是k个事件，这k个时间可以相互独立也可以不相互独立，我们有:<br>$$P(A_1 \cup A_2 … \cup A_k) \leq P(A_1)+P(A_2)+…+P(A_k)$$<br>　　在概率论中，上述是一个公理。任意事件发生的概率显然小于所有事件各自发生的概率之和。该定理可以使用文氏图(韦恩图)进行非正式的证明。<br>　　<strong>Hoeffding不等式</strong>：令\(Z_1,Z_2,…,Z_m\)为m个独立同分布变量(\(i.i.d\)),它们都服从伯努利分布，即\(P(Z_i=1)=\phi,P(Z_i=0)=1-\phi\),我们使用m个服从\(i.i.d的Z_i\)变量的平均值来估计\(\phi\)，得到:<br>$$\hat{\phi}=\frac{1}{m}\sum_{i=1}^m Z_i$$<br>　　那么Hoeffding不等式的定义即，对于任意的固定数值\(\gamma&gt;0\)，满足：<br>$$P(|\phi-\hat{\phi}|&gt;\gamma) \leq 2exp(-2\gamma^2 m)$$<br>　　这个引理是指,如果我们使用m个满足伯努利分布的随机变量的平均值\(\hat{\phi}\)来估计\(\phi\),那么随着样本数目m的增大，我们对参数的估计\(\hat{\phi}\)会越来越接近真实的\(\phi\)值。上式的概率实际上代表犯错误的概率，即犯错误概率会随样本数目的增大而减小。<br>　　运用上述两条引理，我们可以得出一些重要的学习理论结论。</p>
<h2 id="经验风险"><a href="#经验风险" class="headerlink" title="经验风险"></a>经验风险</h2><p>　　为了简化我们的讨论，我们将注意力集中在二分类问题上，即\(y \in \{0,1\}\)。所有关于二分类问题的讨论同样适用于其它的回归问题或者多分类问题。<br>　　假设给定一个训练集\(S=\{(x^{(i)},y^{(i)});i=1,…,m\}\),样本数量为m，每一个样本都满足独立同分布，且服从分布\(D\),即假设每个样本都是通过该分布生成的。对于一个特定的假设h,我们定义训练误差training error（在学习理论中称作经验风险或经验误差）为：<br>$$\hat{\epsilon}(h)=\frac{1}{m} \sum_{i=1}^m I\{h(x^{(i)}) \not= y^{(i)}\}$$<br>　　\(\hat{\epsilon}(h)\)代表对于<strong>特定</strong>训练集S得到的<strong>估计值</strong>，也可以写作\(\hat{\epsilon}_S(h)\)。我们定义泛化误差generalization error为：<br>$$\epsilon(h)=P(x,y) \sim \mathcal{D}(h(x) \not= y)$$<br>　　泛化误差是针对满足分布的新样本而言，指的是对于满足分布\(\mathcal{D}\)的新样本(x,y),会被h误分类的概率。注意，我们假设训练数据是根据分布\(\mathcal{D}\)得到的，同时，\(\mathcal{D}\)也是测试数据的分布，也就是上式泛化误差中的\(\mathcal{D}\)。这也是PAC假设之一，PAC全称probably approximately correct,是学习理论得以证明所依赖的一个框架和一系列假设，训练集和测试集同分布是其中最重要的一个假设。<br>　　考虑一个线性分类问题，令\(h_\theta(x)=I\{\theta^T x \geq 0\}\),即当\(\theta^T x \geq 0\)时，分类结果为1。一种拟合参数\(\theta\)的方法是最小化训练误差:<br>$$\hat{\theta}=\mathop{argmin}_\limits{\theta} \hat{\epsilon}(h_\theta)$$<br>　　我们称这个过程为经验风险最小化(empirical risk minimization,ERM),根据该学习算法，可以得到假设h的估计，即\(\hat{h}=h_{\hat{\theta}}\)。我们认为ERM是最基本的学习算法，其他算法如逻辑回归等也可以被近似当作ERM。<br>　　在研究学习理论中，我们暂且不考虑假设具体的参数和具体使用的分类器是什么。我们定义一个学习算法的假设空间\(\mathcal{H}\)由所有的决策函数或模型的集合构成。例如对于线性分类问题，有：\(\mathcal{H}=\{h_\theta:h_\theta(x)=I\{\theta^Tx \geq 0\},\theta \in {\mathbb{R}}^{n+1}\}\)。拓展来说，如果我们在学习神经网络，那么\(\mathcal{H}\)就代表一系列可以代表神经网络的决策函数组成。<br>　　在这里，ERM可以认为是在假设空间\(\mathcal{H}\)中寻找使得训练误差最小化的某个假设h.即：<br>$$\hat{h}=\mathop{argmin}_\limits{h \in H} \hat{\epsilon}(h)$$</p>
<h2 id="一致收敛"><a href="#一致收敛" class="headerlink" title="一致收敛"></a>一致收敛</h2><p>　　我们首先考虑有限的假设空间\(\mathcal{H}=\{h_1,…,h_k\}\),假设空间由k个假设组成。因此\(\mathcal{H}\)只是k个由映射函数组成，该映射函数负责从输入空间\(\mathcal{X}\)映射到\(\{0,1\}\)。经验风险最小化就是从这k个函数中选择使得训练误差最小的假设\(\hat{h}\)。<br>　　我们关注的是\(\hat{h}\)在泛化误差上表现,即\(\epsilon(\hat{h})\)。我们的策略包含两个步骤，首先证明，对所有的h，\(\hat{\epsilon}(h)\)都是\(\epsilon(h)\)的可靠估计。接着证明\(\hat{h}\)的泛化误差\(\epsilon(\hat{h})\)存在着一个上界。<br>　　先证明第一个，对所有的h，\(\hat{\epsilon}(h)\)都是\(\epsilon(h)\)的可靠估计。<br>　　从假设空间选取某个特定的\(h_i \in \mathcal{H}\)。考虑伯努利随机变量Z，我们根据分布\(\mathcal{D}\)生成新样本(x,y),即\((x,y) \sim \mathcal{D}\)，再令\(Z=I\{h_i(x) \not= y\}\)，即判断\(h_i\)能否正确分类新样本。同样我们定义\(Z_j=I\{h_i(x^{(j)}) \not= y^{(j)}\}\)，即判断训练集中的样本是否被正确分类。因为训练样本也是根据分布\(\mathcal{D}\)获得的，则Z和\(Z_j\)属于同分布。<br>　　训练误差重写为：<br>$$\hat{\epsilon}(h_i)=\frac{1}{m}\sum_{j=1}^m Z_j$$<br>　　\(\hat{\epsilon}(h_i)\)实际上就是m个服从伯努利分布的随机变量\(Z_j\)的平均值。而\(\epsilon(h_i)\)代表伯努利分布Z的均值。根据Hoeffding不等式，有：<br>$$P(|\epsilon(h_i)-\hat{\epsilon}(h_i)|&gt;\gamma) \leq 2exp(-2\gamma^2 m)$$<br>　　这意味着，对于给定的\(h_i\),当m非常大时，泛化误差和训练误差接近的概率很大，即训练误差\(\hat{\epsilon}(h_i)\)能够很好的估计泛化误差\(\epsilon(h_i)\)。更进一步，我们想证明对于任意的\(h \in \mathcal{H}\),都存在上述结果。为了证明这个结论，我们令\(A_i\)代表\(|\epsilon(h_i)-\hat{\epsilon}(h_i)|&gt;\gamma\)，对于任意给定的\(A_i\),都有：\(P(A_i) \leq 2exp(-2\gamma^2 m)\)。因此使用联合界定义，有：<br>$$P(\exists h \in \mathcal{H}.|\epsilon(h_i)-\hat{\epsilon}(h_i)|&gt;\gamma) = P(A_1 \cup A_2 … \cup A_k) \leq \sum_{i=1}^k P(A_i) \\\\<br>\leq \sum_{i=1}^k 2exp(-2\gamma^2 m) \\\\<br>= 2kexp(-2\gamma^2 m)<br>$$<br>　　同时使用1减去不等式两边有，<br>$$P(\nexists h \in \mathcal{H}.|\epsilon(h_i)-\hat{\epsilon}(h_i)|&gt;\gamma)=P(\forall h \in \mathcal{H}.|\epsilon(h_i)-\hat{\epsilon}(h_i)| \leq \gamma) \geq 1-2kexp(-2\gamma^2m)$$<br>　　上式代表，至少以概率\(1-2kexp(-2\gamma^2m)\)保证,对所有的\(h \in \mathcal{H}\)，都有泛化误差和训练误差的差值不大于\(\gamma\).称作一致收敛定理。<br>　　在一致收敛中，有三个参数,\(m,\gamma\),误差的概率(或称作犯错误的概率),这三个参数是互相关联的，我们可以固定其中两个，来推出第三个，其中固定\(m,\gamma\)求概率上式已经得出。下面依次对另外两种参数关联进行说明。<br>　　第一个，对于给定\(\gamma,\delta &gt; 0\)，需要多少样本，才可以保证至少有\(1-\delta\)的概率，使得泛化误差和训练误差相差在\(\gamma\)的范围内.令\(1-2kexp(-2\gamma^2m) \geq 1-\delta \)可以求出m:<br>$$m \geq \frac{1}{2\gamma^2}log\frac{2k}{\delta}$$<br>　　即当m满足上式时，对任意的\(h \in \mathcal{H}\),都至少有\(1-\delta\)的概率保证\(|\epsilon(h_i)-\hat{\epsilon}(h_i)| \leq \gamma\)，也即犯错误(\(|\epsilon(h_i)-\hat{\epsilon}(h_i)| &gt; \gamma\))的概率至多为\(\delta\)<br>　　这个推论的意义在于，一个模型或算法要达到一个确定的性能时，需要的样本数目为多少。这也称作算法的样本复杂度。上式也表明，达到好的性能，对k的限制只需要对数级别就行，k是假设空间的大小。这个很关键。<br>　　同样的，我们也可以固定m和\(\delta&gt;0\),泛化误差会落在训练误差的什么范围内呢？<br>　　$$|\hat{\epsilon}(h)-\epsilon(h)| \leq \sqrt{\frac{1}{2m} log \frac{2k}{\delta}}$$<br>　　接着证明\(\hat{h}\)的泛化误差\(\epsilon(\hat{h})\)存在着一个上界。也就是考察在一致收敛成立的情况下，我们通过ERM方法得到的假设\(\hat{h}\)的泛化能力到底如何?<br>　　首先定义：<br>$$\hat{h}=\mathop{argmin}_\limits{h \in \mathcal{H}} \hat{\epsilon}(h)$$<br>$$h^{*}=\mathop{argmin}_\limits{h \in \mathcal{H}} \epsilon(h)$$<br>　　\(\hat{h}\)可以理解为在假设空间中，寻找使得训练误差最小的那个假设。\(h^{*}\)可以理解为在假设空间\(\mathcal{H}\)中，寻找使得泛化误差最小的假设。\(h^{*}\)是我们能找到的最好的假设。我们可以将使得训练误差最小的那个假设\(\hat{h}\)和其进行对比,\(\epsilon(\hat{h})\)可以表示训练集上选择使训练误差最小的假设\(\hat{h}\)在泛化误差上的表现。<br>$$\epsilon(\hat{h}) \leq \hat{\epsilon}(\hat{h})+\gamma \\\\<br>\leq \hat{\epsilon}(h^{*}) + \gamma \\\\<br>\leq \epsilon(h^{*}) + 2\gamma<br>$$<br>　　首先是第一个不等式，\(\epsilon(\hat{h})\)是指对于某个特定的假设\(\hat{h}\)的泛化误差,\( \hat{\epsilon}(\hat{h})\)代表对于某个特定的假设\(\hat{h}\)的训练误差。根据一致收敛定理, 在\(1-\delta\)的概率下能保证泛化误差和训练误差相差在\(\gamma\)的范围内时, 即：<br>$$|\epsilon(h_i)-\hat{\epsilon}(h_i)| \leq \gamma$$<br>　　展开不等式：<br>$$-\gamma \leq \epsilon(h_i)-\hat{\epsilon}(h_i) \leq \gamma$$<br>　　根据右半边部分可以得到,\(\epsilon(h_i)\leq \hat{\epsilon}(h_i)+\gamma\),进而推出第一个不等式,这里的\(\hat{h}\)相当于上面的\(h_i\),因为\(h_i\)可以是任意属于假设空间的假设。<br>　　第二个不等式是因为根据上述定义的\(\hat{h}=\mathop{argmin}_\limits{h \in \mathcal{H}} \hat{\epsilon}(h)\),即在训练误差表现上，\(\hat{h}\)是表现最好的，没有任何其他的假设在训练误差上表现会更好，因此\(\hat{\epsilon}(\hat{h}) \leq \hat{\epsilon}(h^{*})\)。<br>　　最后一个不等式，仍然是根据一致收敛定理，根据上述绝对值不等式展开后的左半边部分，有:<br>$$\hat{\epsilon}(h_i) \leq \epsilon(h_i) + \gamma$$<br>　　使用\(h^{*}\)替换\(h_i\),则有：<br>$$\hat{\epsilon}(h^{*}) \leq \epsilon(h^{*}) + \gamma$$<br>　　进而可以推出第三个不等式。<br>　　下面将这些推论综合一下，我们得到一个定理：<br>　　令\(|\mathcal{H}|=k\),给定m和\(\delta&gt;0\),那么至少有\(1-\delta\)的概率能否使下面公式成立:<br>$$\epsilon(\hat{h}) \leq \min_{h \in \mathcal{H}} \epsilon(h) + 2\sqrt{\frac{1}{2m} log \frac{2k}{\delta}}$$<br>　　该定理反映了在训练集上选择使训练误差最小的假设\(\hat{h}\)在泛化误差上的表现(\(\epsilon(\hat{h})\))受模型精确度和复杂度的影响，也就是反映了偏差和方差之间的权衡。可以想象，当选择一个复杂的模型假设时,\(|\mathcal{H}|=k\)会变大 ，导致不等式后的第二项变大，意味着方差变大；但是第一项也会相应的变小，因为使用一个更大的假设空间\(\mathcal{H}\)意味着可供选择的假设变多了，在多的那部分假设中可能存在使误差更小的假设，因此偏差就会变小。故应该选择一个最优值，使得偏差和方差之和最小，才能得到一个好的模型。<br>　　同样，该定理还有另外形式的推论：<br>　　令\(|\mathcal{H}|=k\)，给定\(\gamma,\delta&gt;0\)，那么至少有\(1-\delta\)概率使得\(\epsilon(\hat{h}) \leq \min_\limits{h \in \mathcal{H}} \epsilon(h) + 2\gamma\)成立的前提是：<br>$$m \geq \frac{1}{2\gamma^2} log \frac{2k}{\delta}=O(\frac{1}{\gamma^2}log\frac{k}{\delta})$$</p>
<h1 id="推广无限假设空间"><a href="#推广无限假设空间" class="headerlink" title="推广无限假设空间"></a>推广无限假设空间</h1><p>　　上述讨论的是有限假设空间的情况。我们将其拓展到无限空间。</p>
<h2 id="直观理解"><a href="#直观理解" class="headerlink" title="直观理解"></a>直观理解</h2><p>　　先从一个简单的例子说起。假设我们的假设空间\(\mathcal{H}\)由d个实数参数表示。例如对于逻辑回归，如果有n个特征的话,就有d=n+1(多的一个是截距)个参数。因为计算机通常使用双精度64bit来表示double型数据。因此对于每个参数都有\(2^64\)种不同可能的取值，那么d个参数的话，不同排列组合有，\(2^{64} \times 2^{64} \times …\times 2^{64}\)种，共d个式子，即\(k=2^{64d}\)种不同的假设。为了满足在\(1-\sigma\)概率下，有\(\epsilon(\hat{h}) \leq \epsilon(h^{*})+2\gamma\),则：<br>$$m \geq \frac{1}{2\gamma^2} log \frac{2k}{\delta}=O(\frac{1}{\gamma^2}log\frac{2^{64}}{\delta})=O(\frac{d}{\gamma^2} log \frac{1}{\gamma})=O_{\gamma,\delta}(d)$$<br>　　最后一个O代表对于给定的\(\gamma,\delta\)，即认为为常数的时候，所需要的样本个数应该和参数个数呈线性关系。<br>　　上述证明不够严格，下面进行更正式的表述。(具体证明不给出)</p>
<h2 id="VC维"><a href="#VC维" class="headerlink" title="VC维"></a>VC维</h2><p>　　首先引入VC(Vapnik-Chervonenkis)维的概念。VC维是为了研究学习过程一致收敛的速度和推广性，由统计学理论定义的有关函数集学习性能的一个重要指标。<br>　　具体定义，对一个指示函数集，如果存在H个样本能够被函数集中的函数按所有可能的\(2^H\)种形式分开，则称函数集能够把H个样本打散(分开)；函数集的VC维就是它能打散的最大样本数目H。若对任意数目的样本都有函数能将它们打散,则函数集的VC维是无穷大，它可以将任意多样本的任意标注情况精确分开，即在训练集上达到100%的分类正确率。<br>　　我们可以看一下2维空间，对于3个样本而言：<br><img src="/picture/machine-learning/theory2.jpg" alt="theory"><br>　　共用8种可能(\(2^3\),任意一个样本可以在直线任意一侧)。可以发现都可以很好的分开，只要存在一种放置该8种可能的方式即可(例如如果选择同一直线的方式就不能打散，但是存在其他种不再同一直线上的8种方式可以进行打散)。我们再观察4个样本，都不能同时对\(2^4=16\)种标注进行打散，即不是所有的标注形式，都能找到一个假设来分散，因此VC维在二维线性分类器上等于3。推广至<br>维线性分类器上，VC维d=n+1。<br>　　实际上对于无限假设空间，我们令该假设空间的VC维为d，即\(d=VC(\mathcal{H})\)。我们可以得到如下结论，在\(1-\delta\)概率下,对于所有的\(h \in \mathcal{H}\),都有：<br>$$|\epsilon(h)-\hat{\epsilon}(h)| \leq O\left(\sqrt{\frac{d}{m} log \frac{m}{d} + \frac{1}{m} log \frac{1}{\delta}}\right)$$<br>　　同样，我们可以推出，在\(1-\delta\)概率下：<br>$$\epsilon(\hat{h}) \leq \epsilon(h^{*}) + O\left(\sqrt{\frac{d}{m} log \frac{m}{d} + \frac{1}{m} log \frac{1}{\delta}}\right)$$<br>　　换句话说，如果某个假设空间有有限的VC维，那么当m变大的时候，一致收敛会发生。<br>　　进而得出m大小的推论，对于给定的\(\gamma,\delta&gt;0\),在\(1-\delta\)概率下保证\(\epsilon(\hat{h}) \leq \epsilon(h^{*})+2\gamma\)，则:<br>$$m=O_{\gamma,\delta}(d)$$<br>　　最后得出结论，对于一个目标是使训练误差最小的算法而言，需要的样本数量和假设空间的VC维大小呈线性关系，实际上VC维可以理解为假设空间参数的数目,即对大多数模型而言，VC维和模型参数的数目呈正比关系。</p>
<h2 id="拓展：VC维解释SVM"><a href="#拓展：VC维解释SVM" class="headerlink" title="拓展：VC维解释SVM"></a>拓展：VC维解释SVM</h2><p>　　根据上一篇文章我们知道，SVM通过核函数将数据映射到高维空间，模型复杂度增加，那么相应的，其VC维应该变大，要达到较好的效果所需的数据应该增大才对。但SVM只在原数据上就达到了比其他模型更优的结果，为什么呢？<br>　　虽然SVM将数据映射到高维空间，但是其仍然有最大间隔分类器的假设，而对于最大间隔分类器来说，<strong>其VC维并不依赖X的维度</strong>.对于最小间隔为\(\gamma\)的分类器而言，令\(||x^{(i)}||_2 \leq R\),即采样点在半径为R的圆内。那么:<br>$$VC(H) \leq \left \lceil \frac{R^2}{4\gamma^2} \right\rceil + 1$$<br>　　SVM算法会自动寻找一个具有较小VC维的假设，这样反而降低了VC维，使得原来的数据量就相对足够充分(\(m \propto O(d)\))，因此不影响模型的效果。</p>
<h1 id="ERM的直观意义"><a href="#ERM的直观意义" class="headerlink" title="ERM的直观意义"></a>ERM的直观意义</h1><p>　　ERM即经验风险最小化，有公式：<br>$$\hat{\epsilon}(h)=\frac{1}{m} \sum_{i=1}^m I\{h(x^{(i)}) \not= y^{(i)}\}$$<br>　　我们以单个样本为例，其误差函数为\(I\{h(x^{(i)}) \not= y^{(i)}\}\),很显然，这是一个非凸函数，使用机器学习的方法并不能很好的对其进行优化。因而产生了一些算法对该误差函数进行凸性近似，以期能够更好的优化，以svm和logistic为例，如图2所示：<br><img src="/picture/machine-learning/theory3.jpg" alt="theory"><br>　　如上图，logistic模型采用极大似然估计方法，它尝试令负的对数似然最小，即，\(-log P(y^{(i)}|x^{(i)};\theta)\),因而如图2中的曲线所示。SVM使用的是大间隔概念，即不仅仅考虑\(\theta^T x\)大于0,更严格的跟1或者-1比较。可以根据下图理解：<br><img src="/picture/machine-learning/theory4.jpg" alt="theory"><br>　　上图可以令负样本的\(y^{(i)}=-1\),这样就可以利用函数间隔概念统一化。<br>　　因此虽然logistic和svm都不是直接的ERM算法，但基于对ERM的近似而产生，因而可见，ERM的一致性定理在实际中的威力。</p>
<h1 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h1><p><a href="http://open.163.com/special/opencourse/machinelearning.html" target="_blank" rel="external">斯坦福大学机器学习视频教程</a><br><a href="http://www.flickering.cn/machine_learning/2015/04/vc%E7%BB%B4%E7%9A%84%E6%9D%A5%E9%BE%99%E5%8E%BB%E8%84%89/" target="_blank" rel="external">VC维的来龙去脉</a></p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;　　本文与前面不同，主要内容不是算法，而是机器学习的另一部分内容——学习理论。主要包括偏差/方差(Bias/Variance)、经验风险最小化(Empirical Risk Minimization,ERM)、联合界(Union bound)、一致收敛(Uniform Convergence)。&lt;/p&gt;
&lt;h1 id=&quot;偏差-方差权衡&quot;&gt;&lt;a href=&quot;#偏差-方差权衡&quot; class=&quot;headerlink&quot; title=&quot;偏差/方差权衡&quot;&gt;&lt;/a&gt;偏差/方差权衡&lt;/h1&gt;&lt;p&gt;　　回顾一下，当我们讨论线性回归时，我们尝试过使用简单的线性方法，如\(y=\theta_0 + \theta_1 x\),也尝试使用更复杂的模型，如多项式\(y=\theta_0+\theta_1 x+…+\theta_5 x^5\)。观察下图：&lt;br&gt;&lt;img src=&quot;/picture/machine-learning/theory1.jpg&quot; alt=&quot;theory&quot;&gt;&lt;br&gt;　　以上述回归问题为例，机器学习的目标是从训练集中得到一个模型，使之能对测试集进行分类。这里训练集和测试集都是分布\(\mathcal{D}\)的样本。机器学习的关注点在于模型在测试集上的分类效果，这也称作泛化能力(generalization ability)。如上图，最左边的图用一个线性模型进行拟合，显然即使拥有很多的训练集，该模型在测试集上进行预测的话，仍然存在着很大的误差，这种情况称为欠拟合，对应着高偏差。对于最右边的图，用一个高阶(五阶)去拟合，从数据中得到的模型结构很可能碰巧是该训练集特有的，即尽管五次多项式模型对训练集的拟合不错，但并非是一个好的模型，因为对于训练集以外的数据，该模型不一定能很好得进行预测，即泛化能力不够好，因此仍然存在着很大的泛化误差，这种情况称作过拟合，对应高方差。&lt;br&gt;　　在机器学习中，对偏差和方差的权衡是学习理论重点解决的问题。如果我们的模型太过于简单，只有少量的参数要学习，那么就可能存在高偏差(large bias but small variance)。如果我们的模型太过于复杂，拥有大量的参数，那么就可能存在高方差(large variance but small bias)。在上面的例子中，训练一个二次型的模型(对应中间那幅图)，比训练一个过于简单的一次型模型或过于复杂的五次型模型都好。　　&lt;br&gt;
    
    </summary>
    
      <category term="机器学习" scheme="xtf615.com/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="机器学习" scheme="xtf615.com/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="经验风险最小化" scheme="xtf615.com/tags/%E7%BB%8F%E9%AA%8C%E9%A3%8E%E9%99%A9%E6%9C%80%E5%B0%8F%E5%8C%96/"/>
    
      <category term="方差" scheme="xtf615.com/tags/%E6%96%B9%E5%B7%AE/"/>
    
      <category term="偏差" scheme="xtf615.com/tags/%E5%81%8F%E5%B7%AE/"/>
    
  </entry>
  
  <entry>
    <title>SVM支持向量机</title>
    <link href="xtf615.com/2017/03/28/SVM%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BA/"/>
    <id>xtf615.com/2017/03/28/SVM支持向量机/</id>
    <published>2017-03-28T00:26:31.000Z</published>
    <updated>2017-07-09T01:55:48.554Z</updated>
    
    <content type="html"><![CDATA[<p>　　本文将介绍SVM(Support Vector Machine)学习算法。SVM是现有的最强大的监督学习算法。我们首先讨论什么是间隔以及使用最大间隔来分类数据的思想。接着讨论最优间隔分类器，这里面会涉及拉格朗日对偶问题。我们也会讨论关于核方法以及如何有效地应用核方法到高维特征空间。最后我们会讨论SMO算法，它是SVM的一种实现方法。</p>
<h1 id="间隔的直观理解"><a href="#间隔的直观理解" class="headerlink" title="间隔的直观理解"></a>间隔的直观理解</h1><p>　　要理解支持向量机，首先必须先了解间隔以及关于预测置信度的概念。考虑一下逻辑回归，模型是\(h_\theta(x)=g(\theta^Tx)\),当且仅当\(h_\theta(x) \geq 0.5\),也即\(\theta^Tx \geq 0\)时，我们预测结果为1。考虑一个正例样本，显然\(\theta^Tx\)的值越大，\(h_\theta(x)=p(y=1|x;w;b)\)的值也越大，则预测样本label为1的置信程度也越高。更正式的，当\(\theta^Tx \gg 0\)时，可以认为我们的预测样本label为1的置信程度很高，同样，当\(\theta^Tx \ll 0\)时,可以认为我们的预测样本label为0的置信程度很高。给定一个训练集，如果我们能够在标签为1的样本中，找到合适的\(\theta\)，使得\(\theta^Tx^{(i)} \gg 0\)，那么这样拟合的效果就很好。同样，在标签为0的样本中，找到合适的\(\theta\)，使得\(\theta^Tx^{(i)} \ll 0\)。这样的拟合效果能够体现出分类器对于样本分类的置信程度很高。后面我们会使用函数间隔来形式化该思想。<br><a id="more"></a><br>　　我们可以换一种方式来理解。考虑下图，X代表正例，O代表反例，判定边界或称作分离超平面(separating hyperplane)将正反例分开，边界上，我们有\(\theta^T X = 0\)。考虑三个点A,B,C。<br><img src="/picture/machine-learning/svm1.jpg" alt="svm"><br>如上图所示，A点离判定边界很远。如果要预测A点的y值,显然可以很自信的预测y=1. 相反，C点离判定边界很近，尽管目前看我们可以判定C点y=1，但是如果我们对判定稍微移动一点，就很容易导致C点跑到另一侧y=0。因此可以说对A点预测的置信程度是高于C点的。B点介于两种情况之间。更通用的，对于离分离超平面远的点，我们可以对我们的预测结果很自信。因此，对于给定的训练集，我们希望找到一个判定边界，能够使得对于所有的样本都分类正确并且置信程度高，这也就意味着样本点要离判定边界远一点。后面我们会使用几何间隔来形式化该思想。</p>
<h1 id="符号变化"><a href="#符号变化" class="headerlink" title="符号变化"></a>符号变化</h1><p>　　为了使SVMs的讨论更加容易，我们需要对以前习惯的符号进行略微修正。我们首先考虑二分类线性分类器。对于标签y，我们现在不使用\(y \in \{0,1\}\),而使用\(y \in \{-1,1\}\)。对于线性分类器的参数，我们不使用向量\(\theta\),而使用参数\(w,b\)来表示：<br>$$h_{w,b}(x)=g(w^T x + b)$$<br>因此当\(z \geq 0\)时，g(z)=1, 当\(z&lt;0\)时，g(z)=-1。新的标志可以使得我们将截距项b和其他参数分开讨论。我们不再使用之前的\(x_0=1\), b相当于之前的\(\theta_0\),\(w\)相当于之前的\([\theta_1,\theta_2,…,\theta_n]^T\)<br>　　根据上文g的定义，我们的分类器会直接预测样本为1或者-1，而不是像逻辑回归那样先预测样本类别如果为1的概率，再根据概率大小确定样本类别。</p>
<h1 id="函数间隔和几何间隔"><a href="#函数间隔和几何间隔" class="headerlink" title="函数间隔和几何间隔"></a>函数间隔和几何间隔</h1><h2 id="函数间隔"><a href="#函数间隔" class="headerlink" title="函数间隔"></a>函数间隔</h2><p>　　对于一个给定的训练样本\((x^{(i)},y^{(i)})\), 我们定义该样本到w,b确定的分离超平面的函数间隔为：<br>$$\hat{\gamma}^{(i)}=y^{(i)}(w^T x + b)$$<br>　　注意为了使得预测更准确并且置信度更高，函数间隔越大越好。当\(y^{(i)}=1\)时,即\(w^T x + b\)在正方向越大越好。当当\(y^{(i)}=-1\)时,即\(w^T x + b\)在负方向越大越好。归纳起来，当\(y^{(i)}(w^T x + b)&gt;0\)时，我们对该样本的预测结果就是正确的。而最大函数间隔可以代表我们的预测结果是正确并且置信度高。<br>　　对于一个线性分类器, 在我们选择的g函数的基础上(取值为-1和1)，函数间隔的问题在于，只要成倍增大\(w,b\)，\(g(w^Tx+b)=g(2w^Tx+2b)\)，这不会改变\(h_{w,b}\)的值，因为\(h_{w,b}\)的值只取决于其符号，即正负性，而不取决于\(w^T x + b\)的数量级。但是我们发现函数间隔却变大了2倍。这意味着，放大\(w,b\),会使得函数间隔任意大，但是却对模型的改进没有任何帮助。我们可以引入正则化条件，例如规定\(||w||_2=1\)，即二阶范式的值。然后将\((w,b)\)替换成，\((\frac{w}{||w||_2},\frac{b}{||w||_2})\).<br>　　对于给定的训练集\(S=\{(x^{(i)},y^{(i)});i=1,…,m\}\),我们定义训练集S相对于\((w,b)\)决定的分离超平面的函数间隔为，每个样本相对于分离超平面函数间隔中的最小的那个值。即：<br>$$\hat{\gamma}=\min_\limits{i=1,…,m} \hat{\gamma}^{(i)}$$</p>
<h2 id="几何间隔"><a href="#几何间隔" class="headerlink" title="几何间隔"></a>几何间隔</h2><p><img src="/picture/machine-learning/svm2.jpg" alt="svm"><br>　　如上图，\((w,b)\)决定了分离超平面。可以证明\(w\)就是分离超平面的法向量。证明如下，考虑在分离超平面上取两点，\(x^{\\’}和x^{\\’’}\)<br>，则\(w^Tx^{\\’}+b=0\),\(w^Tx^{\\’’}+b=0\),两式相减有：<br>$$w^T(x^{\\’}-x^{\\’’})=0$$<br>因为\(x^{\\’}-x^{\\’’}\)是超平面上的向量，则根据垂直向量乘积为0，可以得到\(w\)就是分离超平面的法向量。<br>　　考虑点Ａ，Ａ代表了某个输入样本\(x^{(i)}\)且标签\(y^{(i)}=1\),设它到分离超平面的距离为\(\gamma^{(i)}\),就是图中AB代表的线段。<br>　　如何求\(\gamma^{(i)}\)的值呢？因为\(\frac{w}{||w||}\)代表和法向量同方向的单位向量，A点为\(x^{(i)}\),设B点位\(x^{(j)}\),BA和法向量同方向。则:<br>$$x^{(i)}-x^{(j)}=\gamma^{(i)}\frac{w}{||w||}$$<br>得出B点为：<br>$$x^{(j)}=x^{(i)}-\gamma^{(i)}\frac{w}{||w||}$$<br>因为B点位于分离超平面上，则：<br>$$w^T\left(x^{(i)}-\gamma^{(i)}\frac{w}{||w||}\right)+b=0$$<br>解的：<br>$$\gamma^{(i)}=\frac{w^Tx^{(i)}+b}{||w||}=\left(\frac{w}{||w||}\right)^T x^{(i)}+\frac{b}{||w||}$$<br>　　上面的结果是针对正样本，对于负样本，结果是一样的。<br>　　更通用的，我们定义某个训练样本\(x^{(i)},y^{(i)})\)相对于由\((w,b)\)决定的分离超平面的几何间隔为：<br>$$\gamma^{(i)}=y^{(i)}\left(\left(\frac{w}{||w||}\right)^T x^{(i)}+\frac{b}{||w||}\right)$$<br>注意到，如果\(||w||=1\),函数间隔等于几何间隔。这也是两种间隔之间的联系。几何间隔不受参数量级的影响，如果同时成倍放大w和b，几何间隔不会改变。正因为如此，在使用训练集拟合w和b的时候，我们可以施加任意的放大缩小约束条件，例如，可以限制\(||w||=1\)或者\(||w_1||=5\),或者\(||w_1+b||+|w_2|=2\),所有的这些可以通过重新调整w和b来恢复。<br>　　对于给定的训练集\(S=\{(x^{(i)},y^{(i)});i=1,…,m\}\),我们定义训练集S相对于\((w,b)\)决定的分离超平面的几何间隔为，每个样本相对于分离超平面几何间隔中的最小的那个值。即：<br>$$\gamma=\min_\limits{i=1,…,m} \gamma^{(i)}$$</p>
<h1 id="最优间隔分类器"><a href="#最优间隔分类器" class="headerlink" title="最优间隔分类器"></a>最优间隔分类器</h1><p>　　对于给定的训练集，根据前面的讨论，我们很自然的希望能够找到一个分离超平面使得几何间隔最大化，只有这样，对于预测结果我们的置信度才足够高，样本拟合的结果才足够好。<br>　　开始前，我们要强调下，本文所讨论的内容仍然是假设数据集市线性可分的。我们可以写出如下的优化问题：<br>$$\max_{\gamma,w,b} \gamma \\\\<br>使得，y^{(i)}(w^T x^{(i)} + b) \geq \gamma,　　i=1,2,…,m \\\\<br>||w||=1<br>$$<br>　　我们要最大化\(\gamma\)，使得每一个训练样本的函数间隔都至少为\(\gamma\)。\(||w||=1\)约束保证了函数间隔等于几何间隔。也即，我们保证了每一个训练样本的几何间隔都至少为\(\gamma\)。因此，解决该优化问题的方法就是不断调整\((w,b)\)，来最大化几何间隔。<br>　　但是||w||=1的约束条件是非凸性约束，最优解容易陷入局部最优，因此我们无法使用现成的标准优化方法来解该优化问题。我们尝试修改该优化问题，考虑如下：<br>$$\max_{\gamma,w,b} \frac{\hat{\gamma}}{||w||} \\\\<br>使得，y^{(i)}(w^T x^{(i)} + b) \geq \hat{\gamma},　　i=1,…,m<br>$$<br>　　我们尝试最大化几何间隔\(\frac{\hat{\gamma}}{||w||}\),使得每个样本的函数间隔都至少为\(\hat{\gamma}\)。几何间隔和函数间隔通过\(\gamma=\frac{\hat{\gamma}}{||w||}\)来联系。我们去掉了非凸约束||w||。我们通过将非凸约束转移到目标函数上，从而使得问题变成了非凸性问题。该问题目前仍然无法用标准通用方法解决。<br>　　对于上式，我们还可以再做一次变换。我们知道，等比例对\(w,b\)进行缩放，不会改变分离超平面的位置，假设已经得到了\(w,b\)，那么就能求得\(\hat{\gamma}\)的值，那么我们可以通过缩放w,b(同时除以\(\hat{\gamma}\)),使得\(\hat{\gamma}\)变为1.这样得到的分离超平面与最开始就将\(\hat{\gamma}\)设为1是一样的。因此优化问题可以改写成：<br>$$\max_{\gamma,w,b} \frac{1}{||w||}$$<br>也即：（加上1/2系数是为了使结果更漂亮）<br>$$\min_{\gamma,w,b} \frac{1}{2}{||w||}^2 \\\\<br>使得, y^{(i)}(w^T x^{(i)} + b) \geq 1, 　　i=1,…,m<br>$$<br>　　上述问题已经转换成凸性问题了，约束条件只有线性约束。该优化问题的结果就是最优间隔分类器。为了更好解决该问题，需要使用它的对偶问题，下面首先介绍拉格朗日对偶性。</p>
<h2 id="拉格朗日对偶性"><a href="#拉格朗日对偶性" class="headerlink" title="拉格朗日对偶性"></a>拉格朗日对偶性</h2><p>　　下面回顾一下线性约束优化问题。如下形式：<br>$$\min_w f(w) \\\\<br>使得， h_i(w)=0,　　i=1,…,l<br>$$<br>　　我们使用拉格朗日乘子(Lagrange multipliers)，拉格朗日方程如下：<br>$$L(w,\beta)=f(w)+\sum_{i=1}^l \beta_i h_i(w)$$<br>　　这里，\(\beta_i\)称作拉格朗日乘子，我们对其求偏导数并设为0.求得的值就是原问题的解了。<br>$$\frac{\partial L}{\partial w_i}=0; \frac{\partial L}{\partial \beta_i}=0$$<br>　　至于为什么引入拉格朗日算子可以求出极值，原因是f(w)的方向导数dw受等式的约束，dw的变化方向与f(w)的梯度度垂直时才能获得极值，而且在极值处，f(w)的梯度与其他等式梯度的线性组合平行，因此他们之间存在线性关系。<br>　　上述问题对应的是等式约束条件，我们需要将其扩展为不等式约束条件。考虑如下原始优化问题(primal optimization problem):<br>$$\min_w f(w) \\\\<br>使得， g_i(w) \leq 0,　　i=1,…,k \\\\<br>h_i(w)=0,　　i=1,…,l<br>$$<br>　　我们定义通用的拉格朗日方程(generalized Lagrangian)：<br>$$L(w,\alpha,\beta)=f(w)+\sum_{i=1}^k \alpha_i g_i(w) + \sum_{i=1}^l \beta_i h_i(w)$$<br>\(\alpha_i和\beta_i\)是拉格朗日乘子。考虑如下等式:<br>$$\theta_p(w)=\max_{\alpha,\beta:\alpha_i \geq 0} L(w,\alpha,\beta)$$<br>  　这里的下标p代表原始Primal。对于某些给定的\(w\)，如果w不符合约束条件，例如当某个\(g_i(w)&gt;0\)时，我们可以使得和该\(g_i(w)&gt;0\)相乘的乘子\(\alpha_i\)趋向于正无穷，则\(\theta_p(w)\)也趋向于正无穷；同样，当\(h_i(w) \neq 0\)时，根据\(h_i(w)\)的正负性，选择相应的\(\beta_i\)趋向于正无穷或负无穷，则\(\theta_p(w)\)也趋向于正无穷。因此对于不满足约束条件时，上述问题结果是：<br>  $$\theta_p(w)=\max_{\alpha,\beta:\alpha_i \geq 0} L(w,\alpha,\beta) \\\\<br>  =f(w)+\sum_{i=1}^k \alpha_i g_i(w) + \sum_{i=1}^l \beta_i h_i(w) \\\\<br>  =\infty$$<br>  　　相反，如果约束条件满足的话，对于给定的\(w\)，我们有,\(\theta_p(w)=f(w)\),这里的关键是我们的\(\alpha_i \geq 0且g_i(w) \leq 0\), 则\(\sum_{i=1}^k \alpha_i g_i(w) \leq 0\)，那么为了最大化\(\theta_p(w)\),有\(\sum_{i=1}^k \alpha_i g_i(w)=0\)。<br>  　　因此，我们有：<br>  $$<br>\begin{eqnarray} \theta_p(w)=\begin{cases} f(w),　　if \ w \ satisfies \ primal \ constraints \cr \infty,　　otherwise. \end{cases} \end{eqnarray}<br>  $$<br>　　因而，我们可以认为\(\theta_p(w)\)即使将约束条件与目标函数融合在一起的表达方法，考虑最小化问题，我们得到了如下公式：<br>$$\min_w \theta_p(w) = \min_w \max_{\alpha,\beta:\alpha_i \geq 0} L(w,\alpha,\beta) 　　　　　　　　　　　(1)$$<br>　　因为,\(\theta_p(w)\)在满足约束条件下等价于\(f(w)\)问题，则\(\min_w \theta_p(w)\)问题等价于我们最初的原始优化问题(primal problem) \(\min_w f(w)\)。<br>　　我们定义\(p^{*}\)为原始问题取得最优解时的函数值，也即:<br>$$p^{*} = \min_w \theta_p(w)$$<br>　　接着定义：<br>$$\theta_D(\alpha,\beta)=\min_w L(w,\alpha,\beta)$$<br>　　上式中的下标D代表对偶(dual)。注意，\(\theta_p(w)\)优化的参数是\(\alpha,\beta\)，而这里\(\theta_D\)优化的参数是w.<br>　　我们可以得到对偶问题：<br>$$\max_{\alpha,\beta:\alpha_i \geq 0} \theta_D(\alpha,\beta)=\max_{\alpha,\beta:\alpha_i \geq 0} \min_w L(w,\alpha,\beta)　　　　　　　　　　(2)$$<br>　　该式就是我们原始问题的对偶形式，我们对比(1)和(2)发现：两个式子很相似，只是max和min位置调换了。<br>　　我们定义对偶问题的最优解为:<br>$$d^{*}=\max_{\alpha,\beta:\alpha_i \geq 0} \theta_D(w)$$<br>　　对偶问题和原始问题的关联如下：<br>$$d^{*}=\max_{\alpha,\beta:\alpha_i \geq 0} \min_w L(w,\alpha,\beta) \leq \min_w \max_{\alpha,\beta:\alpha_i \geq 0} L(w,\alpha,\beta) = p^{*}$$<br>　　可以证明,对于任意函数,max min的结果总是小于等于min max。在特定条件下, 我们有：<br>$$d^{*}=p^{*}$$<br>　　这些特定的条件包括，约束不等式\(g_i\)都是凸函数(线性函数都属于凸函数);约束\(h_i\)都是仿射函数(其实就是在线性函数基础上加上截距b);不等式\(g_i(w)\)约束条件严格可行,即对于任意的i,存在w,使得\(g_i(w)&lt;0\)<br>　　在这些假设下，肯定存在\(w^{*},\alpha^{*},\beta^{*}\),使得\(w^{*}\)是原始问题的解，\(\alpha^{*},\beta^{*}\)是对偶问题的解,且\(p^{*}=d^{*}=L(w^{*},\alpha^{*},\beta^{*})\),这样的\(w^{*},\alpha^{*},\beta^{*}\)需要满足KKT(Karush-Kuhn-Tucker)条件，KKT条件如下：<br>$$\frac{\partial}{\partial w_i}L(w^{*},\alpha^{*},\beta^{*})=0,　i=1,…,n \\\\<br>\frac{\partial}{\partial \beta_i}L(w^{*},\alpha^{*},\beta^{*})=0,　i=1,…,l \\\\<br>\alpha_i^{*}g_i(w^{*})=0,　i=1,…,k \\\\<br>g_i(w^{*}) \leq 0,　i=1,…,k \\\\<br>\alpha^{*} \geq 0,　i=1,…,k<br>$$<br>注意第3个式子，特别的:<br>$$当\alpha_i^{*}大于0时,g_i(w^{*})=0$$<br>我们称之为KKT互补条件。也就是说，\(g_i(w^{*})=0\)时，w处于可行域的边界上，这时才是起作用的约束，其他位于可行域内内部(\(g_i(w^{*})小于0\)的点都是不起作用的约束,其\(\alpha_i^{*}=0\))。KKT的总体思想是将极值会在可行域边界上取得，也就是不等式为0或等式约束里取得，而最优下降方向一般是这些等式的线性组合，其中每个元素要么是不等式为0的约束，要么是等式约束。对于在可行域边界内的点，对最优解不起作用，因此前面的系数为0。这个条件比较重要，在后文中，它将展示SVM只有一些支持向量点会起作用，在SMO算法中会给出收敛测试。</p>
<h2 id="最优间隔分类器求解"><a href="#最优间隔分类器求解" class="headerlink" title="最优间隔分类器求解"></a>最优间隔分类器求解</h2><p>　　上面讲述的原始／对偶优化问题(primal/dual optimal problem)，其目的在于对原始问题上不易求解的问题进行转换，使之更易求解。下面介绍通过对最优间隔分类器的对偶问题进行求解，得到的简化后的问题的过程。之前我们的优化问题是：<br>$$\min_{\gamma,w,b} \frac{1}{2}{||w||}^2 \\\ 使得, y^{(i)}(w^T x^{(i)} + b) \geq 1, 　　i=1,…,m$$<br>　　我们将约束条件改写成:<br>$$g_i(w)=-y^{(i)}(w^T x^{(i)} + b)+1 \leq 0$$<br>　　该约束条件对每个样本都成立，根据KKT对偶互补条件，对于函数间隔为1(即满足约束g_i(w)=0)的样本点，我们有\(\alpha_i &gt; 0\)<br>　　考虑下图，最大间隔分离超平面是图中的实线。<br><img src="/picture/machine-learning/svm3.jpg" alt="svm"><br>　　最小间隔的点是那些靠近分离超平面的点，这里有3个这样的点，位于图中虚线处，1个负样本，2个正样本，因此只有该3个点对应的\(\alpha_i\)非零。这3个点叫做该问题的支持向量，意味着在求解问题时，使用到的支持向量数比样本大小少的多。<br>　　上述问题对应的拉格朗日方程是：<br>$$L(w,b,\alpha)=\frac{1}{2}{||w||}^2-\sum_{i=1}^m \alpha_i[y^{(i)}(w^T x^{(i)}+b)-1]$$<br>　　我们的问题只有不等式约束，没有等式约束，故拉格朗日乘子只有\(\alpha\)。并且该问题符合\(d^{*}=p^{*}\)的假设，肯定存在\(w^{*},\alpha^{*},\beta^{*}\)使得原始问题和对偶问题共用最优解。<br>　　求解对偶问题时，首先要固定\(\alpha\),以w,b为变量，最小化L；最小化L时，求解L对w和b的偏导，并将导数设为0，可以得到：<br>$$\nabla_w L(w,b,\alpha)=w-\sum_{i=1}^m \alpha_i y^{(i)}x^{(i)}=0$$<br>得到:<br>$$w=\sum_{i=1}^m \alpha_i y^{(i)}x^{(i)}$$<br>对b求偏导，得到：<br>$$\frac{\partial}{\partial b}(w,b,\alpha)=\sum_{i=1}^m \alpha_i y^{(i)}=0$$<br>我们将上式代入拉格朗日方程得到：<br>$$L(w,b,\alpha)=\frac{1}{2}{||w||}^2-\sum_{i=1}^m \alpha_i[y^{(i)}(w^T x^{(i)}+b)-1] \\\\<br>= \frac{1}{2}w^T w-\sum_{i=1}^m \alpha_i y^{(i)} w^T x^{(i)}-\sum_{i=1}^m \alpha_i y^{(i)} b + \sum_{i=1}^m \alpha_i \\\\<br>= \frac{1}{2}w^T \sum_{i=1}^m \alpha_i y^{(i)}x^{(i)} - \sum_{i=1}^m \alpha_i y^{(i)} w^T x^{(i)} + \sum_{i=1}^m \alpha_i \\\\<br>= \sum_{i=1}^m \alpha_i - \frac{1}{2}\sum_{i=1}^m \alpha_i y^{(i)} w^T x^{(i)} \\\\<br>= \sum_{i=1}^m \alpha_i - \frac{1}{2}\sum_{i=1}^m \alpha_i y^{(i)} \left(\sum_{j=1}^m \alpha_j y^{(j)} (x^{(j)})^T \right)x^{(i)} \\\\<br>= \sum_{i=1}^m \alpha_i - \frac{1}{2}\sum_{i=1}^m \sum_{j=1}^m \alpha_i \alpha_j y^{(i)} y^{(j)} (x^{(j)})^T x^{(i)} \\\\<br>=\sum_{i=1}^m \alpha_i - \frac{1}{2}\sum_{i,j=1}^m y^{(i)} y^{(j)} \alpha_i \alpha_j (x^{(j)})^T x^{(i)}<br>$$<br>得到如下对偶优化问题：<br>$$<br>\max_{\alpha} W(\alpha) = \sum_{i=1}^m \alpha_i - \frac{1}{2}\sum_{i,j=1}^m y^{(i)} y^{(j)} \alpha_i \alpha_j ＜x^{(i)},x^{(j)}＞ \\\\<br>使得， \alpha_i \geq 0,　　i=1,…,m \\\\<br>\sum_{i=1}^m \alpha_i y^{(i)} = 0<br>$$<br>　　\(＜x^{(i)},x^{(j)}＞\)代表向量的内积,该式子可以当作一个整体，在后续核技巧中发挥重要作用。<br>　　上式第一步为原问题，第二部将累加和展开，第三步代入w和b求导并设置为0后的结果，第四步合并系数，第五步代入w求导结果。<br>　　再强调一次，该问题符合\(d^{*}=p^{*}\)的假设和KKT条件。因此我们可以求解对偶优化问题，从而求得原始优化问题。上述对偶优化问题求解的参数只有\(\alpha_i\)。如果我们能够求解使得对偶问题最大化的\(\alpha\), 我们就可以通过\(w=\sum_{i=1}^m \alpha_i y^{(i)}x^{(i)}\)推出原始优化问题的\(w\).如果找到了\(w^{*}\),可以得到截距b的值：<br>$$b^{*}=-\frac{\max_{i:y^{(i)}=-1} {w^{*}}^T x^{(i)} + \min_{i:y^{(i)}=1} {w^{*}}^T x^{(i)}}{2}$$<br>　　上式是在确定\(w^{*}\)后，正例和负例的支持向量所对应的截距的平均值，为了更直观的理解，考虑下图：<br><img src="/picture/machine-learning/svm3.jpg" alt="svm"><br>　　上式即是两条虚线与纵轴的截距的平均值。也可以理解为离超平面最近的正的函数间隔要等于离超平面最近的负的函数间隔，可以将分母2乘到b，再整理式子：<br>$$(\max_{i:y^{(i)}=-1} {w^{*}}^T x^{(i)}+b^{*}) + (\min_{i:y^{(i)}=1} {w^{*}}^T x^{(i)}+b^{*})=0$$<br>　　可以理解为二者函数间隔互为相反数，抵消了。<br>　　我们进一步考察一下式子\(w=\sum_{i=1}^m \alpha_i y^{(i)}x^{(i)}\)，假设我们已经找到了优化问题的最优解，现在需要对新样本ｘ作出预测,我们可以通过计算\(w^T x+b\)来判断，大于等于0则预测y=1,否则y=0.但是使用上述式子，我们有：<br>$$w^T x+b=\left(\sum_{i=1}^m \alpha_i y^{(i)}x^{(i)}\right)^T x + b \\\\<br>=\sum_{i=1}^m \alpha_i y^{(i)}＜x^{(i)},x＞+b$$<br>　　根据上式，如果我们求得了\(\alpha_i\)，为了对新样本做出预测，我们可以只计算x和每一个训练样本的内积。更进一步，前面我们讨论过，只有在支持向量处，\(\alpha_i\)才非零，因此我们只需要计算新样本和每一个支持向量之间的内积，这样计算数据量就少了很多。相当于只有这些支持向量为目标函数的计算做出贡献。<br>　　下文将引入核技巧到目标函数中，从而得到完全的支持向量机算法，然后介绍SMO序列最小化算法，该算法是优化问题的一种较快的解决方法。</p>
<h1 id="核Kernels"><a href="#核Kernels" class="headerlink" title="核Kernels"></a>核Kernels</h1><p>　　上文中，我们的对偶最优问题中都会出现内积的形式，本部分将介绍可以使用核来替代内积，一定程度上可以解决非线性可分的问题。</p>
<h2 id="核函数的理解"><a href="#核函数的理解" class="headerlink" title="核函数的理解"></a>核函数的理解</h2><p>　　核函数！＝内积！＝映射！＝相似度，核函数是一种表征映射、实现内积逻辑关系且降低计算复杂度的一类特殊函数。(满足Mercer’s condition)。简单来说，核函数只是用来计算映射到高维空间滞后的内积的一种简便方法。<br>　　一般英文文献中对Kernel有两种提法，意识Kernel Function,二是Kernel Trick。从Trick一词可以看出，这只是一种运算技巧，不涉及高深莫测的东西。具体巧在哪里，我们如果想进行原本线性不可分的数据集的分割，那么一种方法是引入Soft Margin来容忍一些错误；另一种方法是对输入空间(Input Space)做特征扩展(Feature Expansion)，把数据集映射到高维中去，形成了特征空间(Feature Space)。我们几乎可以认为原本在低维中线性不可分的数据集在足够高的维度中存在线性可分的超平面。<br>　　对于第二种方法，我们所做的就是在特征空间中，使用原本在线性可分的情况下的优化方法，来找到最优分离超平面，唯一不同的是代入数据不同，我们需要代入得是\(\phi_(x_i)\),而不是\(x_i\)，\(\phi_(x_i)\)才是真正的从低维空间到高维空间的映射。考虑我们前面的优化式子，存在着一步内积计算。也即必须求出\(\phi_(x_i)^T \phi_(x_j)\),我们定义核函数为\(K(x_i,x_j)=\phi_(x_i)^T \phi_(x_j)\),使得我们不需要显示的计算每一个\(\phi_(x_i)\),甚至不需要知道\(\phi(\dot)\)长什么样，就可以直接求出\(\phi_(x_i)^T \phi_(x_j)\)的值，同时也能够大幅度提高计算速度和效率。<br>　　另外需要强调的一点是，kernel是一个独立的概念，kernel在SVM中的应用只是冰山一角，kernel的应用非常广泛，甚至出现的比SVM还早。</p>
<h2 id="核函数"><a href="#核函数" class="headerlink" title="核函数"></a>核函数</h2><p>　　首先让我们回顾一下线性回归中房价预测的例子，我们曾尝试对变量居住面积Ｘ进行处理以获得三次型函数。即使用特征\(x,x^2,x^3\)。为了区别这两种形式的变量，我们称原始输入为输入属性(attributes)，映射之后的输入称为输入特征(features)。我们记\(\phi\)为特征映射(feature mappping)，即从属性到特征的映射。在上面例子中，我们有:<br>$$\phi(x)=\begin{bmatrix}x \\\ x^2 \\\ x^3 \end{bmatrix}$$<br>　　我们现在不使用原始的属性ｘ,而使用映射后的特征\(\phi(x)\).我们将之前的对偶优化问题使用\(\phi(x)\)来替换\(x\)。因为我们的对偶优化问题有内积的形式存在，意味着我们可以使用\(＜\phi(x),\phi(z)＞\)来替换\(＜x,z＞\)。对于给定的映射\(\phi\),我们定义相应的核为：<br>$$K(x,z)=\phi(x)^T \phi(Z)$$<br>　　这样，对于出现\(＜x,z＞\)的地方，我们可以使用\(K(x,z)\)来替换，这样我们的算法就是使用特征\(\phi\)得到的,而不是原始的属性。这样的处理可以使数据线性可分的概率变大，即不能保证在高维上一定是线性可分的，但一般情况下高维空间比低维空间上更倾向于线性可分。<br>　　但通常通过\(\phi\)映射后的向量维度过高，导致映射后向量内积的计算复杂度过高，核函数的引入就是为了解决这个问题，它既可以使得我们不需要显示定义出映射函数就能计算两个向量在高维空间的内积了，又使得时间复杂度降低。<br>　　下面介绍一个核函数的例子，假设\(x,z \in {\mathbb{R}}^n\)，考虑如下核函数：<br>$$K(x,z)=(x^T z)^2$$<br>　　重写为：<br>$$K(x,z)=\left(\sum_{i=1}^n x_i z_i \right) \left(\sum_{j=1}^n x_i z_i \right) \\\ <br>= \sum_{i=1}^n \sum_{j=1}^n x_i x_j z_i z_j \\\\<br>= \sum_{i,j=1}^n (x_i x_j) (z_i z_j)<br>$$　　<br>因为\(K(x,z)=\phi(x)^T \phi(Z)\), 则相应的映射为：<br>$$\phi(x)=\begin{bmatrix} x_1 x_1 \\\ x_1 x_2 \\\ … \\\ x_n x_n\end{bmatrix}<br>$$<br>因此计算\(\phi(x)\)需要\(O(n^2)\)时间复杂度(内外两层循环计算乘积)。而计算\(K(x,z)\)只需要\(O(n)\)时间复杂度(一层循环计算\(\sum_{i=1}^n x_i z_i\),再对结果平方即可)。<br>　　一个相似的核函数如下：<br>$$K(x,z)=(x^T z+c)^2 \\\\<br>=\sum_{i,j=1}^n (x_i x_j)(z_i z_j) + \sum_{i=1}^n (\sqrt{2c}x_i)(\sqrt{2c}z_i) + c^2$$<br>　　对应的映射如下：<br>$$\phi(x)=\begin{bmatrix} x_1 x_1 \\\ x_1 x_2 \\\ … \\\ x_n x_n \\\ \sqrt{2c}x_1 \\\ … \\\  \sqrt{2c}x_n \\\ c \end{bmatrix}$$<br>　　一个更一般化的核函数如下：<br>$$K(x,z)=(x^T z+c)^d$$<br>　　该核函数对应的映射函数结果是一个\(\binom{n+d}{n}\)(组合符号)大小的向量，向量中每个向量都是最高为d阶的变量的组合。<br>　　对于上面的这些核函数，虽然它们对应的映射函数的维度可能是\(n^2,n^4\)等，意味着直接计算映射结果的内积的复杂度为\(O(n^2),O(n^4)\)。但是如果直接计算核函数的值，其复杂度均为\(O(n)\).<br>　　直观上来看，如果\(\phi(x)和\phi(z)\)在其对应的维度空间中位置接近，那么内积值K会很大(可认为同方向比较大)，如果很远则内积很小。这意味着核函数K是一个向量x和向量z接近程度的度量函数，从而引出SVM中使用较广泛的高斯核：<br>$$K(x,z)=exp\left(-\frac{ ||x-z||^2 }{2 \sigma^2}\right)$$<br>　　高斯核函数是一种度量x和z相似度的方法，如果x和y位置很接近，则K值接近1;如果很远，则k值接近0。高斯核函数对应的映射函数\(\phi\)是可以映射到无限维的。</p>
<h2 id="Mercer定理"><a href="#Mercer定理" class="headerlink" title="Mercer定理"></a>Mercer定理</h2><p>　　那么，什么样的核函数才是正确有效的核函数的？即如何判断一个函数是不是能拆分成映射函数乘积的形式？这两种表述是等价的，核函数是由映射函数乘积得到的，因而如果核函数合法，那么必然能写成两个映射函数的乘积。<br>　　我们首先定义核矩阵，对一个数据集\(\{x^{(1)},x^{(2)},…,x^{(m)}\}\),定义一个m*m的矩阵K，K中每个元素定义如下：<br>$$K_{ij}=K(x^{(i)},x^{(j)})$$<br>　　注意K既代表核函数又代表核矩阵。对于核矩阵，有几个性质，首先很显然\(K_{ij}=K_{ji}\),因此核矩阵是一个对称矩阵(symmetric matrix)。其次，对于任意的m维向量z，记\(\phi_k(x)\)为向量\(\phi(x)\)的第k个分量，我们有:<br>$$z^T K z = \sum_i \sum_j z_i K_{ij} z_j = \sum_i \sum_j z_i \phi(x^{(i)})^T \phi(x^{(j)}) z_j \\\\<br>=\sum_i \sum_j z_i \left(\sum_k \phi_k(x^{(i)}) \phi_k(x^{(j)}) \right)z_j \\\\<br>=\sum_k \sum_i \sum_j z_i \phi_k(x^{(i)}) \phi_k(x^{(j)}) z_j  \\\\<br>= \sum_k \left(\sum_i z_i \phi_k(x^{(i)})\right)^2 \geq 0<br>$$<br>　　因为z是任意向量，可以得出K是半正定矩阵，加上对称性，K是对称半正定矩阵。如果K是有效的核，可以推出K是对称半正定矩阵，同样如果K是对称半正定矩阵，可以得出K是有效核。因此我们可以归纳出一个充要条件，该定理称作Mercer定理：<br>　　给定一个K：\({\mathbb{R}}^n \times {\mathbb{R}}^n -&gt; \mathbb{R}\),那么K是有效核的充分必要条件是，对于任意一个有限数据集，对应的核矩阵式对称半正定矩阵。<br>　　对于核来说，不仅仅只存在SVM内，对于任意的算法，只要计算时出现了内积，都可以用核函数替代，从而提高在高维数据上的性能，例如感知机算法，代入后发展为核感知机算法，这也是核函数被称作核技巧的原因。</p>
<h1 id="软间隔分类器"><a href="#软间隔分类器" class="headerlink" title="软间隔分类器"></a>软间隔分类器</h1><p>　　上文提到的最优间隔分类器时，一直强调数据是线性可分的。但是，当数据线性不可分时，或者映射到高维空间后仍然线性不可分，再或者即便是线性可分的但实际应用中不可避免出现噪声时，该如何处理呢？下文使用软间隔的方式进行解决。<br><img src="/picture/machine-learning/svm4.jpg" alt="svm"><br>　　首先观察上图，左边图代表最优间隔分类器，右边图在左上角添加一个负样本，为了实现最优间隔分类器，这导致分离超平面发生了一个急剧的旋转变换，这也导致得到的新的分类器在整体上只拥有一个比原来更小的间隔。<br>　　为了使算法能够适用于非线性分类器，对一些额外的点不那么敏感，我们重新修改优化问题，加入L1正则化项：<br>$$\min_{\gamma,w,b} \frac{1}{2}{||w||}^2+C \sum_{i=1}^m \zeta_i \\\\<br>使得, y^{(i)}(w^T x^{(i)} + b) \geq 1-\zeta_i, 　　i=1,…,m \\\\<br>\zeta_i \geq 0,　　i=1,…,m<br>$$<br>　　注意上述的正则化是指第二项\(C \sum_{i=1}^m \zeta_i\)，称作hinge loss。如果是L2正则化则为，\(C\sum_{i=1}^m \zeta_i^2\),称作squared-hinge-loss。可参考<a href="http://www.doc88.com/p-6761653257229.html" target="_blank" rel="external">A study on L2-Loss(Squared Hinge-Loss Multi-Class SVM)</a>。另外对于第一项\(\frac{1}{2}{||w||}^2\)有的时候也叫做正则化，我们上述公式的第一项就是L2正则化。为了区别，对于上式，称第一项为<strong>L2-regularized</strong>，第二项为<strong>L1-loss</strong>。可参考<a href="https://www.quora.com/Support-Vector-Machines/Liblinear-does-not-support-L1-regularized-L1-loss-hinge-loss-support-vector-classification-Why" target="_blank" rel="external">Liblinear does not support L1-regularized L1-loss ( hinge loss ) support vector classification. Why?</a>。在sklearn的LinearSVC中，L2-regularized对应参数penalty=’l2’,L1-loss对应参数loss=’hinge’。如果使用L2-loss，则loss=’squared_hinge’。<br>　　因此现在样本允许函数间隔小于1,如果一个样本的函数间隔为\(1-\zeta_i\),我们在目标函数上增加一个代价项\(C\zeta_i\). 参数C在\(||w||^2\)变大(我们之前希望其越小越好)和保证大多数样本函数间隔至少为1之间进行权衡。相当于现在放宽了约束条件，但是我们也对目标函数进行惩罚，使之更加严格。松弛因子\(\zeta\)使SVM能够容忍异常离群点的存在。惩罚因子\(C\)决定了你有多重视离群点带来的损失，显然当所有离群点的松弛变量(\(\zeta\))的和一定时，你定的C越大，对目标函数的损失也越大，此时就暗示着你非常不愿意放弃这些离群点，最极端的情况是你把C定为无限大，这样只要稍有一个点离群，目标函数的值马上变成无限大，马上让问题变成无解，这就退化成了硬间隔问题，即C越大，你越希望在训练数据上少犯错误，而实际上这是不可能/没有意义的，于是就造成过拟合。<br>　　根据上述软间隔优化问题得到新的拉格朗日方程：<br>$$L(w,b,\zeta,\alpha,r)=\frac{1}{2}w^T w+C \sum_{i=1}^m \zeta_i - \sum_{i=1}^m \alpha_i[y^{(i)}(w^T x^{(i)}+b)-1+\zeta_i]- \sum_{i=1}^m r_i \zeta_i$$<br>　　\(\alpha_i,r_i\)是拉格朗日乘子，并且都\(\geq 0\).我们将上述改写成对偶问题的形式，具体过程不进行推导，和前文类似,得到如下对偶问题：<br>$$ \max_\alpha W(\alpha)= \sum_{i=1}^m \alpha_i - \frac{1}{2} \sum_{i,j=1}^m y^{(i)} y^{(j)} \alpha_i \alpha_j ＜x^{(i)},x^{(j)}＞ \\\\<br>使得, 0 \leq \alpha_i \leq C,　　　i=1,…,m \\\\<br>\sum_{i=1}^m \alpha_i y^{(i)} = 0<br>$$<br>　　我们惊奇的发现，该式子和之前最优间隔分类器优化问题唯一的区别是对\(\alpha_i\)做了进一步约束。求解得到\(\alpha\)后，w仍然可以按照公式\(w=\sum_{i=1}^m \alpha_i y^{(i)} x^{(i)}\)给出，但是截距b得到方式需要变化。KKT中的互补条件也要有略微变化：<br>$$\alpha_i=0 \Longrightarrow y^{(i)}(w^T x^{(i)} + b) \geq 1 \\\\<br>\alpha_i=C \Longrightarrow y^{(i)}(w^T x^{(i)} + b) \leq 1 \\\\<br>0 &lt; \alpha_i &lt; C \Longrightarrow y^{(i)}(w^T x^{(i)} + b) = 1<br>$$<br>　　这些条件将在下一节中用于判断SMO算法是否收敛。至此，终于已经得到一个可以应用于实际的具体问题了，下面一节将对对偶问题求解进行讨论。</p>
<h1 id="SMO算法"><a href="#SMO算法" class="headerlink" title="SMO算法"></a>SMO算法</h1><p>　　SMO全称序列最小优化(sequential minimal optimization)，给出了一种解决对偶问题的有效方法。</p>
<h2 id="坐标上升法"><a href="#坐标上升法" class="headerlink" title="坐标上升法"></a>坐标上升法</h2><p>　　在介绍SMO算法前，先介绍一个简化的但与SMO使用同一思想的算法，坐标上升法(Coordinate Ascent)。<br>　　考虑如下无约束优化问题:<br>$$\max_\alpha W(\alpha_1,\alpha_2,…,\alpha_m)$$<br>　　我们认为W是关于\(\alpha_i\)参数的函数。暂且先不考虑SVM。我们已经学习过梯度上升和牛顿法，现在介绍一种新方法：<br><strong>Loop Until Convergence: {</strong><br><strong>For　i=1,2,…,m {</strong><br>        $$\alpha_i := \mathop{argmax}\limits_{\hat{\alpha_i}}W(\alpha_1,…,\alpha_{i-1}, \hat{\alpha_i},…,\alpha_n)<br>        $$<br>    　　}<br><strong>}</strong><br>　<br>　　最内层循环,当更新\(\alpha_i\)时，保持其它参数不变。在上述方法中，我们内层循环按照下标顺序遍历，在其他情况下，我们也可能根据一些规则，例如使得\(W(\alpha)\)增幅最大来选择相应的\(\alpha_i\)。如果argmax方法能够有效的执行，那么坐标上升法会是一个效率不错的算法。下图是只有二次型函数的坐标上升法：<br><img src="/picture/machine-learning/svm5.jpg" alt="svm"><br>　　上图的椭圆代表二次型函数的等高线。坐标上升法初始\(\alpha\)为(2,-2)。图中线条代表了从起始点到全局最大值经过的路径，每一步坐标上升法的收敛方法都与一个坐标轴平行，因为我们一次优化只更新一个变量，固定其余的变量。这张图只有2个参数，所以才能在二维图中展示出来。</p>
<h2 id="SMO算法-1"><a href="#SMO算法-1" class="headerlink" title="SMO算法"></a>SMO算法</h2><p>　　我们的优化问题是：<br>$$ \max_\alpha W(\alpha)= \sum_{i=1}^m \alpha_i - \frac{1}{2} \sum_{i,j=1}^m y^{(i)} y^{(j)} \alpha_i \alpha_j ＜x^{(i)},x^{(j)}＞ \\\\<br>使得, 0 \leq \alpha_i \leq C,　　　i=1,…,m<br>\sum_{i=1}^m \alpha_i y^{(i)} = 0<br>$$<br>　　SMO算法与坐标上升法的不同地方在于，我们的对偶优化问题存在着约束\(\sum_{i=1}^m \alpha_i y^{(i)} = 0\),使得当固定其他参数而只改变一个参数的时候，发现该参数实际上是固定不变的，这样就无法进行更新。例如第一次更新\(\alpha_1\),则：<br>$$\alpha_1 y^{(1)} = - \sum_{i=2}^m \alpha_i y^{(i)}$$<br>　　两边同乘\(y^{(1)}\),即：<br>$$\alpha_1 = -y^{(1)} \sum_{i=2}^m \alpha_i y^{(i)}$$<br>　　因此，\(\alpha_1\)由其他参数唯一决定，我们就无法在不违反约束的情况下更新\(\alpha_1\).<br>　　因而SMO算法每次选择两个参数进行优化。选择参数时，使用一些启发式规则选择使全局目标函数增长最大的参数，具体：<br><img src="/picture/machine-learning/svm6.jpg" alt="svm"><br>　　假设我们在一次优化中选择\(\alpha_1,\alpha_2\)，根据约束条件，有:<br>$$\alpha_1 y^{(1)} + \alpha_2 y^{(2)} = - \sum_{i=3}^m \alpha_i y^{(i)}$$<br>　　右边式子是固定已知的(因为我们固定了其余的参数,注意这些参数一开始都有初始化的,因此上一次迭代完的参数值是已知的)。我们设右边式子为\(\zeta\):<br>$$\alpha_1 y^{(1)} + \alpha_2 y^{(2)} = \zeta$$<br><img src="/picture/machine-learning/svm7.jpg" alt="svm"><br>　　如上图，画出了该次迭代的约束直线，我们知道\(\alpha_1,\alpha_2\)一定位于\([0,C] \times [0,C]\)决定的边界以内。我们进一步得出,\(L \leq \alpha_2 \leq H\),否则\((\alpha_1,\alpha_2)\)无法同时满足在边界内核约束直线上。在这个例子中，L=0。实际上，两个参数中可以将一个当作变量，另外一个当作该变量的函数。我们重写为:<br>$$\alpha_1 = (\zeta-\alpha_2 y^{(2)})y^{(1)}$$<br>　　因此，目标函数W可以重写为：<br>$$W(\alpha_1,\alpha_2,…,\alpha_m)=W((\zeta-\alpha_2 y^{(2)})y^{(1)}, \alpha_2,…,\alpha_m)$$<br>　　将\(\alpha_3,…,\alpha_m\)当作常量，我们不难证明W是关于\(\alpha_2\)的二次函数，可以表达为\(a\alpha_2^2+b\alpha_2+c\)。加上[L,H]的约束条件，我们不难求出该二次函数最大时的\(\alpha_2\)取值。记二次函数无约束时的最优解为,\(\alpha_2^{new,unclipped}\),则有:<br><img src="/picture/machine-learning/svm8.jpg" alt="svm"><br>　　得到\(\alpha_2^{new}\)后，再使用\(\alpha_1,\alpha_2\)的关系求得\(\alpha_1^{new}\),最后在循环中进行更新。<br>　　关于选择更新参数的规则以及截距b如何求解，可以参考SMO作者Platt的论文。</p>
<h1 id="SVM的应用"><a href="#SVM的应用" class="headerlink" title="SVM的应用"></a>SVM的应用</h1><p>　　SVM作为一种分类器，在分类问题上表现惊人。比如文本分类、图像分类等。下面列举几个SVM的实际应用。<br>　　第一个例子是关于手写数字识别的问题。给定一张16*16的图片，上面写着0-9的10个数字，使用SVM判断。这本来是NN(Neural Network)比较擅长的问题，但初次用在SVM上时效果就好的惊人，因为SVM没有图片识别的先验知识，它只依据像素就能达到很好的效果。SVM上使用高斯核和多项式核都能在该问题上达到和NN相当的效果。<br>　　再比如通过氨基酸序列对蛋白质种类进行判别，假设有20种氨基酸，它们按照不同的序列组成不同的蛋白质，但是这些氨基酸序列的长度差异很大，那么该如何表示特征呢？有一种方式是使用连续4个氨基酸序列出现次数作为向量。首先求出氨基酸序列所有的排列组合\(20^4\)种，每个排列组合作为特征向量的一个特征，值是出现的次数，例如对于AABAABACDEF，可以得到AABA：2，ABAA：1，…，CDEF：1。因为特征向量的长度为\(20^4\)种,难以载入内存，但有一种高效的动态规划算法可以解决该问题。最后，再使用SVM进行建模。</p>
<h1 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h1><p><a href="http://open.163.com/special/opencourse/machinelearning.html" target="_blank" rel="external">斯坦福大学机器学习视频教程</a><br><a href="https://www.zhihu.com/question/24627666/answer/28460490" target="_blank" rel="external">知乎: 核函数理解</a><br><a href="http://www.svms.org/mercer/" target="_blank" rel="external">Mercer’s Condition</a><br><a href="https://www.microsoft.com/en-us/research/wp-content/uploads/2016/02/tr-98-14.pdf" target="_blank" rel="external">Platt, John (1998), Sequential Minimal Optimization: A Fast Algorithm for Training Support Vector Machines</a></p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;　　本文将介绍SVM(Support Vector Machine)学习算法。SVM是现有的最强大的监督学习算法。我们首先讨论什么是间隔以及使用最大间隔来分类数据的思想。接着讨论最优间隔分类器，这里面会涉及拉格朗日对偶问题。我们也会讨论关于核方法以及如何有效地应用核方法到高维特征空间。最后我们会讨论SMO算法，它是SVM的一种实现方法。&lt;/p&gt;
&lt;h1 id=&quot;间隔的直观理解&quot;&gt;&lt;a href=&quot;#间隔的直观理解&quot; class=&quot;headerlink&quot; title=&quot;间隔的直观理解&quot;&gt;&lt;/a&gt;间隔的直观理解&lt;/h1&gt;&lt;p&gt;　　要理解支持向量机，首先必须先了解间隔以及关于预测置信度的概念。考虑一下逻辑回归，模型是\(h_\theta(x)=g(\theta^Tx)\),当且仅当\(h_\theta(x) \geq 0.5\),也即\(\theta^Tx \geq 0\)时，我们预测结果为1。考虑一个正例样本，显然\(\theta^Tx\)的值越大，\(h_\theta(x)=p(y=1|x;w;b)\)的值也越大，则预测样本label为1的置信程度也越高。更正式的，当\(\theta^Tx \gg 0\)时，可以认为我们的预测样本label为1的置信程度很高，同样，当\(\theta^Tx \ll 0\)时,可以认为我们的预测样本label为0的置信程度很高。给定一个训练集，如果我们能够在标签为1的样本中，找到合适的\(\theta\)，使得\(\theta^Tx^{(i)} \gg 0\)，那么这样拟合的效果就很好。同样，在标签为0的样本中，找到合适的\(\theta\)，使得\(\theta^Tx^{(i)} \ll 0\)。这样的拟合效果能够体现出分类器对于样本分类的置信程度很高。后面我们会使用函数间隔来形式化该思想。&lt;br&gt;
    
    </summary>
    
      <category term="机器学习" scheme="xtf615.com/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="人工智能" scheme="xtf615.com/tags/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/"/>
    
      <category term="机器学习" scheme="xtf615.com/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="支持向量机" scheme="xtf615.com/tags/%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BA/"/>
    
      <category term="SVM" scheme="xtf615.com/tags/SVM/"/>
    
      <category term="线性分类器" scheme="xtf615.com/tags/%E7%BA%BF%E6%80%A7%E5%88%86%E7%B1%BB%E5%99%A8/"/>
    
      <category term="非线性分类器" scheme="xtf615.com/tags/%E9%9D%9E%E7%BA%BF%E6%80%A7%E5%88%86%E7%B1%BB%E5%99%A8/"/>
    
  </entry>
  
  <entry>
    <title>生成算法</title>
    <link href="xtf615.com/2017/03/25/%E7%94%9F%E6%88%90%E7%AE%97%E6%B3%95/"/>
    <id>xtf615.com/2017/03/25/生成算法/</id>
    <published>2017-03-25T02:09:22.000Z</published>
    <updated>2017-03-27T15:54:15.135Z</updated>
    
    <content type="html"><![CDATA[<p>　　这篇笔记主要针对斯坦福ML公开课的第五个视频，主要内容包括生成学习算法(generate learning algorithm)、高斯判别分析(Gaussian Discriminant Analysis)、朴素贝叶斯(Navie Bayes)、拉普拉斯平滑(Laplace Smoothing)。</p>
<h1 id="生成学习算法概述"><a href="#生成学习算法概述" class="headerlink" title="生成学习算法概述"></a>生成学习算法概述</h1><p>　　到目前为止，我们学习的方法主要是直接对问题进行求解，比如二分类问题中的感知机算法和逻辑回归算法，都是在解空间中寻找一条直线从而把两种类别的样例分开，对于新的样例只要判断在直线的哪一侧即可，这种截至对问题求解的方法可以称作判别学习方法(discriminative learning algorithm)。判别学习方法的任务是训练如下模型：<br>$$p(y|x;\theta)$$<br><a id="more"></a><br>即在给定特征x的情况下，我们直接求出y的条件概率作为模型的输出。例如逻辑回归中，我们将\(h_\theta(x)=g(\theta^Tx)\)作为模型\(p(y|x;\theta)\)的结果。也就是说，判别方法并不关心数据长什么样子，它直接面向分类任务，只关心数据之间的区别或者差异，然后利用学习到的区别来对样本做出预测。<br>　　而生成学习算法则是试图弄清楚数据是怎样产生的，每种数据的分布规律是怎样的，例如二分类问题中，生成学习算法对两个类别分别进行建模，用新的样例去匹配两个模型，<strong>匹配度较高的作为新样例的类别</strong>。比如良性肿瘤与恶性肿瘤分类，首先对两个类别分别建模，比如分别计算两类肿瘤是否扩散的概率，计算肿瘤大小大于某个值的概率；再比如狗与大象的分类，分别对狗与大象建模，比如计算两种类别体重大于某个值的概率，鼻子长度大于某个值的概率等等。即，每种特征都计算在不同类别下的概率，后面我们会讲到，可以利用朴素贝叶斯假设，每种特征在不同类别下的概率的累积可以作为\(P(x|y)\)的结果，即\(P(x|y)=\prod_{j=1}^n p(x_j|y)\), n是特征的数量，j是特征的下标。<br>　　形式化的说，判别学习方法是直接对\(p(y|x)\)进行建模，或者直接学习输入空间到输出空间的映射关系，其中x是类样本的特征，y是某类样本的分类标记。而生成学习方法是对\(p(x|y)\)(条件概率)和\(p(y)\)(先验概率)进行建模，然后依照贝叶斯法则求出后验概率\(p(y|x)\):<br>$$p(y|x)=\frac{p(x|y)p(y)}{p(x)}$$<br>使得后验概率最大的类别y即是新样本的预测值：<br>$$\mathop{argmax}\limits_y p(y|x)=\mathop{argmax}\limits_y \frac{p(x|y)p(y)}{p(x)}=\mathop{argmax}\limits_y p(x|y)p(y)$$<br>　　这个式子直观上也很好理解：比如我们在马路边远远地望到一个小动物，你想要判断它是一只狗还是一只鹿。首先，你肯定是要观察它的特征，然后根据你之前已经建立的经验，把它分别与狗和鹿的特征相对比，看更像哪个，也就是利用条件概率p(x|y)。然而距离太远了，根据你现在能够观察到的来看，可能和两者的特征都很符合。这个时候如果要让你作出判断的话，我想大多数人都会认为这个在马路边出现的动物是狗。为什么呢？因为狗在城市中更为常见，先验概率p(y=dog)的值更大。在这个过程中，我们便是同时利用了条件概率和先验概率来作出判断的。<br>　　注意，虽然在这个公式中也出现了\(p(y|x)\)，但它和判别模型中要预测的\(p(y|x;θ)\)是不同的。判别模型中我们直接计算出了\(p(y|x;θ)\)的大小，并把它作为预测结果的概率。而在生成模型中，我们是通过选取不同的y，来得到一个最大的\(p(y|x)\)，也就是看哪种类别会令测试样本出现的概率最大，把这种类别对应的y值作为预测结果。</p>
<h1 id="高斯判别分析"><a href="#高斯判别分析" class="headerlink" title="高斯判别分析"></a>高斯判别分析</h1><p>　　高斯判别分析（GDA）是其中一种生成学习算法。虽然它的名字里有判别两字，但却是生成学习算法。<br>　　在GDA中，假设\(p(x|y)\)属于多变量正态分布(multivariate normal distribution)。</p>
<h2 id="多变量正态分布"><a href="#多变量正态分布" class="headerlink" title="多变量正态分布"></a>多变量正态分布</h2><p>　　多变量正态分布是正态分布在多维变量下的扩展，它的参数是一个均值向量(mean vector) \(\mu\)和协方差矩阵(covariance matrix) \(\Sigma \sim \mathbb{R}^{n*n}\),其中n是多维变量的向量长度，\(\Sigma\)是对称正定矩阵。也写作，\(N(\mu,\Sigma)\),多变量正态分布的概率密度函数为：<br>$$p(x;\mu,\Sigma)=\frac{1}{(2\pi)^{n/2}|\Sigma|^{1/2}}exp\left(-\frac{1}{2}(x-\mu)^T \Sigma^{-1}(x-\mu) \right)$$<br>　　其中，\(|\Sigma|\)是行列式的值。<br>　　对于服从多变量正态分布的随机变量x，均值由下面的公式给出：<br>$$E[X] = \int_x xp(x;\mu,\Sigma)dx = \mu$$<br>　　协方差矩阵由协方差函数Cov得到,如果 \(X \sim N(\mu,\Sigma)\),则：<br>$$Cov(X)=\Sigma$$<br>　　其中，Cov的计算过程为：<br>$$Cov(Z)=E[(Z-E[Z])(Z-E[Z])^T]=E[ZZ^T]-(E[Z])(E[Z])^T$$<br>　　下图是部分多元正态分布概率密度函数图：<br>    <img src="/picture/machine-learning/gda1.jpg" alt="gda"><br>　　最左边代表\(\mu\)为0(\(\mu\)是2*1规格的零向量)，并且协方差矩阵\(\Sigma=I\)(\(\Sigma\)为2*2的单位矩阵)。这也称作标准正态分布。中间那幅密度函数图代表零均值，\(\Sigma=0.6I\),最右边代表零均值，\(\Sigma=2I\).我们可以看出随着\(\Sigma\)对角元素变大,密度函数变得更扁平分散(spread-out)，随着其变小，则变得更瘦高集中(compressed)。<br>    下图是当协方差矩阵不是单位矩阵倍数时的情况：<br>    <img src="/picture/machine-learning/gda2.jpg" alt="gda"><br>    两幅图的\(\Sigma\)分别为：<br>    $$\Sigma_1=\begin{bmatrix} 1　0.5 \\\ 0.5　1\end{bmatrix} \\\\<br>    \Sigma_2=\begin{bmatrix} 1　0.8 \\\ 0.8　1\end{bmatrix}<br>    $$<br>可以看出随着反对角元素的增大，密度函数往45°方向变得更加的瘦高集中(compressed)。我们可以通过观察下图的等高线图来对比：<br>    <img src="/picture/machine-learning/gda3.jpg" alt="gda"><br>　　实际上，对于一个二维向量，\(\Sigma\)的非对角元素表示了两个分量之间的相关性，而主对角元素则是各分量本身的方差，即：<br>    $$\Sigma=\begin{bmatrix} E[(X_1-\mu_1)^2]　E[(X_1-\mu_1)(X_2-\mu_2)] \\\ E[(X_2-\mu_2)(X_1-\mu_1)]　E[(X_2-\mu_2)^2] \end{bmatrix} =  \begin{bmatrix} \sigma_1^2　\sigma_{12}^2 \\\ \sigma_{21}^2　\sigma_2^2 \end{bmatrix}$$<br>　　显然对于上图，右边的图因为反对角数字更大，即相关性更强。则横轴变量变大时，右图的纵轴变量变化比左图大，即右图相关性更强。实际上可以体现出数据更集中，故右图在图形上体现出更加瘦高。</p>
<h2 id="GDA模型"><a href="#GDA模型" class="headerlink" title="GDA模型"></a>GDA模型</h2><p>　　GDA模型针对的是输入特征为连续值时的分类问题。这个模型的基本假设是目标值y服从伯努利分布，条件概率\(p(x|y)\)服从多元正态分布。即:<br>$$y \sim Bernoulli(\phi) \\\\<br>x|y=0 \sim N(\mu_0, \Sigma) \\\\<br>x|y=1 \sim N(\mu_1, \Sigma)<br>$$<br>于是它们的概率密度为：<br>$$p(y)=\phi_y(1-y)^{1-y} \\\\<br>p(x|y=0)=\frac{1}{(2\pi)^{n/2}|\Sigma|^{1/2}}exp\left(-\frac{1}{2}(x-\mu_0)^T \Sigma^{-1}(x-\mu_0) \right) \\\\<br>p(x|y=1)=\frac{1}{(2\pi)^{n/2}|\Sigma|^{1/2}}exp\left(-\frac{1}{2}(x-\mu_1)^T \Sigma^{-1}(x-\mu_1) \right)<br>$$<br>我们模型的参数包括，\(\phi, \Sigma, \mu_0, \mu_1 \).注意到，我们使用了两种不同的均值向量\(\mu_0和\mu_1\)，但是使用了同一种协方差矩阵\(\Sigma\), 则我们的极大似然函数的对数如下所示：<br>$$L(\phi,\mu_0,\mu_1,\Sigma)=log \prod_{i=1}^m p(x^{(i)},y^{(i)};\phi,\mu_0,\mu_1,\Sigma) \\\ =log \prod_{i=1}^m p(x^{(i)}|y^{(i)};\phi,\mu_0,\mu_1,\Sigma)p(y^{(i)};\phi)$$<br>对极大似然函数对数最大化，我们就得到了GDA模型各参数的极大虽然估计，即得到了如何使用GDA算法的方法，各参数极大似然估计如下：<br>$$\phi = \frac{1}{m}\sum_{i=1}^m I\{y^{(i)}=1\} \\\ \\\\<br>\mu_0 = \frac{\sum_{i=1}^m I\{y^{(i)}=0\} x^{(i)}}{\sum_{i=1}^m I\{y^{(i)}=0\}} \\\ \\\\<br>\mu_1 = \frac{\sum_{i=1}^m I\{y^{(i)}=1\} x^{(i)}}{\sum_{i=1}^m I\{y^{(i)}=1\}} \\\ \\\\<br>\Sigma = \frac{1}{m}\sum_{i=1}^m(x^{(i)}-\mu_{y^{(i)}})(x^{(i)}-\mu_{y^{(i)}})^T<br>$$<br>上述\(\mu_0\)代表类别为0的样本中，所有变量组成的均值向量。\(\mu_1\)代表类别为1的样本中，所有变量组成的均值向量，\(x^{(i)}\)实际上就是一个样本向量，\(\mu_{y^{(i)}}\)是指每个样本需要根据其类别来选择均值向量。<br>上述第一个式子可以由伯努利的最大似然估计得到，后面的三个式子在后一节里将进行详细推导。<br>一个二维GDA模型的例子如下图所示：<br><img src="/picture/machine-learning/gda4.jpg" alt="gda"><br>注意到两个二维高斯分布分别对两类数据进行拟合，它们使用相同的协方差矩阵，但却有不同的均值，在直线所示的部分,\(p(y=1|x)=(y=0|x)=0.5\)</p>
<h2 id="多元正态分布参数估计推导"><a href="#多元正态分布参数估计推导" class="headerlink" title="多元正态分布参数估计推导"></a>多元正态分布参数估计推导</h2><p>　　我们想证明：多元正态分布的参数\(\mu,\Sigma\)可由最大似然法求出，即：<br>$$\hat{\mu}=\overline{X}, \hat{\Sigma}=\frac{1}{m}S$$<br>其中，S为样本协方差矩阵，\(\overline{X}为样本均值向量\)<br>设，\(X_{(1)},X_{(2)},…,X_{(m)}\)来自正态总体\(N_n(\mu,\Sigma)\)容量为m的样本，每个样本\(X_{(a)}=(X_{a1},X_{a2},…,X_{an})^T,a=1,2,…,m\),<br>其中，m为样本容量，n为变量个数。构造似然函数：<br>$$<br>L(\mu,\Sigma)=\prod_{i=1}^m f(X_i, \mu, \Sigma)=\frac{1}{(2\pi)^{nm/2}|\Sigma|^{m/2}}exp\left(-\frac{1}{2}\sum_{i=1}^m(X_i-\mu)^T \Sigma^{-1}(X_i-\mu) \right)<br>$$<br>两边取对数，得到:<br>$$log(\mu,\Sigma)=-\frac{1}{2}nm log(2\pi)-\frac{m}{2}log|\Sigma| \\\\<br>        -\frac{1}{2}\sum_{i=1}^m(X_i-\mu)^T\Sigma^{-1}(X_i-\mu)<br>$$<br>因为对数函数为严格单调递增函数，所以可以通过极大值求参数估计量。<br>根据矩阵代数理论：对于实对称矩阵A，我们有：<br>$$\frac{\partial(X^TAX)}{\partial X} = 2AX \\\\<br>\frac{\partial(X^TAX)}{\partial A} = XX^T \\\\<br>\frac{\partial log(|A|)}{\partial A} = A^{-1}<br>$$<br>这里的\(\Sigma\)就是实对称矩阵，分别对\(\mu,\Sigma\)求偏导，则有：<br>$$<br>\frac{\partial log L(\mu,\Sigma)}{\partial \mu} = \sum_{i=1}^m \Sigma^{-1}(X_i-\mu)=0 \\\\<br>\frac{\partial log L(\mu,\Sigma)}{\partial \Sigma} = -\frac{m}{2}\Sigma^{-1}+\frac{1}{2}\sum_{i=1}^m(X_i-\mu)(X_i-\mu)^T(\Sigma^{-1})^2=0<br>$$<br>则：<br>$$<br>\hat{\mu} = \frac{1}{m}\sum_{i=1}^m X_i = \overline{X} \\\\<br>\hat{\Sigma} = \frac{1}{m}\sum_{i=1}^m (X_i-\overline{X})(X_i-\overline{X})^T=\frac{1}{m}S<br>$$</p>
<h2 id="GDA模型与logistic模型的关系"><a href="#GDA模型与logistic模型的关系" class="headerlink" title="GDA模型与logistic模型的关系"></a>GDA模型与logistic模型的关系</h2><p>前面我们提到：<br>$$\mathop{argmax}\limits_y p(y|x)=\mathop{argmax}\limits_y \frac{p(x|y)p(y)}{p(x)}=\mathop{argmax}\limits_y p(x|y)p(y)$$<br>我们有：<br>$$p(y=1|x)=\frac{p(x|y=1)p(y=1)}{p(x|y=1)p(y=1)+p(x|y=0)p(y=0)}$$<br>上式实际上可以表示成logistic函数的形式：<br>$$p(y=1|x;\phi,\mu_0,\mu_1,\Sigma)=\frac{1}{1+exp(-\theta^T X)}$$<br>　　其中，\(\theta是参数\phi,\mu_0,\mu_1,\Sigma\)某种形式的函数。GDA的后验分布可以表示logistic函数的形式。<br>　　实际上，可以证明，不仅仅当先验概率分布服从多变量正态分布时可以推导出逻辑回归的模型，当先验分布属于指数分布簇中的任何一个分布，如泊松分布时，都可以推导出逻辑回归模型。而反之不成立，逻辑回归的先验概率分布不一定必须得是指数分布簇中的成员，因此也可以说明logistic在建模上的鲁棒性。<br>　　因此推导逻辑回归模型有两种方法。第一种是之前提到的通过指数分布簇来推导，第二种则是通过生成学习假设先验概率分布的方式进行推导。<br>　　那么如何选择GDA与logistic回归模型呢？由上面的分析可以知道，GDA与logistic回归是泛化与特化的关系.GDA比logistic回归有更强的前置假设。当数服从或大致服从正态分布时，使用GDA会达到更好的效果，因为利用了更多的信息构建模型。但当数据不服从正态分布时，那么logistic回归更有效，因为它做出了更弱的假设，构建的模型更加鲁棒性。生成学习还有另外一个好处，就是可以使用比判别模型更少的数据构建出更鲁棒的模型。</p>
<h1 id="朴素贝叶斯"><a href="#朴素贝叶斯" class="headerlink" title="朴素贝叶斯"></a>朴素贝叶斯</h1><p>　　GDA针对的是特征向量X为连续值时的问题。而朴素贝叶斯Navie Bayes则针对的是特征向量为离散值时的问题。<br>　　NB算法的常见的应用是文本分类问题，例如邮件是否为垃圾邮件的分类问题。<br>　　对于文本分类问题来说，使用向量空间模型VSM来表示文本。何为VSM？首先，我们需要有一个词典，词典可以是现有词典，也可以从数据中统计出来的词典，对于每个文本，我们用长度等于词典大小的向量表示，如果文本包含某个词，则该词在词典中的索引为index，则表示文本向量的index出设为1，否则设为0。该方法对应多元伯努利分布。<br>　　如果按直接对p(x|y)进行建模，那么会遇到参数过多的问题，我们假设词典有50000个词，则向量长度为50000，向量中每个分量的取值为{0,1}，那么可能有\(2^50000\)个可能的结果，如果我们使用多项式分布对这\(2^50000\)个可能的结果进行建模，则对其建模需要\(2^50000-1\)个参数。<strong>这里仍然存在着一点疑问？为什么参数个数不是50000，而是\(2^50000-1\)呢？个人理解是，这里所说的是指对X建模, 暂时没有考虑y的因素。那么相当于我们需要计算每种X组合出现的概率。</strong><br>原话如下图所示：<br><img src="/picture/machine-learning/gda5.jpg" alt="gda"><br>　　朴素贝叶斯假设是在给定分类y后，假设特征向量中各个分量是条件独立的。例如对于垃圾邮件，即y=1，”购买”是第2087个词，“价格”是第39831个词，假设已经告诉了我们某个邮件是垃圾邮件，即y=1, 那么\(x_{2087}\)不会对\(x_{39831}\)的值产生任何影响。更正式的，这可以写作：\(p(x_{2087}|y)=p(x_{2087}|y,x_{39831})\)。注意我们不是说\(x_{2087}和x_{39831}\)是相互独立的，即不是指\(p(x_{2087})=p(x_{2087}|x_{39831})\),而只是假设在给定y的情况下，\(x_{2087}和x_{39831}\)是条件独立的。现在我们有：<br>$$p(x_1,…,x_{50000}|y)=p(x_1|y)p(x_2|y,x_1)p(x_3|y,x_1,x_2)…p(x_{50000}|y,x_1,x_2,…,x_{49999}) \\\\<br>= p(x_1|y)p(x_2|y)p(x_3|y)…p(x_{50000}|y) \\\\<br>= \prod_{i=1}^n p(x_i|y)<br>$$<br>第一个等式是根据通常的概率论得到的，第二个等式是根据贝叶斯假设得到的。虽然贝叶斯假设是个很强的假设，但是实践证明在许多问题上都表现得很好。<br>我们得到了NB方法的参数：<br>$$\phi_y=p(y=1) \\\\<br>\phi_{j|y=1} = p(x_j=1|y=1) \\\\<br>\phi_{j|y=0} = p(x_j=1|y=0)<br>$$<br>这里的\(\phi\)是向量，向量的分量代表每个特征项伯努利分布的参数值，实际上就是每个特征项出现的概率组成的向量。我们需要求出每个特征项在类别为0下出现的概率值和类别为1下出现的概率值。<br>于是，我们就得到NB方法的极大似然估计的对数函数：<br>$$L(\phi_y,\phi_{j|y=1},\phi_{j|y=0})=\prod_{i=1}^m p(x^{(i)},y^{(i)})=\prod_{i=1}^m p(x^{(i)}|y^{(i)})p(y^{(i)}) \\\\<br>=\prod_{i=1}^m \left(\prod_{j=1}^n p(x_j^{(i)}|y^{(i)})\right)p(y^{(i)})<br>$$<br>其中，n为词典的大小，也就是特征项的数目。m为样本的数量，也就是邮件数。第一个式子是根据极大似然估计方法得到的，第二个式子中\(p(x^{(i)}|y^{(i)})=\left(\prod_{j=1}^n p(x_j^{(i)}|y^{(i)})\right)\)是根据朴素贝叶斯假设得到的。j是特征项的下标。我们假设\(p(x_j|y)\)满足伯努利分布，因为特征项只取0和1，那么根据伯努利分布的最大似然估计：<br>我们得到如下参数的极大似然估计：<br>$$\phi_y = \frac{\sum_{i=1}^m I\{y^{(i)}=1\}}{m} \\\ \\\ \phi_{j|y=1} = \frac{\sum_{i=1}^m I\{x_j^{(i)}=1 \wedge y^{(i)}=1\}}{\sum_{i=1}^m I\{y^{(i)}=1\}} \\\ \\\ \phi_{j|y=0} = \frac{\sum_{i=1}^m I\{x_j^{(i)}=1 \wedge y^{(i)}=0\}}{\sum_{i=1}^m I\{y^{(i)}=0\}}$$<br>　　其中，\(\wedge\)代表并且，\(\phi_y\)是类别y的先验概率，二分类问题就是2*1的向量。\(\phi_{j|y=1}\)是类别为1的情况下，各个特征项参数组成的向量，也即各个特征项出现的概率组成的向量。\(\phi_{j|y=0}\)是类别为0的情况下，各个特征项参数组成的向量，也即各个特征项出现的概率组成的向量。\(x_j^{(i)}\)代表第i个样本的第j个特征项的值。<br>　　对于新样本，按照如下公式计算其后验概率值：<br>$$p(y=1|x)=\frac{p(x|y=1)p(y=1)}{p(x)}=\frac{p(x|y=1)p(y=1)}{p(x|y=1)p(y=1)+p(x|y=0)p(y=0)}=\frac{(\prod_{j=1}^n p(x_j|y=1))p(y=1)}{(\prod_{j=1}^n p(x_j|y=1))p(y=1)+(\prod_{j=1}^n p(x_j|y=0))p(y=0)}$$<br>我们可以将上述特征项的取值扩展到{0,1,2…,k}，而\(p(x_j|y)\)的概率分布由伯努利分布变为多项式分布，对于一些连续的变量，我们可以将其离散化后使其可以用NB方法解决，例如离散化方法可以是通过将连续变量按值分段实现。</p>
<h1 id="拉普拉斯平滑"><a href="#拉普拉斯平滑" class="headerlink" title="拉普拉斯平滑"></a>拉普拉斯平滑</h1><p>　　拉普拉斯平滑(Laplace Smoothing)又称为加1平滑。平滑方法的存在是为了解决零概率问题。<br>　　所谓的零概率问题，就是在计算新实例的概率时，如果某个分量在训练集中从没出现过，会导致整个实例的概率计算结果为０，针对文本分类问题就是当一个词语在训练集中没有出现过，那么该词语的概率为０，使用连乘计算文本出现的概率时，整个文本出现的概率也为０，这显然不合理，因为不能因为一个事件没有观测到就判断该事件的概率为０.<br>　　对于一个随机变量ｚ，它的取值范围为｛1,2,3,…,k｝，对于m次试验后的观测结果\({z^{(1)},z^{(2)},z^{(3)},…,z^{(m)}} \),极大似然估计按照下式计算：<br>$$\phi_j=\frac{\sum_{i=1}^m I(z^{(i)}=j)}{m}$$<br>使用Laplace平滑后，计算公式变为：<br>$$\phi_j=\frac{\sum_{i=1}^m I(z^{(i)}=j)+1}{m+k}$$<br>即在分母上加上随机变量Z取值范围的大小，在分子加1。<strong>注意不是y的取值范围！！</strong><br>可以发现，\(\sum_{j=1}^k \phi_j=1\)仍然满足。<br>回到NB算法，我们可以修正各分量的计算公式：<br>$$\phi_{j|y=1} = \frac{\sum_{i=1}^m I\{x_j^{(i)}=1 \wedge y^{(i)}=1\}+1}{\sum_{i=1}^m I\{y^{(i)}=1\}+2} \\\ \\\ \phi_{j|y=0} = \frac{\sum_{i=1}^m I\{x_j^{(i)}=1 \wedge y^{(i)}=0\}+1}{\sum_{i=1}^m I\{y^{(i)}=0\}+2}$$</p>
<p>　　另外，上文我们假设特征项是服从伯努利分布，对应于，词集型模型–根据分类中某词是否出现计算概率，不考虑权重。我们可以假设特征项服从多项式分布，对应于词频型模型–根据分类中某词出现的次数计算概率。这部分是下一个视频的内容，这里我们提前讲下这部分内容。</p>
<h1 id="拓展：朴素贝叶斯多项式事件模型"><a href="#拓展：朴素贝叶斯多项式事件模型" class="headerlink" title="拓展：朴素贝叶斯多项式事件模型"></a>拓展：朴素贝叶斯多项式事件模型</h1><p>　　前面我们提到的NB模型也被称作多元伯努利事件模型(Multivariate Bernoulli Event Model, 简称NB-MBEM)。这部分将介绍一种与多元伯努利事件模型有较大区别的NB模型，即多项式事件模型（Multinomial Event Model，简称NB-MEM）。<br>　　首先，NB-MEM改变了特征向量的表示方法。在NB-MBEM中，特征向量的每个分量代表词典中该index上的词语是否在文本中出现过，其取值范围为{0,1}，特征向量的长度为词典的大小。而在NB-MEM中，特征向量中的每个分量是文本中处于该分量位置的词语在词典中的索引，其取值范围是{1,2,…,|V|},|V|是词典的大小，特征向量的长度为相应样例文本中词语的数目，它会因为样本的不同而不同。<br>　　在NB-MEM中，假设文本的生成过程如下：</p>
<ul>
<li>随机确定文本的类别，比如是否为垃圾文本、教育类文本、财经类文本。</li>
<li>遍历词典的每个位置，以相同的多项式分布决定是否将该词包含在该文本中。</li>
</ul>
<p>由上面的生成过程可以知道，NB-MEM假设文本类别服从多项式分布或伯努利分布，即根据p(y)的先验概率分布得到的；而词典中所有的词语则服从多项式分布。生成过程还可以如下解释，即先在类别服从的先验分布p(y)下选取类别，然后遍历整个词典，在词典所服从的多项式分布\(p(x_i=1|y)=\phi_{i|y}\)中选择词语，确定是否放在文本中相应的位置上。因此根据朴素贝叶斯假设，某个文本整体出现的概率是：\(p(y)\prod_{i=1}^n p(x_i|y)\)<br>　　因此，NB-MEM的参数如下所示：<br>$$\phi_y=p(y) \\\\<br>\phi_{k|y=1}=p(x_j=k|y=1) \\\\<br>\phi_{k|y=0}=p(x_j=k|y=0) \\\\<br>其中，\phi_{k|y=1}可以代表字典中每个词语在类别为1的文本中出现的概率组成的向量<br>$$<br>　　得到参数在训练集上的极大似然估计：<br>$$\ell(\phi_y,\phi_{k|y=1},\phi_{k|y=0})=\prod_{i=1}^m p(x^{(i)},y^{(i)})<br>= \prod_{i=1}^m \left(\prod_{j=1}^{n_i} p(x_j^{(i)}|y);\phi_{k|y=1},\phi_{k|y=0}) \right) p(y^{(i)};\phi_y)<br>$$<br>注意上式的\(n_i\)是每个文本的单词量；m是文本数量。<br>可以根据多项式分布的极大似然估计方法(查查资料)，得到各个参数的极大似然估计：<br>$$<br>\phi_{k|y=1} = \frac{\sum_{i=1}^m \sum_{j=1}^{n_i} I\{x_j^{(i)}=k \wedge y^{(i)}=1\}}{\sum_{i=1}^m I\{y^{(i)}=1)\}n_i} \\\\<br>\phi_{k|y=0} = \frac{\sum_{i=1}^m \sum_{j=1}^{n_i} I\{x_j^{(i)}=k \wedge y^{(i)}=0\}}{\sum_{i=1}^m I\{y^{(i)}=0)\}n_i} \\\\<br>\phi_y = \frac{\sum_{i=1}^m I\{y^{(i)}=1\}}{m}<br>$$<br>实际上，\(\phi_{k|y=1}\)分子代表：对所有的m个文本样本进行外层遍历，再对每个文本的\(n_i\)个单词进行内层遍历，如果该文本类别为1，统计该文本中该单词出现的次数，最后将所有文本中该单词出现的次数进行累加作为分子。分母则代表，对所有类别为1的文本，累加其所有的单词量。<br>使用拉普拉斯平滑得到：<br>$$<br>\phi_{k|y=1} = \frac{\sum_{i=1}^m \sum_{j=1}^{n_i} I\{x_j^{(i)}=k \wedge y^{(i)}=1\}+1}{\sum_{i=1}^m I\{y^{(i)}=1)\}n_i+|V|} \\\\<br>\phi_{k|y=0} = \frac{\sum_{i=1}^m \sum_{j=1}^{n_i} I\{x_j^{(i)}=k \wedge y^{(i)}=0\}+1}{\sum_{i=1}^m I\{y^{(i)}=0)\}n_i+|V|}<br>$$<br>其中，|V|为词典的大写，也就是变量\(X\)的每个特征分量的取值范围。</p>
<h1 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h1><p><a href="http://open.163.com/special/opencourse/machinelearning.html" target="_blank" rel="external">斯坦福大学机器学习视频教程</a><br><a href="http://blog.csdn.net/win_in_action/article/details/51260486" target="_blank" rel="external">Naive Bayes Spam detection</a></p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;　　这篇笔记主要针对斯坦福ML公开课的第五个视频，主要内容包括生成学习算法(generate learning algorithm)、高斯判别分析(Gaussian Discriminant Analysis)、朴素贝叶斯(Navie Bayes)、拉普拉斯平滑(Laplace Smoothing)。&lt;/p&gt;
&lt;h1 id=&quot;生成学习算法概述&quot;&gt;&lt;a href=&quot;#生成学习算法概述&quot; class=&quot;headerlink&quot; title=&quot;生成学习算法概述&quot;&gt;&lt;/a&gt;生成学习算法概述&lt;/h1&gt;&lt;p&gt;　　到目前为止，我们学习的方法主要是直接对问题进行求解，比如二分类问题中的感知机算法和逻辑回归算法，都是在解空间中寻找一条直线从而把两种类别的样例分开，对于新的样例只要判断在直线的哪一侧即可，这种截至对问题求解的方法可以称作判别学习方法(discriminative learning algorithm)。判别学习方法的任务是训练如下模型：&lt;br&gt;$$p(y|x;\theta)$$&lt;br&gt;
    
    </summary>
    
      <category term="机器学习" scheme="xtf615.com/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="机器学习" scheme="xtf615.com/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="生成算法" scheme="xtf615.com/tags/%E7%94%9F%E6%88%90%E7%AE%97%E6%B3%95/"/>
    
      <category term="朴素贝叶斯" scheme="xtf615.com/tags/%E6%9C%B4%E7%B4%A0%E8%B4%9D%E5%8F%B6%E6%96%AF/"/>
    
      <category term="文本挖掘" scheme="xtf615.com/tags/%E6%96%87%E6%9C%AC%E6%8C%96%E6%8E%98/"/>
    
  </entry>
  
</feed>
